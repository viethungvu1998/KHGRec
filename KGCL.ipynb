{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afbc852c-cdc2-4c48-9ebe-82b2076a5b5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiotlab3/anaconda3/envs/hungvv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.init import xavier_normal_, xavier_uniform_\n",
    "import argparse\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "from os.path import abspath\n",
    "import random\n",
    "import collections  \n",
    "from collections import defaultdict\n",
    "import scipy.sparse as sp\n",
    "from itertools import product\n",
    "from random import shuffle,randint,choice,sample\n",
    "import csv \n",
    "\n",
    "from util.conf import OptionConf\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse.linalg import eigs\n",
    "from util.loss_torch import bpr_loss, l2_reg_loss, EmbLoss, contrastLoss\n",
    "from util.init import *\n",
    "from base.torch_interface import TorchGraphInterface\n",
    "import os\n",
    "import numpy as np \n",
    "import time \n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from util.conf import ModelConf\n",
    "from base.recommender import Recommender\n",
    "from util.algorithm import find_k_largest\n",
    "from time import strftime, localtime\n",
    "from data.loader import FileIO\n",
    "from util.evaluation import ranking_evaluation\n",
    "\n",
    "from data.data import Data\n",
    "from data.graph import Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a243349-ffeb-4ef0-a4d6-362b54059c1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Graph Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5809872-6a59-4b9c-8bcf-c23fa68963d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GraphRecommender(Recommender):\n",
    "    def __init__(self, conf, data, data_kg, knowledge_set, **kwargs):\n",
    "        super(GraphRecommender, self).__init__(conf, data, data_kg, knowledge_set,**kwargs)\n",
    "        self.data = data\n",
    "        self.data_kg = data_kg\n",
    "        self.bestPerformance = []\n",
    "        top = self.ranking['-topN'].split(',')\n",
    "        self.topN = [int(num) for num in top]\n",
    "        self.max_N = max(self.topN)\n",
    "        \n",
    "        self.output_path = kwargs['output_path']\n",
    "        if not os.path.exists(self.output_path):\n",
    "            os.makedirs(self.output_path)\n",
    "            \n",
    "    def print_model_info(self):\n",
    "        super(GraphRecommender, self).print_model_info()\n",
    "        # # print dataset statistics\n",
    "        print('Training Set Size: (user number: %d, item number %d, interaction number: %d)' % (self.data.training_size()))\n",
    "        print('Test Set Size: (user number: %d, item number %d, interaction number: %d)' % (self.data.test_size()))\n",
    "        print('=' * 80)\n",
    "\n",
    "    def build(self):\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        pass\n",
    "\n",
    "    def predict(self, u):\n",
    "        pass\n",
    "\n",
    "    def test(self, user_emb, item_emb):\n",
    "        def process_bar(num, total):\n",
    "            rate = float(num) / total\n",
    "            ratenum = int(50 * rate)\n",
    "            r = '\\rProgress: [{}{}]{}%'.format('+' * ratenum, ' ' * (50 - ratenum), ratenum*2)\n",
    "            sys.stdout.write(r)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        # predict\n",
    "        rec_list = {}\n",
    "        user_count = len(self.data.test_set)\n",
    "        lst_users =  list(self.data_kg.userent.keys())\n",
    "        lst_items =  list(self.data_kg.itement.keys())\n",
    "        \n",
    "        for i, user in enumerate(self.data.test_set):\n",
    "            user_id  = lst_users.index(user)\n",
    "            score = torch.matmul(user_emb[user_id], item_emb.transpose(0, 1))\n",
    "            candidates = score.cpu().numpy()\n",
    "            \n",
    "            # e_find_candidates = time.time()\n",
    "            # print(\"Calculate candidates time: %f s\" % (e_find_candidates - s_find_candidates))\n",
    "            # predictedItems = denormalize(predictedItems, self.data.rScale[-1], self.data.rScale[0])\n",
    "            rated_list, li = self.data.user_rated(user)\n",
    "            for item in rated_list:\n",
    "                candidates[lst_items.index(item)] = -10e8\n",
    "            # s_find_k_largest = time.time()\n",
    "            ids, scores = find_k_largest(self.max_N, candidates)\n",
    "            # e_find_k_largest = time.time()\n",
    "            # print(\"Find k largest candidates: %f s\" % (e_find_k_largest - s_find_k_largest))\n",
    "            item_names = [lst_items[iid] for iid in ids]\n",
    "            rec_list[user] = list(zip(item_names, scores))\n",
    "            if i % 1000 == 0:\n",
    "                process_bar(i, user_count)\n",
    "        process_bar(user_count, user_count)\n",
    "        print('')\n",
    "        return rec_list\n",
    "    \n",
    "    def evaluate(self, rec_list):\n",
    "        self.recOutput.append('userId: recommendations in (itemId, ranking score) pairs, * means the item is hit.\\n')\n",
    "        for user in self.data.test_set:\n",
    "            line = str(user) + ':'\n",
    "            for item in rec_list[user]:\n",
    "                line += ' (' + str(item[0]) + ',' + str(item[1]) + ')'\n",
    "                if item[0] in self.data.test_set[user]:\n",
    "                    line += '*'\n",
    "            line += '\\n'\n",
    "            self.recOutput.append(line)\n",
    "        current_time = strftime(\"%Y-%m-%d %H-%M-%S\", localtime(time.time()))\n",
    "        # output prediction result\n",
    "        out_dir = self.output_path\n",
    "        file_name = self.config['model.name'] + '@' + current_time + '-top-' + str(self.max_N) + 'items' + '.txt'\n",
    "        FileIO.write_file(out_dir, file_name, self.recOutput)\n",
    "        print('The result has been output to ', abspath(out_dir), '.')\n",
    "        file_name = self.config['model.name'] + '@' + current_time + '-performance' + '.txt'\n",
    "        self.result = ranking_evaluation(self.data.test_set, rec_list, self.topN)\n",
    "        self.model_log.add('###Evaluation Results###')\n",
    "        self.model_log.add(self.result)\n",
    "        FileIO.write_file(out_dir, file_name, self.result)\n",
    "        print('The result of %s:\\n%s' % (self.model_name, ''.join(self.result)))\n",
    "\n",
    "    def fast_evaluation(self, model, epoch, user_embed, item_embed, kwargs=None):\n",
    "        print('Evaluating the model...')\n",
    "        s_test = time.time()\n",
    "        rec_list = self.test(user_embed, item_embed)\n",
    "        e_test = time.time() \n",
    "        print(\"Test time: %f s\" % (e_test - s_test))\n",
    "        \n",
    "        s_measure = time.time()\n",
    "        measure = ranking_evaluation(self.data.test_set, rec_list, [self.max_N])\n",
    "        e_measure = time.time()\n",
    "        print(\"Measure time: %f s\" % (e_measure - s_measure))\n",
    "        \n",
    "        if len(self.bestPerformance) > 0:\n",
    "            count = 0\n",
    "            performance = {}\n",
    "            for m in measure[1:]:\n",
    "                k, v = m.strip().split(':')\n",
    "                performance[k] = float(v)\n",
    "            for k in self.bestPerformance[1]:\n",
    "                if self.bestPerformance[1][k] > performance[k]:\n",
    "                    count += 1\n",
    "                else:\n",
    "                    count -= 1\n",
    "            if count < 0:\n",
    "                self.bestPerformance[1] = performance\n",
    "                self.bestPerformance[0] = epoch + 1\n",
    "                # try:\n",
    "                #     self.save(kwargs)\n",
    "                # except:\n",
    "                self.save(model)\n",
    "        else:\n",
    "            self.bestPerformance.append(epoch + 1)\n",
    "            performance = {}\n",
    "            for m in measure[1:]:\n",
    "                k, v = m.strip().split(':')\n",
    "                performance[k] = float(v)\n",
    "            self.bestPerformance.append(performance)\n",
    "            # try:\n",
    "            #     self.save(kwargs)\n",
    "            # except:\n",
    "            self.save(model)\n",
    "        print('-' * 120)\n",
    "        print('Real-Time Ranking Performance ' + ' (Top-' + str(self.max_N) + ' Item Recommendation)')\n",
    "        measure = [m.strip() for m in measure[1:]]\n",
    "        print('*Current Performance*')\n",
    "        print('Epoch:', str(epoch + 1) + ',', '  |  '.join(measure))\n",
    "        bp = ''\n",
    "        # for k in self.bestPerformance[1]:\n",
    "        #     bp+=k+':'+str(self.bestPerformance[1][k])+' | '\n",
    "        bp += 'Hit Ratio' + ':' + str(self.bestPerformance[1]['Hit Ratio']) + '  |  '\n",
    "        bp += 'Precision' + ':' + str(self.bestPerformance[1]['Precision']) + '  |  '\n",
    "        bp += 'Recall' + ':' + str(self.bestPerformance[1]['Recall']) + '  |  '\n",
    "        # bp += 'F1' + ':' + str(self.bestPerformance[1]['F1']) + ' | '\n",
    "        bp += 'NDCG' + ':' + str(self.bestPerformance[1]['NDCG'])\n",
    "        print('*Best Performance* ')\n",
    "        print('Epoch:fast_evaluation', str(self.bestPerformance[0]) + ',', bp)\n",
    "        print('-' * 120)\n",
    "        return measure\n",
    "    \n",
    "    def save(self, model):\n",
    "        with torch.no_grad():\n",
    "            ego_emb =  model.calc_cf_embeddings()\n",
    "            user_emb = ego_emb[list(rec.data_kg.userent.keys())]\n",
    "            item_emb = ego_emb[list(rec.data_kg.itement.keys())]\n",
    "            self.best_user_emb, self.best_item_emb = user_emb, item_emb\n",
    "        self.save_model(model)\n",
    "    \n",
    "    def save_model(self, model):\n",
    "        # save model \n",
    "        current_time = strftime(\"%Y-%m-%d\", localtime(time.time()))\n",
    "        out_dir = self.output_path\n",
    "        file_name =  self.config['model.name'] + '@' + current_time + '-weight' + '.pth'\n",
    "        weight_file = out_dir + '/' + file_name \n",
    "        torch.save(model.state_dict(), weight_file)\n",
    "\n",
    "\n",
    "    def save_performance_row(self, ep, data_ep):\n",
    "        # opening the csv file in 'w' mode\n",
    "        csv_path = self.output_path + 'train_performance.csv'\n",
    "        \n",
    "        # 'Hit Ratio:0.00328', 'Precision:0.00202', 'Recall:0.00337', 'NDCG:0.00292\n",
    "        hit = float(data_ep[0].split(':')[1])\n",
    "        precision = float(data_ep[1].split(':')[1])\n",
    "        recall = float(data_ep[2].split(':')[1])\n",
    "        ndcg = float(data_ep[3].split(':')[1])\n",
    "        \n",
    "        with open(csv_path, 'a+', newline = '') as f:\n",
    "            header = ['ep', 'hit@20', 'prec@20', 'recall@20', 'ndcg@20']\n",
    "            writer = csv.DictWriter(f, fieldnames = header)\n",
    "            # writer.writeheader()\n",
    "            writer.writerow({\n",
    "                 'ep' : ep,\n",
    "                 'hit@20': hit,\n",
    "                 'prec@20': precision,\n",
    "                 'recall@20': recall,\n",
    "                 'ndcg@20': ndcg,\n",
    "            })\n",
    "            \n",
    "    def save_loss_row(self, data_ep):\n",
    "        csv_path = self.output_path + 'loss.csv'\n",
    "        with open(csv_path, 'a+', newline ='') as f:\n",
    "            header = ['ep', 'train_loss', 'cf_loss', 'kg_loss']\n",
    "            writer = csv.DictWriter(f, fieldnames = header)\n",
    "            # writer.writeheader()\n",
    "            writer.writerow({\n",
    "                'ep' : data_ep[0],\n",
    "                'train_loss': data_ep[1],\n",
    "                 'cf_loss': data_ep[2],\n",
    "                 'kg_loss': data_ep[3]\n",
    "            })\n",
    "\n",
    "    def save_loss(self, train_losses, rec_losses, kg_losses):\n",
    "        df_train_loss = pd.DataFrame(train_losses, columns = ['ep', 'loss'])\n",
    "        df_rec_loss = pd.DataFrame(rec_losses, columns = ['ep', 'loss'])\n",
    "        df_kg_loss = pd.DataFrame(kg_losses, columns = ['ep', 'loss'])\n",
    "        df_train_loss.to_csv(self.output_path + '/train_loss.csv')\n",
    "        df_rec_loss.to_csv(self.output_path + '/rec_loss.csv')\n",
    "        df_kg_loss.to_csv(self.output_path + '/kg_loss.csv')\n",
    "    \n",
    "    def save_perfomance_training(self, log_train):\n",
    "        df_train_log = pd.DataFrame(log_train)\n",
    "        df_train_log.to_csv(self.output_path + '/train_performance.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62daafda-3820-49bb-b862-c76b720d0b32",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a0c5817-6093-4b16-8dc7-652683f7dbc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Interaction(Data, Graph):\n",
    "    def __init__(self, conf, training, test):\n",
    "        self.conf = conf \n",
    "        Graph.__init__(self)\n",
    "        Data.__init__(self,conf,training,test)\n",
    "\n",
    "        self.user = {}\n",
    "        self.item = {}\n",
    "        self.id2user = {}\n",
    "        self.id2item = {}\n",
    "        self.training_set_u = defaultdict(dict)\n",
    "        self.training_set_i = defaultdict(dict)\n",
    "        self.test_set = defaultdict(dict)\n",
    "        self.user_history_dict = defaultdict(dict)\n",
    "\n",
    "        self.test_set_item = set()\n",
    "        self.__generate_set()\n",
    "\n",
    "        self.n_users = len(self.training_set_u)\n",
    "        self.n_items = len(self.training_set_i) \n",
    "\n",
    "        self.n_cf_train = len(self.training_data)\n",
    "        self.n_cf_test = len(self.test_data)\n",
    "\n",
    "        # self.ui_adj = self.__create_sparse_bipartite_adjacency()\n",
    "        # self.norm_adj = self.normalize_graph_mat(self.ui_adj)\n",
    "        # self.interaction_mat, self.inv_interaction_mat = self.__create_sparse_interaction_matrix()\n",
    "        \n",
    "    def __generate_set(self):\n",
    "        for entry in self.training_data:\n",
    "            user, item, rating = entry\n",
    "            user, item = int(user), int(item)\n",
    "            if user not in self.user:\n",
    "                self.user[user] = len(self.user)\n",
    "                self.id2user[self.user[user]] = user\n",
    "            if item not in self.item:\n",
    "                self.item[item] = len(self.item)\n",
    "                self.id2item[self.item[item]] = item\n",
    "                # userList.append\n",
    "            # construct user_history_dict \n",
    "            if rating == 1.0:\n",
    "                if user not in self.user_history_dict:\n",
    "                    self.user_history_dict[user] = []\n",
    "                self.user_history_dict[user].append(item)\n",
    "            \n",
    "            self.training_set_u[user][item] = rating\n",
    "            self.training_set_i[item][user] = rating\n",
    "        \n",
    "        for entry in self.test_data:\n",
    "            user, item, rating = entry\n",
    "            if user not in self.user:\n",
    "                continue\n",
    "            self.test_set[user][item] = rating\n",
    "            self.test_set_item.add(item)\n",
    "\n",
    "    def __create_sparse_bipartite_adjacency(self, self_connection=False):\n",
    "        '''\n",
    "        return a sparse adjacency matrix with the shape (user number + item number, user number + item number)\n",
    "        '''\n",
    "        n_nodes = self.n_users + self.n_items\n",
    "        row_idx = [int(pair[0]) for pair in self.training_data]\n",
    "        col_idx = [int(pair[1]) for pair in self.training_data]\n",
    "        user_np = np.array(row_idx)\n",
    "        item_np = np.array(col_idx)\n",
    "        ratings = np.ones_like(user_np, dtype=np.float32)\n",
    "        tmp_adj = sp.csr_matrix((ratings, (user_np, item_np + self.n_users)), shape=(n_nodes, n_nodes),dtype=np.float32)\n",
    "        adj_mat = tmp_adj + tmp_adj.T\n",
    "        if self_connection:\n",
    "            adj_mat += sp.eye(n_nodes)\n",
    "        return adj_mat\n",
    "    \n",
    "    def __create_sparse_interaction_matrix(self):\n",
    "        \"\"\"\n",
    "            return a sparse adjacency matrix with the shape (user number, item number)\n",
    "        \"\"\"\n",
    "        row, col, entries = [], [], []\n",
    "        for pair in self.training_data:\n",
    "            row += [int(pair[0])]\n",
    "            col += [int(pair[1])]\n",
    "            entries += [1.0]\n",
    "        interaction_mat = sp.csr_matrix((entries, (row, col)), shape=(self.n_users,self.n_items),dtype=np.float32)\n",
    "        inv_interaction_mat = sp.csr_matrix((entries, (col, row)), shape=(self.n_items, self.n_users), dtype=np.float32)\n",
    "        return interaction_mat, inv_interaction_mat\n",
    "            \n",
    "    def get_user_id(self, u):\n",
    "        if u in self.user:\n",
    "            return self.user[u]\n",
    "\n",
    "    def get_item_id(self, i):\n",
    "        if i in self.item:\n",
    "            return self.item[i]\n",
    "\n",
    "    def training_size(self):\n",
    "        return len(self.user), len(self.item), len(self.training_data)\n",
    "\n",
    "    def test_size(self):\n",
    "        return len(self.test_set), len(self.test_set_item), len(self.test_data)\n",
    "\n",
    "    def contain(self, u, i):\n",
    "        'whether user u rated item i'\n",
    "        if u in self.user and i in self.training_set_u[u]:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def contain_user(self, u):\n",
    "        'whether user is in training set'\n",
    "        if u in self.user:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def contain_item(self, i):\n",
    "        \"\"\"whether item is in training set\"\"\"\n",
    "        if i in self.item:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def user_rated(self, u):\n",
    "        return list(self.training_set_u[u].keys()), list(self.training_set_u[u].values())\n",
    "\n",
    "    def item_rated(self, i):\n",
    "        return list(self.training_set_i[i].keys()), list(self.training_set_i[i].values())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb4bb4a-e750-4360-801b-4a90b4f06bec",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "193ecb06-7fc5-405b-a3e4-cd8063fafc64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Knowledge(Interaction):\n",
    "    def __init__(self, conf, training, test, knowledge):\n",
    "        super().__init__(conf, training, test)\n",
    "        self.conf = conf \n",
    "        self.kg_data = knowledge\n",
    "\n",
    "        self.entity = {}\n",
    "        self.id2ent = {}\n",
    "\n",
    "        self.userent = {}\n",
    "        self.itement = {}\n",
    "        \n",
    "        self.u2id = {}\n",
    "        self.id2u = {}\n",
    "        \n",
    "        self.i2id = {}\n",
    "        self.id2i = {}\n",
    "        \n",
    "        self.relation = {}\n",
    "        self.id2rel = {}\n",
    "\n",
    "        self.cf_train_data = np.array(training)\n",
    "        self.training_set_e = defaultdict(dict)\n",
    "\n",
    "        self.construct_data()\n",
    "        \n",
    "        self.laplacian_type = 'random-walk'\n",
    "        self.create_adjacency_dict()\n",
    "        self.create_laplacian_dict()\n",
    "        \n",
    "        self.kg_interaction_mat = self.__create_sparse_knowledge_interaction_matrix()\n",
    "    \n",
    "    def construct_data(self):\n",
    "        kg_data = self.kg_data\n",
    "        n_relations = max(kg_data['r']) + 1\n",
    "        inverse_kg_data = kg_data.copy()\n",
    "        inverse_kg_data = inverse_kg_data.rename({'h': 't', 't': 'h'}, axis='columns')\n",
    "        inverse_kg_data['r'] += n_relations\n",
    "\n",
    "        kg_data = pd.concat([kg_data, inverse_kg_data], axis=0, ignore_index=True, sort=False)\n",
    "\n",
    "        # remap user_id \n",
    "        kg_data['r'] += 2\n",
    "        \n",
    "        kg_train_data = pd.concat([kg_data, inverse_kg_data], axis=0, ignore_index=True, sort=False)\n",
    "        self.n_entities = max(max(kg_train_data['h']), max(kg_train_data['t'])) + 1\n",
    "        self.n_relations = max(kg_train_data['r']) + 1\n",
    "\n",
    "        # add interactions to kg data\n",
    "        cf2kg_train_data = pd.DataFrame(np.zeros((self.n_cf_train, 3), dtype=np.int32), columns=['h', 'r', 't'])\n",
    "        cf2kg_train_data['h'] = self.cf_train_data[:,0]\n",
    "        cf2kg_train_data['t'] = self.cf_train_data[:,1]\n",
    "\n",
    "        inverse_cf2kg_train_data = pd.DataFrame(np.ones((self.n_cf_train, 3), dtype=np.int32), columns=['h', 'r', 't'])\n",
    "        inverse_cf2kg_train_data['h'] = self.cf_train_data[:,1]\n",
    "        inverse_cf2kg_train_data['t'] = self.cf_train_data[:,0]\n",
    "\n",
    "        self.kg_train_data = pd.concat([kg_train_data, cf2kg_train_data, inverse_cf2kg_train_data], ignore_index=True)\n",
    "        self.n_kg_train = len(self.kg_train_data)\n",
    "\n",
    "        self.n_users_entities = int(max(max(self.kg_train_data['h']), max(self.kg_train_data['t'])) + 1)\n",
    "\n",
    "        # construct kg dict\n",
    "        h_list = []\n",
    "        t_list = []\n",
    "        r_list = []\n",
    "\n",
    "        self.train_kg_dict = collections.defaultdict(list)\n",
    "        self.train_relation_dict = collections.defaultdict(list)\n",
    "\n",
    "        for idx, row in self.kg_train_data.iterrows():\n",
    "            h, r, t = int(row['h']), int(row['r']), int(row['t'])\n",
    "            h_list.append(h)\n",
    "            t_list.append(t)\n",
    "            r_list.append(r)\n",
    "\n",
    "            if h not in self.entity:\n",
    "                self.entity[h] = len(self.entity)\n",
    "                self.id2ent[self.entity[h]] = h\n",
    "                # check h co phai user hay item k\n",
    "                if h in self.user:\n",
    "                    self.userent[h] = len(self.userent)\n",
    "                #     # self.id2userent[self.userent[h]] = h\n",
    "                if h in self.item:\n",
    "                    self.itement[h] = len(self.itement)\n",
    "                #     # self.id2itement[self.itement[h]] = h\n",
    "\n",
    "            if t not in self.entity:\n",
    "                self.entity[t] = len(self.entity)\n",
    "                self.id2ent[self.entity[t]] = t \n",
    "                # check h co phai user hay item k \n",
    "                if t in self.user:\n",
    "                    self.userent[t] = len(self.userent)\n",
    "                #     # self.id2userent[self.userent[t]] = t\n",
    "                if t in self.item:\n",
    "                    self.itement[t] = len(self.itement)\n",
    "                #     # self.id2itement[self.itement[t]] = t\n",
    "            if r not in self.relation:\n",
    "                self.relation[r] = len(self.relation)\n",
    "                self.id2rel[self.relation[r]] = r \n",
    "            \n",
    "            self.training_set_e[t][h] = r\n",
    "            self.train_kg_dict[h].append((t, r))\n",
    "            self.train_relation_dict[r].append((h, t))\n",
    "        \n",
    "        self.h_list = torch.LongTensor(h_list).cuda()\n",
    "        self.t_list = torch.LongTensor(t_list).cuda()\n",
    "        self.r_list = torch.LongTensor(r_list).cuda()\n",
    "        \n",
    "        lst_user_entities = list(self.userent.keys())\n",
    "        lst_item_entities = list(self.itement.keys())\n",
    "\n",
    "        for idx, u in enumerate(lst_user_entities):\n",
    "            self.u2id[u] = idx\n",
    "            self.id2u[idx] = u\n",
    "        for idx, i in enumerate(lst_item_entities):\n",
    "            self.i2id[i] = idx\n",
    "            self.id2i[idx] = i\n",
    "        \n",
    "    def get_entity_id(self, e):\n",
    "        if e in self.entity:\n",
    "            return self.entity[e]\n",
    "    \n",
    "    def __create_sparse_knowledge_interaction_matrix(self):\n",
    "        \"\"\"\n",
    "            return a sparse adjacency matrix with the shape (entity number, entity number)\n",
    "        \"\"\"\n",
    "        row, col, entries = [], [], []\n",
    "        for idx, pair in self.kg_train_data.iterrows():\n",
    "            head, tail = int(pair['h']), int(pair['t'])\n",
    "            row += [head]\n",
    "            col += [tail]\n",
    "            entries += [1.0]\n",
    "        interaction_mat = sp.csr_matrix((entries, (row, col)), shape=(self.n_users_entities, self.n_users_entities),dtype=np.float32)\n",
    "        return interaction_mat\n",
    "    \n",
    "    def convert_coo2tensor(self, coo):\n",
    "        values = coo.data\n",
    "        indices = np.vstack((coo.row, coo.col))\n",
    "\n",
    "        i = torch.LongTensor(indices)\n",
    "        v = torch.FloatTensor(values)\n",
    "        shape = coo.shape\n",
    "        return torch.sparse.FloatTensor(i, v, torch.Size(shape))\n",
    "    \n",
    "    def create_adjacency_dict(self):\n",
    "        self.adjacency_dict = {}\n",
    "        for r, ht_list in self.train_relation_dict.items():\n",
    "            rows = [e[0] for e in ht_list]\n",
    "            cols = [e[1] for e in ht_list]\n",
    "            vals = [1] * len(rows)\n",
    "            adj = sp.coo_matrix((vals, (rows, cols)), shape=(self.n_users_entities, self.n_users_entities))\n",
    "            self.adjacency_dict[r] = adj\n",
    "    \n",
    "    def create_laplacian_dict(self):\n",
    "        def symmetric_norm_lap(adj):\n",
    "            rowsum = np.array(adj.sum(axis=1))\n",
    "\n",
    "            d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "            d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0\n",
    "            d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "\n",
    "            norm_adj = d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt)\n",
    "            return norm_adj.tocoo()\n",
    "\n",
    "        def random_walk_norm_lap(adj):\n",
    "            rowsum = np.array(adj.sum(axis=1))\n",
    "\n",
    "            d_inv = np.power(rowsum, -1.0).flatten()\n",
    "            d_inv[np.isinf(d_inv)] = 0\n",
    "            d_mat_inv = sp.diags(d_inv)\n",
    "\n",
    "            norm_adj = d_mat_inv.dot(adj)\n",
    "            return norm_adj.tocoo()\n",
    "\n",
    "        if self.laplacian_type == 'symmetric':\n",
    "            norm_lap_func = symmetric_norm_lap\n",
    "        elif self.laplacian_type == 'random-walk':\n",
    "            norm_lap_func = random_walk_norm_lap\n",
    "        \n",
    "        self.laplacian_dict = {}\n",
    "        for r, adj in self.adjacency_dict.items():\n",
    "            self.laplacian_dict[r] = norm_lap_func(adj)\n",
    "\n",
    "        A_in = sum(self.laplacian_dict.values())\n",
    "        self.A_in = self.convert_coo2tensor(A_in.tocoo())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1216710e-2725-450e-a2e8-116c959bc8f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "732e6cd4-c756-4382-a457-104ac8446f66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Aggregator(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, dropout, aggregator_type):\n",
    "        super(Aggregator, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.dropout = dropout\n",
    "        self.aggregator_type = aggregator_type\n",
    "\n",
    "        self.message_dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.LeakyReLU()\n",
    "\n",
    "        if self.aggregator_type == 'gcn':\n",
    "            self.linear = nn.Linear(self.in_dim, self.out_dim)       # W in Equation (6)\n",
    "            # nn.init.xavier_uniform_(self.linear.weight)\n",
    "\n",
    "        elif self.aggregator_type == 'graphsage':\n",
    "            self.linear = nn.Linear(self.in_dim * 2, self.out_dim)   # W in Equation (7)\n",
    "            # nn.init.xavier_uniform_(self.linear.weight)\n",
    "\n",
    "        elif self.aggregator_type == 'bi-interaction':\n",
    "            \n",
    "            self.linear1 = nn.Linear(self.in_dim, self.out_dim)      # W1 in Equation (8)\n",
    "            self.linear2 = nn.Linear(self.in_dim, self.out_dim)      # W2 in Equation (8)\n",
    "            # nn.init.xavier_uniform_(self.linear1.weight)\n",
    "            # nn.init.xavier_uniform_(self.linear2.weight)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        self.ln1 = nn.LayerNorm(self.out_dim)\n",
    "        self.ln2 = nn.LayerNorm(self.out_dim)\n",
    "        \n",
    "    def forward(self, ego_embeddings, A_in):\n",
    "        \"\"\"\n",
    "        ego_embeddings:  (n_users + n_entities, in_dim)\n",
    "        A_in:            (n_users + n_entities, n_users + n_entities), torch.sparse.FloatTensor\n",
    "        \"\"\"\n",
    "        # Equation (3)\n",
    "        side_embeddings = torch.matmul(A_in, ego_embeddings)\n",
    "\n",
    "        if self.aggregator_type == 'gcn':\n",
    "            # Equation (6) & (9)\n",
    "            embeddings = ego_embeddings + side_embeddings\n",
    "            embeddings = self.ln1(self.activation(self.linear(embeddings)))\n",
    "            \n",
    "        elif self.aggregator_type == 'graphsage':\n",
    "            # Equation (7) & (9)\n",
    "            embeddings = torch.cat([ego_embeddings, side_embeddings], dim=1)\n",
    "            embeddings = self.ln1(self.activation(self.linear(embeddings)))\n",
    "\n",
    "        elif self.aggregator_type == 'bi-interaction':\n",
    "            # Equation (8) & (9)\n",
    "            sum_embeddings = self.ln1(self.activation(self.linear1(ego_embeddings + side_embeddings)))\n",
    "            bi_embeddings = self.ln2(self.activation(self.linear2(ego_embeddings * side_embeddings)))\n",
    "            embeddings = bi_embeddings + sum_embeddings\n",
    "\n",
    "        embeddings = self.message_dropout(embeddings)           # (n_users + n_entities, out_dim)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8ac61ab-4ae5-4708-8a7c-f6e1baa0c7e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class KGAT(nn.Module):\n",
    "\n",
    "    def __init__(self, args, rec, A_in=None, user_pre_embed=None, item_pre_embed=None):\n",
    "\n",
    "        super(KGAT, self).__init__()\n",
    "        \n",
    "        self.user_indices = torch.LongTensor(list(rec.data_kg.userent.keys())).cuda()\n",
    "        self.item_indices =  torch.LongTensor(list(rec.data_kg.itement.keys())).cuda()\n",
    "        \n",
    "        self.use_pretrain = args['use_pretrain']\n",
    "        \n",
    "        self.n_users = rec.data_kg.n_users\n",
    "        self.n_entities = rec.data_kg.n_entities\n",
    "        self.n_relations = rec.data_kg.n_relations\n",
    "        self.n_users_entities = rec.data_kg.n_users_entities\n",
    "        self.embed_dim = args['embed_dim']\n",
    "        self.relation_dim = args['relation_dim']\n",
    "\n",
    "        self.aggregation_type = args['aggregation_type']\n",
    "        self.conv_dim_list = [args['embed_dim']] + eval(args['conv_dim_list'])\n",
    "        self.mess_dropout = eval(args['mess_dropout'])\n",
    "        self.n_layers = len(eval(args['conv_dim_list']))\n",
    "\n",
    "        self.kg_l2loss_lambda = args['kg_l2loss_lambda']\n",
    "        self.cf_l2loss_lambda = args['cf_l2loss_lambda']\n",
    "\n",
    "        self.entity_user_embed = nn.Embedding(self.n_users_entities, self.embed_dim).cuda()\n",
    "        self.relation_embed = nn.Embedding(self.n_relations, self.relation_dim).cuda()\n",
    "        self.trans_M = nn.Parameter(torch.Tensor(self.n_relations, self.embed_dim, self.relation_dim)).cuda()\n",
    "\n",
    "        self.all_user_idx = list(rec.data_kg.userent.keys())\n",
    "        self.all_item_idx =  list(rec.data_kg.itement.keys())\n",
    "        \n",
    "        if (self.use_pretrain == 1) and (user_pre_embed is not None) and (item_pre_embed is not None):\n",
    "            other_entity_embed = nn.Parameter(torch.Tensor(self.n_entities - item_pre_embed.shape[0], self.embed_dim))\n",
    "            nn.init.xavier_uniform_(other_entity_embed)\n",
    "            entity_user_embed = torch.cat([item_pre_embed, other_entity_embed, user_pre_embed], dim=0)\n",
    "            self.entity_user_embed.weight = nn.Parameter(entity_user_embed)\n",
    "        else:\n",
    "            nn.init.xavier_uniform_(self.entity_user_embed.weight)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.relation_embed.weight)\n",
    "        nn.init.xavier_uniform_(self.trans_M)\n",
    "\n",
    "        self.aggregator_layers = nn.ModuleList()\n",
    "        for k in range(self.n_layers):\n",
    "            self.aggregator_layers.append(Aggregator(self.conv_dim_list[k], self.conv_dim_list[k + 1], self.mess_dropout[k], self.aggregation_type).cuda())\n",
    "\n",
    "        self.A_in = nn.Parameter(torch.sparse.FloatTensor(self.n_users_entities, self.n_users_entities))\n",
    "        if A_in is not None:\n",
    "            self.A_in.data = A_in \n",
    "        self.A_in.requires_grad = False\n",
    "\n",
    "    def calc_cf_embeddings(self):\n",
    "        ego_embed = self.entity_user_embed.weight\n",
    "        all_embed = [ego_embed]\n",
    "\n",
    "        for idx, layer in enumerate(self.aggregator_layers):\n",
    "            ego_embed = layer(ego_embed, self.A_in.cuda())\n",
    "            norm_embed = F.normalize(ego_embed, p=2, dim=1)\n",
    "            all_embed.append(norm_embed)\n",
    "\n",
    "        # Equation (11)\n",
    "        all_embed = torch.cat(all_embed, dim=1)         # (n_users + n_entities, concat_dim)\n",
    "        return all_embed\n",
    "\n",
    "    def calc_cf_loss(self, user_embed, item_pos_embed, item_neg_embed):\n",
    "        \"\"\"\n",
    "        user_ids:       (cf_batch_size)\n",
    "        item_pos_ids:   (cf_batch_size)\n",
    "        item_neg_ids:   (cf_batch_size)\n",
    "        \"\"\"\n",
    "        # Equation (12)\n",
    "        pos_score = torch.sum(user_embed * item_pos_embed, dim=1)   # (cf_batch_size)\n",
    "        neg_score = torch.sum(user_embed * item_neg_embed, dim=1)   # (cf_batch_size)\n",
    "\n",
    "        # Equation (13)\n",
    "        # cf_loss = F.softplus(neg_score - pos_score)\n",
    "        cf_loss = (-1.0) * F.logsigmoid(pos_score - neg_score)\n",
    "        cf_loss = torch.mean(cf_loss)\n",
    "\n",
    "        l2_loss = _L2_loss_mean(user_embed) + _L2_loss_mean(item_pos_embed) + _L2_loss_mean(item_neg_embed)\n",
    "        loss = cf_loss + self.cf_l2loss_lambda * l2_loss\n",
    "        return loss\n",
    "\n",
    "    def calc_kg_loss(self, h, r, pos_t, neg_t):\n",
    "        \"\"\"\n",
    "        h:      (kg_batch_size)\n",
    "        r:      (kg_batch_size)\n",
    "        pos_t:  (kg_batch_size)\n",
    "        neg_t:  (kg_batch_size)\n",
    "        \"\"\"\n",
    "        r_embed = self.relation_embed(r)                                                # (kg_batch_size, relation_dim)\n",
    "        W_r = self.trans_M[r]                                                           # (kg_batch_size, embed_dim, relation_dim)\n",
    "\n",
    "        h_embed = self.entity_user_embed(h)                                             # (kg_batch_size, embed_dim)\n",
    "        pos_t_embed = self.entity_user_embed(pos_t)                                     # (kg_batch_size, embed_dim)\n",
    "        neg_t_embed = self.entity_user_embed(neg_t)                                     # (kg_batch_size, embed_dim)\n",
    "\n",
    "        r_mul_h = torch.bmm(h_embed.unsqueeze(1), W_r).squeeze(1)                       # (kg_batch_size, relation_dim)\n",
    "        r_mul_pos_t = torch.bmm(pos_t_embed.unsqueeze(1), W_r).squeeze(1)               # (kg_batch_size, relation_dim)\n",
    "        r_mul_neg_t = torch.bmm(neg_t_embed.unsqueeze(1), W_r).squeeze(1)               # (kg_batch_size, relation_dim)\n",
    "\n",
    "        # Equation (1)\n",
    "        pos_score = torch.sum(torch.pow(r_mul_h + r_embed - r_mul_pos_t, 2), dim=1)     # (kg_batch_size)\n",
    "        neg_score = torch.sum(torch.pow(r_mul_h + r_embed - r_mul_neg_t, 2), dim=1)     # (kg_batch_size)\n",
    "\n",
    "        # Equation (2)\n",
    "        # kg_loss = F.softplus(pos_score - neg_score)\n",
    "        kg_loss = (-1.0) * F.logsigmoid(neg_score - pos_score)\n",
    "        kg_loss = torch.mean(kg_loss)\n",
    "\n",
    "        l2_loss = _L2_loss_mean(r_mul_h) + _L2_loss_mean(r_embed) + _L2_loss_mean(r_mul_pos_t) + _L2_loss_mean(r_mul_neg_t)\n",
    "        loss = kg_loss + self.kg_l2loss_lambda * l2_loss\n",
    "        return loss\n",
    "\n",
    "    def update_attention_batch(self, h_list, t_list, r_idx):\n",
    "        r_embed = self.relation_embed.weight[r_idx]\n",
    "        W_r = self.trans_M[r_idx]\n",
    "\n",
    "        h_embed = self.entity_user_embed.weight[h_list]\n",
    "        t_embed = self.entity_user_embed.weight[t_list]\n",
    "\n",
    "        # Equation (4)\n",
    "        r_mul_h = torch.matmul(h_embed, W_r)\n",
    "        r_mul_t = torch.matmul(t_embed, W_r)\n",
    "        v_list = torch.sum(r_mul_t * torch.tanh(r_mul_h + r_embed), dim=1)\n",
    "        return v_list\n",
    "\n",
    "    def update_attention(self, h_list, t_list, r_list, relations):\n",
    "        device = self.A_in.device\n",
    "\n",
    "        rows = []\n",
    "        cols = []\n",
    "        values = []\n",
    "\n",
    "        for r_idx in relations:\n",
    "            index_list = torch.where(r_list == r_idx)\n",
    "            batch_h_list = h_list[index_list]\n",
    "            batch_t_list = t_list[index_list]\n",
    "\n",
    "            batch_v_list = self.update_attention_batch(batch_h_list, batch_t_list, r_idx)\n",
    "            rows.append(batch_h_list)\n",
    "            cols.append(batch_t_list)\n",
    "            values.append(batch_v_list)\n",
    "\n",
    "        rows = torch.cat(rows)\n",
    "        cols = torch.cat(cols)\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        indices = torch.stack([rows, cols])\n",
    "        shape = self.A_in.shape\n",
    "        A_in = torch.sparse.FloatTensor(indices, values, torch.Size(shape))\n",
    "\n",
    "        # Equation (5)\n",
    "        A_in = torch.sparse.softmax(A_in.cpu(), dim=1)\n",
    "        self.A_in.data = A_in.to(device)\n",
    "\n",
    "    def calc_score(self, user_ids, item_ids):\n",
    "        \"\"\"\n",
    "        user_ids:  (n_users)\n",
    "        item_ids:  (n_items)\n",
    "        \"\"\"\n",
    "        all_embed = self.calc_cf_embeddings()           # (n_users + n_entities, concat_dim)\n",
    "        user_embed = all_embed[user_ids]                # (n_users, concat_dim)\n",
    "        item_embed = all_embed[item_ids]                # (n_items, concat_dim)\n",
    "\n",
    "        # Equation (12)\n",
    "        cf_score = torch.matmul(user_embed, item_embed.transpose(0, 1))    # (n_users, n_items)\n",
    "        return cf_score \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ac95e8-3eb4-49b0-87c5-c81ab3b2504d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55990a44-beea-4041-b23b-229be826bbab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime \n",
    "def next_batch_pairwise(rec, batch_size, n_negs=1):\n",
    "    training_data = rec.data.training_data\n",
    "    shuffle(training_data)\n",
    "    ptr = 0\n",
    "    data_size = len(training_data)\n",
    "    while ptr < data_size:\n",
    "        if ptr + batch_size < data_size:\n",
    "            batch_end = ptr + batch_size\n",
    "        else:   \n",
    "            batch_end = data_size\n",
    "        users = [training_data[idx][0] for idx in range(ptr, batch_end)]\n",
    "        items = [training_data[idx][1] for idx in range(ptr, batch_end)]\n",
    "        ptr = batch_end\n",
    "        u_idx, i_idx, j_idx = [], [], []\n",
    "        item_list = list(data.item.keys())\n",
    "        for i, user in enumerate(users):\n",
    "            i_idx.append(items[i])\n",
    "            u_idx.append(user)\n",
    "            for m in range(n_negs):\n",
    "                neg_item = choice(item_list)\n",
    "                while neg_item in data.training_set_u[user]:\n",
    "                    neg_item = choice(item_list)\n",
    "                j_idx.append(neg_item)\n",
    "\n",
    "        u_idx  = torch.LongTensor(u_idx).cuda()\n",
    "        i_idx  = torch.LongTensor(i_idx).cuda()\n",
    "        j_idx  = torch.LongTensor(j_idx).cuda()\n",
    "        yield u_idx, i_idx, j_idx\n",
    "    \n",
    "def next_batch_kg(kg_data, kg_dict, batch_size, n_negs=1):\n",
    "    ptr = 0\n",
    "    exist_heads= kg_dict.keys()\n",
    "    h_list = list(exist_heads)\n",
    "    h_dict = {value: idx for idx, value in enumerate(h_list)}\n",
    "    all_tails = list(set(kg_data[:,2]))\n",
    "    data_size = len(kg_data)\n",
    "    # Pre-compute positive tail sets and negative tails for each head\n",
    "    pos_tail_sets = {head: set([it[0] for it in tails]) for head, tails in kg_dict.items()}\n",
    "    # neg_tail_sets = {head: np.random.choice(list(all_tails - pos_tails), size=n_negs) for head, pos_tails in pos_tail_sets.items()}\n",
    "    \n",
    "    while ptr < data_size:\n",
    "        if ptr + batch_size < data_size:\n",
    "            batch_end = ptr + batch_size\n",
    "        else:   \n",
    "            batch_end = data_size\n",
    "        \n",
    "        heads, relations, tails = kg_data[ptr:batch_end, 0], kg_data[ptr:batch_end, 1], kg_data[ptr:batch_end, 2]\n",
    "        \n",
    "        ptr = batch_end\n",
    "        h_idx, r_idx, pos_t_idx, neg_t_idx = [], [], [], []\n",
    "        # time1 = datetime.datetime.now()\n",
    "        h_idx = [h_dict[head] for head in heads]\n",
    "        \n",
    "        r_idx.extend([int(rel) for rel in relations])\n",
    "        pos_t_idx.extend([tails])\n",
    "        for head in heads:\n",
    "            neg_t = random.choice(all_tails)\n",
    "            while neg_t in pos_tail_sets[head]:\n",
    "                neg_t = random.choice(all_tails)\n",
    "            neg_t_idx.append(int(h_dict[neg_t]))\n",
    "                \n",
    "        # time2 = datetime.datetime.now()\n",
    "        # time_difference = time2 - time1\n",
    "        # print(time_difference)\n",
    "\n",
    "        h_idx  = torch.LongTensor(h_idx).cuda()\n",
    "        r_idx  = torch.LongTensor(r_idx).cuda()\n",
    "        pos_t_idx  = torch.LongTensor(pos_t_idx).cuda()\n",
    "        neg_t_idx  = torch.LongTensor(neg_t_idx).cuda()\n",
    "        yield h_idx, r_idx, pos_t_idx, neg_t_idx\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60d4353-61de-4154-bfe9-2e2f6c46b1a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4a81a3f-94f9-4607-a288-21ed5bde7f54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _L2_loss_mean(x):\n",
    "    return torch.mean(torch.sum(torch.pow(x, 2), dim=1, keepdim=False) / 2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2a94f0-e377-4e3f-ae79-8f7500f4d621",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4038820d-8b17-40c9-ae1b-a5e5e7a6f7f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(train_model, rec, args):\n",
    "    # seed\n",
    "    random.seed(args['seed'])\n",
    "    np.random.seed(args['seed'])\n",
    "    torch.manual_seed(args['seed'])\n",
    "    torch.cuda.manual_seed_all(args['seed'])\n",
    "    \n",
    "    lst_train_losses = []\n",
    "    lst_rec_losses = []\n",
    "    lst_kg_losses = []\n",
    "    lst_performances = []\n",
    "    \n",
    "    reg = args['reg']\n",
    "    reg_kg = args['reg_kg']\n",
    "    alpha = args['alpha']\n",
    "    \n",
    "    \n",
    "    cf_optimizer  = torch.optim.Adam(train_model.parameters(), lr=lRate)\n",
    "    kg_optimizer = torch.optim.Adam(train_model.parameters(), lr=lRateKG)\n",
    "\n",
    "    kg_data = rec.data_kg.kg_train_data.to_numpy()\n",
    "    kg_dict = rec.data_kg.train_kg_dict\n",
    "        \n",
    "    for ep in range(maxEpoch):\n",
    "        train_model.train()\n",
    "        \n",
    "        train_losses = []\n",
    "        cf_losses = []\n",
    "        kg_losses = []\n",
    "        \n",
    "        cf_total_loss = 0\n",
    "        kg_total_loss = 0\n",
    "        \n",
    "        n_cf_batch = int(rec.data.n_cf_train // batchSize + 1)\n",
    "        n_kg_batch = int(rec.data_kg.n_kg_train // batchSizeKG + 1)\n",
    "        \n",
    "        shuffle(kg_data)\n",
    "        \n",
    "        # Learn cf graph\n",
    "        for n, batch in enumerate(next_batch_pairwise(rec, batchSize)):\n",
    "            user_idx, pos_idx, neg_idx = batch\n",
    "            entity_emb = train_model.calc_cf_embeddings()\n",
    "            \n",
    "            user_emb = entity_emb[user_idx]\n",
    "            pos_item_emb = entity_emb[pos_idx]\n",
    "            neg_item_emb = entity_emb[neg_idx]\n",
    "            \n",
    "            cf_batch_loss = train_model.calc_cf_loss(user_emb, pos_item_emb, neg_item_emb)\n",
    "            if np.isnan(cf_batch_loss.cpu().detach().numpy()):\n",
    "                print('ERROR (CF Training): Epoch {:04d} Iter {:04d} / {:04d} Loss is nan.'.format(ep, n, n_cf_batch))\n",
    "\n",
    "            cf_batch_loss.backward()\n",
    "            cf_optimizer.step()\n",
    "            cf_optimizer.zero_grad()\n",
    "            cf_total_loss += cf_batch_loss.item()\n",
    "            \n",
    "            cf_losses.append(cf_batch_loss.item())\n",
    "            if (n % 20) == 0:\n",
    "                print('CF Training: Epoch {:04d} Iter {:04d} / {:04d} | Iter Loss {:.4f} | Iter Mean Loss {:.4f}'.format(ep, n, n_cf_batch,  cf_batch_loss.item(), cf_total_loss / (n+1)))\n",
    "        \n",
    "        # Learn knowledge grap\n",
    "        for n, batch in enumerate(next_batch_kg(kg_data, kg_dict, batchSizeKG)):\n",
    "            kg_batch_head, kg_batch_relation, kg_batch_pos_tail, kg_batch_neg_tail = batch\n",
    "            \n",
    "            kg_batch_loss = train_model.calc_kg_loss(kg_batch_head, kg_batch_relation, kg_batch_pos_tail, kg_batch_neg_tail)\n",
    "            if np.isnan(kg_batch_loss.cpu().detach().numpy()):\n",
    "                print('ERROR (KG Training): Epoch {:04d} Iter {:04d} / {:04d} Loss is nan.'.format(ep, n, n_kg_batch))\n",
    "            kg_batch_loss.backward()\n",
    "            kg_optimizer.step()\n",
    "            kg_optimizer.zero_grad()\n",
    "            kg_total_loss += kg_batch_loss.item()\n",
    "            \n",
    "            kg_losses.append(kg_batch_loss.item())\n",
    "            if (n % 10) == 0:\n",
    "                print('KG Training: Epoch {:04d} Iter {:04d} / {:04d} | Iter Loss {:.4f} | Iter Mean Loss {:.4f}'.format(ep, n, n_kg_batch,  kg_batch_loss.item(), kg_total_loss / (n+1)))\n",
    "\n",
    "        # Learn attention \n",
    "        h_list = rec.data_kg.h_list.cuda()\n",
    "        t_list = rec.data_kg.t_list.cuda()\n",
    "        r_list = rec.data_kg.r_list.cuda()\n",
    "        relations = list(rec.data_kg.laplacian_dict.keys())\n",
    "        train_model.update_attention(h_list, t_list, r_list, relations)\n",
    "        train_model.eval()\n",
    "\n",
    "        cf_loss = np.mean(cf_losses)\n",
    "        kg_loss = np.mean(kg_losses)\n",
    "        train_loss = cf_loss + kg_loss\n",
    "\n",
    "        with torch.no_grad():\n",
    "            entity_emb = train_model.calc_cf_embeddings()\n",
    "            user_emb = entity_emb[train_model.user_indices]\n",
    "            item_emb = entity_emb[train_model.item_indices]\n",
    "            data_ep = rec.fast_evaluation(train_model, ep, user_emb, item_emb)\n",
    "        rec.save_performance_row(ep, data_ep)\n",
    "        rec.save_loss_row([ep, train_loss, cf_loss, kg_loss])\n",
    "    \n",
    "        lst_performances.append(data_ep)\n",
    "        lst_train_losses.append([ep, train_loss]) \n",
    "        lst_rec_losses.append([ep, cf_loss])\n",
    "        lst_kg_losses.append([ep, kg_loss])\n",
    "    rec.save_loss(lst_train_losses, lst_rec_losses, lst_kg_losses)\n",
    "    rec.save_perfomance_training(lst_performances)\n",
    "    user_emb, item_emb = rec.best_user_emb, rec.best_item_emb\n",
    "    return user_emb, item_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed034219-dbe5-407a-9111-873b9aa94e3e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e277edf0-c61c-49cf-ad9b-80f235ae808d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(rec, user_emb, item_emb):\n",
    "    def process_bar(num, total):\n",
    "        rate = float(num) / total\n",
    "        ratenum = int(50 * rate)\n",
    "        r = '\\rProgress: [{}{}]{}%'.format('+' * ratenum, ' ' * (50 - ratenum), ratenum*2)\n",
    "        sys.stdout.write(r)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    # predict\n",
    "    rec_list = {}\n",
    "    user_count = len(rec.data.test_set)\n",
    "    for i, user in enumerate(rec.data.test_set):\n",
    "        user_id = rec.data_kg.u2id[user]\n",
    "        score = torch.matmul(user_emb[user_id], item_emb.transpose(0, 1))\n",
    "        candidates = score.cpu().numpy()\n",
    "        \n",
    "        rated_list, li = rec.data.user_rated(user)\n",
    "        for item in rated_list:\n",
    "            candidates[rec.data_kg.i2id[item]] = -10e8\n",
    "        # s_find_k_largest = time.time()\n",
    "        ids, scores = find_k_largest(rec.max_N, candidates)\n",
    "\n",
    "        item_names = [rec.data_kg.id2i[iid] for iid in ids]\n",
    "        rec_list[user] = list(zip(item_names, scores))\n",
    "        if i % 1000 == 0:\n",
    "            process_bar(i, user_count)\n",
    "    process_bar(user_count, user_count)\n",
    "    print('')\n",
    "    rec.evaluate(rec_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d52b83-7ba1-4d77-9211-71a8a4932501",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a4b7819-e0e2-492b-b18a-9d2d2d105a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'KGAT'\n",
    "config = ModelConf('./conf/' + model + '.conf')\n",
    "lRates = [0.01]\n",
    "lRateKGs = [0.01]\n",
    "lrDecays = [0.9]\n",
    "maxEpochs = [500]\n",
    "batchSizes = [2048]\n",
    "batchSizeKGs = [8192]\n",
    "nLayers = [2]\n",
    "regs = [0.1]\n",
    "regkgs = [ 1e-5]\n",
    "embeddingSizes = [128]\n",
    "datasets = ['lastfm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "562c19b3-cdea-483e-9bcf-dc36ab74fd36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter ss_rate is not found in the configuration file!\n",
      "CF Training: Epoch 0000 Iter 0000 / 0034 | Iter Loss 1.1136 | Iter Mean Loss 1.1136\n",
      "CF Training: Epoch 0000 Iter 0020 / 0034 | Iter Loss 0.7253 | Iter Mean Loss 0.8166\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(weight_path):\n\u001b[1;32m     47\u001b[0m     train_model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(weight_path))\n\u001b[0;32m---> 48\u001b[0m user_emb, item_emb \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m   \n\u001b[1;32m     49\u001b[0m test(rec, user_emb, item_emb)\n",
      "Cell \u001b[0;32mIn[9], line 62\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_model, rec, args)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCF Training: Epoch \u001b[39m\u001b[38;5;132;01m{:04d}\u001b[39;00m\u001b[38;5;124m Iter \u001b[39m\u001b[38;5;132;01m{:04d}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{:04d}\u001b[39;00m\u001b[38;5;124m | Iter Loss \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m | Iter Mean Loss \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(ep, n, n_cf_batch,  cf_batch_loss\u001b[38;5;241m.\u001b[39mitem(), cf_total_loss \u001b[38;5;241m/\u001b[39m (n\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)))\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Learn knowledge grap\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(next_batch_kg(kg_data, kg_dict, batchSizeKG)):\n\u001b[1;32m     63\u001b[0m     kg_batch_head, kg_batch_relation, kg_batch_pos_tail, kg_batch_neg_tail \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     65\u001b[0m     kg_batch_loss \u001b[38;5;241m=\u001b[39m train_model\u001b[38;5;241m.\u001b[39mcalc_kg_loss(kg_batch_head, kg_batch_relation, kg_batch_pos_tail, kg_batch_neg_tail)\n",
      "Cell \u001b[0;32mIn[17], line 69\u001b[0m, in \u001b[0;36mnext_batch_kg\u001b[0;34m(kg_data, kg_dict, batch_size, n_negs)\u001b[0m\n\u001b[1;32m     67\u001b[0m h_idx  \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(h_idx)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     68\u001b[0m r_idx  \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(r_idx)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m---> 69\u001b[0m pos_t_idx  \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLongTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos_t_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     70\u001b[0m neg_t_idx  \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(neg_t_idx)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m h_idx, r_idx, pos_t_idx, neg_t_idx\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "hyperparameters = [lRates, lRateKGs, lrDecays, maxEpochs, batchSizes, batchSizeKGs, nLayers, regs, regkgs, embeddingSizes, datasets]\n",
    "\n",
    "for params in product(*hyperparameters):\n",
    "    lRate, lRateKG, lrDecay, maxEpoch, batchSize, batchSizeKG, nLayer, reg, reg_kg, embeddingSize, dataset = params\n",
    "    args = {\n",
    "        'lr': lRate,\n",
    "        'lr_kg': lRateKG,\n",
    "        'max_epoch': maxEpoch,\n",
    "        'batch_size': batchSize, \n",
    "        'lr_decay': lrDecay,\n",
    "        'dataset': dataset,\n",
    "        'n_layers': nLayer,\n",
    "        'use_pretrain': 0,\n",
    "        'input_dim': 32,\n",
    "        'embed_dim': embeddingSize,\n",
    "        'relation_dim': 32,\n",
    "        'reg': 1e-5,\n",
    "        'reg_kg': 1e-5,\n",
    "        'aggregation_type': 'bi-interaction',\n",
    "        'mess_dropout': '[0.1, 0.1, 0.1]',\n",
    "        'conv_dim_list': '[64, 32, 16]',\n",
    "        'kg_l2loss_lambda': reg_kg,\n",
    "        'cf_l2loss_lambda': reg,\n",
    "        'seed': 123,\n",
    "        'alpha': 0.1\n",
    "    }\n",
    "    args['output_path'] =  f\"./results/KGAT/{dataset}/@KGAT-inp_emb:{args['input_dim']}-emb:{args['embed_dim']}-bs:{args['batch_size']}-lr:{args['lr']}-lr_kg:{args['lr_kg']}-n_layers:{args['n_layers']}/\"\n",
    "    if not os.path.exists(args['output_path']):\n",
    "        os.makedirs(args['output_path'])\n",
    "\n",
    "    current_time = strftime(\"%Y-%m-%d\", localtime(time.time()))\n",
    "    file_name =  config['model.name'] + '@' + current_time + '-weight' + '.pth'\n",
    "    weight_path = args['output_path'] + file_name \n",
    "    # data\n",
    "    # training_data = FileIO.load_data_set('./dataset/' + dataset + '/' +config['training.set'], config['model.type'])\n",
    "    # test_data = FileIO.load_data_set('./dataset/' + dataset + '/'  +config['test.set'], config['model.type'])\n",
    "    # knowledge_set = FileIO.load_kg_data('./dataset/' + dataset +'/'+ dataset +'.kg')\n",
    "    # data = Interaction(config, training_data, test_data)\n",
    "    # data_kg = Knowledge(config, training_data, test_data, knowledge_set)\n",
    "    # rec \n",
    "    rec = GraphRecommender(config, data, data_kg, knowledge_set, **args)\n",
    "    \n",
    "    A_in = TorchGraphInterface.convert_sparse_mat_to_tensor(rec.data_kg.kg_interaction_mat).cuda()\n",
    "    \n",
    "    train_model = KGAT(args, rec, A_in=A_in)\n",
    "    if os.path.exists(weight_path):\n",
    "        train_model.load_state_dict(torch.load(weight_path))\n",
    "    user_emb, item_emb = train(train_model, rec, args)   \n",
    "    test(rec, user_emb, item_emb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hungvv",
   "language": "python",
   "name": "hungvv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
