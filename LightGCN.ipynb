{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.init import xavier_normal_, xavier_uniform_\n",
    "import argparse\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "from os.path import abspath\n",
    "\n",
    "from base.graph_recommender import GraphRecommender\n",
    "from util.sampler import next_batch_pairwise_kg, next_batch_pairwise\n",
    "from util.conf import OptionConf\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from scipy.sparse import coo_matrix\n",
    "from util.loss_torch import bpr_loss, l2_reg_loss, EmbLoss, contrastLoss\n",
    "from util.init import *\n",
    "from base.torch_interface import TorchGraphInterface\n",
    "import os\n",
    "import numpy as np \n",
    "import time \n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from data.loader import FileIO\n",
    "from util.conf import ModelConf\n",
    "from base.recommender import Recommender\n",
    "from data.ui_graph import Interaction\n",
    "# from data.knowledge import Knowledge\n",
    "from util.algorithm import find_k_largest\n",
    "from time import strftime, localtime\n",
    "from data.loader import FileIO\n",
    "from util.evaluation import ranking_evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphRecommender(Recommender):\n",
    "    def __init__(self, conf, training_set, test_set, **kwargs):\n",
    "        super(GraphRecommender, self).__init__(conf, training_set, test_set, **kwargs)\n",
    "        self.data = Interaction(conf, training_set, test_set)\n",
    "        self.bestPerformance = []\n",
    "        top = self.ranking['-topN'].split(',')\n",
    "        self.topN = [int(num) for num in top]\n",
    "        self.max_N = max(self.topN)\n",
    "\n",
    "    def print_model_info(self):\n",
    "        super(GraphRecommender, self).print_model_info()\n",
    "        # # print dataset statistics\n",
    "        print('Training Set Size: (user number: %d, item number %d, interaction number: %d)' % (self.data.training_size()))\n",
    "        print('Test Set Size: (user number: %d, item number %d, interaction number: %d)' % (self.data.test_size()))\n",
    "        print('=' * 80)\n",
    "\n",
    "    def build(self):\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        pass\n",
    "\n",
    "    def predict(self, u):\n",
    "        pass\n",
    "\n",
    "    def test(self, user_emb, item_emb):\n",
    "        def process_bar(num, total):\n",
    "            rate = float(num) / total\n",
    "            ratenum = int(50 * rate)\n",
    "            r = '\\rProgress: [{}{}]{}%'.format('+' * ratenum, ' ' * (50 - ratenum), ratenum*2)\n",
    "            sys.stdout.write(r)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        # predict\n",
    "        rec_list = {}\n",
    "        user_count = len(self.data.test_set)\n",
    "        for i, user in enumerate(self.data.test_set):\n",
    "            # s_find_candidates = time.time()\n",
    "            \n",
    "            \n",
    "            # candidates = predict(user)\n",
    "            user_id  = self.data.get_user_id(user)\n",
    "            score = torch.matmul(user_emb[user_id], item_emb.transpose(0, 1))\n",
    "            candidates = score.cpu().numpy()\n",
    "            \n",
    "            # e_find_candidates = time.time()\n",
    "            # print(\"Calculate candidates time: %f s\" % (e_find_candidates - s_find_candidates))\n",
    "            # predictedItems = denormalize(predictedItems, self.data.rScale[-1], self.data.rScale[0])\n",
    "            rated_list, li = self.data.user_rated(user)\n",
    "            for item in rated_list:\n",
    "                candidates[self.data.item[item]] = -10e8\n",
    "            \n",
    "            # s_find_k_largest = time.time()\n",
    "            ids, scores = find_k_largest(self.max_N, candidates)\n",
    "            # e_find_k_largest = time.time()\n",
    "            # print(\"Find k largest candidates: %f s\" % (e_find_k_largest - s_find_k_largest))\n",
    "            item_names = [self.data.id2item[iid] for iid in ids]\n",
    "            rec_list[user] = list(zip(item_names, scores))\n",
    "            if i % 1000 == 0:\n",
    "                process_bar(i, user_count)\n",
    "        process_bar(user_count, user_count)\n",
    "        print('')\n",
    "        return rec_list\n",
    "\n",
    "    def evaluate(self, rec_list):\n",
    "        self.recOutput.append('userId: recommendations in (itemId, ranking score) pairs, * means the item is hit.\\n')\n",
    "        for user in self.data.test_set:\n",
    "            line = str(user) + ':'\n",
    "            for item in rec_list[user]:\n",
    "                line += ' (' + str(item[0]) + ',' + str(item[1]) + ')'\n",
    "                if item[0] in self.data.test_set[user]:\n",
    "                    line += '*'\n",
    "            line += '\\n'\n",
    "            self.recOutput.append(line)\n",
    "        current_time = strftime(\"%Y-%m-%d %H-%M-%S\", localtime(time.time()))\n",
    "        # output prediction result\n",
    "        out_dir = self.output['-dir']\n",
    "        file_name = self.config['model.name'] + '@' + current_time + '-top-' + str(self.max_N) + 'items' + '.txt'\n",
    "        FileIO.write_file(out_dir, file_name, self.recOutput)\n",
    "        print('The result has been output to ', abspath(out_dir), '.')\n",
    "        file_name = self.config['model.name'] + '@' + current_time + '-performance' + '.txt'\n",
    "        self.result = ranking_evaluation(self.data.test_set, rec_list, self.topN)\n",
    "        self.model_log.add('###Evaluation Results###')\n",
    "        self.model_log.add(self.result)\n",
    "        FileIO.write_file(out_dir, file_name, self.result)\n",
    "        print('The result of %s:\\n%s' % (self.model_name, ''.join(self.result)))\n",
    "\n",
    "    def fast_evaluation(self, epoch, user_embed, item_embed, kwargs=None):\n",
    "        print('Evaluating the model...')\n",
    "        s_test = time.time()\n",
    "        rec_list = self.test(user_embed, item_embed)\n",
    "        e_test = time.time() \n",
    "        print(\"Test time: %f s\" % (e_test - s_test))\n",
    "        \n",
    "        s_measure = time.time()\n",
    "        measure = ranking_evaluation(self.data.test_set, rec_list, [self.max_N])\n",
    "        e_measure = time.time()\n",
    "        print(\"Measure time: %f s\" % (e_measure - s_measure))\n",
    "        \n",
    "        if len(self.bestPerformance) > 0:\n",
    "            count = 0\n",
    "            performance = {}\n",
    "            for m in measure[1:]:\n",
    "                k, v = m.strip().split(':')\n",
    "                performance[k] = float(v)\n",
    "            for k in self.bestPerformance[1]:\n",
    "                if self.bestPerformance[1][k] > performance[k]:\n",
    "                    count += 1\n",
    "                else:\n",
    "                    count -= 1\n",
    "            if count < 0:\n",
    "                self.bestPerformance[1] = performance\n",
    "                self.bestPerformance[0] = epoch + 1\n",
    "                try:\n",
    "                    self.save(kwargs)\n",
    "                except:\n",
    "                    self.save()\n",
    "        else:\n",
    "            self.bestPerformance.append(epoch + 1)\n",
    "            performance = {}\n",
    "            for m in measure[1:]:\n",
    "                k, v = m.strip().split(':')\n",
    "                performance[k] = float(v)\n",
    "            self.bestPerformance.append(performance)\n",
    "            try:\n",
    "                self.save(kwargs)\n",
    "            except:\n",
    "                self.save()\n",
    "        print('-' * 120)\n",
    "        print('Real-Time Ranking Performance ' + ' (Top-' + str(self.max_N) + ' Item Recommendation)')\n",
    "        measure = [m.strip() for m in measure[1:]]\n",
    "        print('*Current Performance*')\n",
    "        print('Epoch:', str(epoch + 1) + ',', '  |  '.join(measure))\n",
    "        bp = ''\n",
    "        # for k in self.bestPerformance[1]:\n",
    "        #     bp+=k+':'+str(self.bestPerformance[1][k])+' | '\n",
    "        bp += 'Hit Ratio' + ':' + str(self.bestPerformance[1]['Hit Ratio']) + '  |  '\n",
    "        bp += 'Precision' + ':' + str(self.bestPerformance[1]['Precision']) + '  |  '\n",
    "        bp += 'Recall' + ':' + str(self.bestPerformance[1]['Recall']) + '  |  '\n",
    "        # bp += 'F1' + ':' + str(self.bestPerformance[1]['F1']) + ' | '\n",
    "        bp += 'NDCG' + ':' + str(self.bestPerformance[1]['NDCG'])\n",
    "        print('*Best Performance* ')\n",
    "        print('Epoch:fast_evaluation', str(self.bestPerformance[0]) + ',', bp)\n",
    "        print('-' * 120)\n",
    "        return measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LGCN_Encoder(nn.Module):\n",
    "    def __init__(self, data, emb_size, n_layers):\n",
    "        super(LGCN_Encoder, self).__init__()\n",
    "        self.data = data\n",
    "        self.latent_size = emb_size\n",
    "        self.layers = n_layers\n",
    "        self.norm_adj = data.norm_adj\n",
    "        self.embedding_dict = self._init_model()\n",
    "        self.sparse_norm_adj = TorchGraphInterface.convert_sparse_mat_to_tensor(self.norm_adj).to(device)\n",
    "\n",
    "    def _init_model(self):\n",
    "        initializer = nn.init.xavier_uniform_\n",
    "        embedding_dict = nn.ParameterDict({\n",
    "            'user_emb': nn.Parameter(initializer(torch.empty(self.data.user_num, self.latent_size)).to(device)),\n",
    "            'item_emb': nn.Parameter(initializer(torch.empty(self.data.item_num, self.latent_size)).to(device)),\n",
    "        })\n",
    "        return embedding_dict\n",
    "\n",
    "    def forward(self):\n",
    "        ego_embeddings = torch.cat([self.embedding_dict['user_emb'], self.embedding_dict['item_emb']], 0)\n",
    "        all_embeddings = [ego_embeddings]\n",
    "        for k in range(self.layers):\n",
    "            ego_embeddings = torch.sparse.mm(self.sparse_norm_adj, ego_embeddings)\n",
    "            all_embeddings += [ego_embeddings]\n",
    "        all_embeddings = torch.stack(all_embeddings, dim=1)\n",
    "        all_embeddings = torch.mean(all_embeddings, dim=1)\n",
    "        user_all_embeddings = all_embeddings[:self.data.user_num]\n",
    "        item_all_embeddings = all_embeddings[self.data.user_num:]\n",
    "        return user_all_embeddings, item_all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(anchor_emb, pos_emb, neg_emb, batch_size):\n",
    "    calc_reg_loss = EmbLoss()\n",
    "    rec_loss = bpr_loss(anchor_emb, pos_emb, neg_emb)\n",
    "    reg_loss = reg * calc_reg_loss(anchor_emb, pos_emb, neg_emb) / batch_size\n",
    "    return rec_loss, reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, u):\n",
    "    user_id  = self.data.get_user_id(u)\n",
    "    score = torch.matmul(self.user_emb[user_id], self.item_emb.transpose(0, 1))\n",
    "    return score.cpu().numpy()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'input_dim': 32,\n",
    "    'hyper_dim': 64,\n",
    "    'p': 0.1,\n",
    "    'drop_rate': 0.2,\n",
    "    'n_layers': 2\n",
    "}\n",
    "model = 'LightGCN'\n",
    "config = ModelConf('./conf/' + model + '.conf')\n",
    "\n",
    "lRate = float(config['learnRate'])\n",
    "maxEpoch = int(config['num.max.epoch'])\n",
    "batchSize = int(config['batch_size'])\n",
    "reg = float(config['reg.lambda'])\n",
    "embeddingSize = int(config['embedding.size'])\n",
    "# ss_rate = float(config['ss_rate'])\n",
    "\n",
    "training_data = FileIO.load_data_set(config['training.set'], config['model.type'])\n",
    "test_data = FileIO.load_data_set(config['test.set'], config['model.type'])\n",
    "\n",
    "rec = GraphRecommender(config, training_data, test_data, **args)\n",
    "train_model = LGCN_Encoder(rec.data, embeddingSize, args['n_layers'])\n",
    "optimizer  = torch.optim.Adam(train_model.parameters(), weight_decay=1e-3, lr=lRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in range(maxEpoch):\n",
    "    train_losses = [] \n",
    "    \n",
    "    for n, batch in enumerate(next_batch_pairwise(rec.data, batchSize)):\n",
    "        user_idx, pos_idx, neg_idx = batch\n",
    "        train_model.train()\n",
    "        user_emb, item_emb = train_model()\n",
    "        \n",
    "        anchor_emb = user_emb[user_idx]\n",
    "        pos_emb = item_emb[pos_idx]\n",
    "        neg_emb = item_emb[neg_idx]\n",
    "        \n",
    "        loss_rec, loss_reg = calculate_loss(anchor_emb, pos_emb, neg_emb, batchSize)\n",
    "#         print(f\"Loss rec: {loss_rec}; Loss reg: {loss_reg}\")\n",
    "        batch_loss = loss_rec + loss_reg \n",
    "        \n",
    "#         print(f\"Batch loss: {batch_loss}\")\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(train_model.parameters(), 4)\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Evaluation\n",
    "    train_model.eval()\n",
    "    with torch.no_grad():\n",
    "        user_emb, item_emb = train_model()\n",
    "    rec.fast_evaluation(ep, user_emb, item_emb)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
