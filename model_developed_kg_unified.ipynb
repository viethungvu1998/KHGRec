{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afbc852c-cdc2-4c48-9ebe-82b2076a5b5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jun/anaconda3/envs/hungvv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.init import xavier_normal_, xavier_uniform_\n",
    "import argparse\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "from os.path import abspath\n",
    "import random\n",
    "import collections  \n",
    "from collections import defaultdict\n",
    "import scipy.sparse as sp\n",
    "from itertools import product\n",
    "from random import shuffle,randint,choice,sample\n",
    "import torch.nn.init as init \n",
    "import csv \n",
    "\n",
    "from util.conf import OptionConf\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse.linalg import eigs\n",
    "from util.loss_torch import bpr_loss, l2_reg_loss, EmbLoss, contrastLoss\n",
    "from util.init import *\n",
    "from base.torch_interface import TorchGraphInterface\n",
    "import os\n",
    "import numpy as np \n",
    "import time \n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from util.conf import ModelConf\n",
    "from base.recommender import Recommender\n",
    "from util.algorithm import find_k_largest\n",
    "from time import strftime, localtime\n",
    "from data.loader import FileIO\n",
    "from util.evaluation import ranking_evaluation\n",
    "\n",
    "from data.data import Data\n",
    "from data.graph import Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a243349-ffeb-4ef0-a4d6-362b54059c1b",
   "metadata": {},
   "source": [
    "## Graph Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd3dade5-626a-4e05-bbb6-22f268ef5e33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GraphRecommender(Recommender):\n",
    "    def __init__(self, conf, data, data_kg, knowledge_set, **kwargs):\n",
    "        super(GraphRecommender, self).__init__(conf, data, data_kg, knowledge_set,**kwargs)\n",
    "        self.data = data\n",
    "        self.data_kg = data_kg\n",
    "        self.bestPerformance = []\n",
    "        top = self.ranking['-topN'].split(',')\n",
    "        self.topN = [int(num) for num in top]\n",
    "        self.max_N = max(self.topN)\n",
    "        \n",
    "        self.output_path = kwargs['output_path']\n",
    "        if not os.path.exists(self.output_path):\n",
    "            os.makedirs(self.output_path)\n",
    "            \n",
    "    def print_model_info(self):\n",
    "        super(GraphRecommender, self).print_model_info()\n",
    "        # # print dataset statistics\n",
    "        print('Training Set Size: (user number: %d, item number %d, interaction number: %d)' % (self.data.training_size()))\n",
    "        print('Test Set Size: (user number: %d, item number %d, interaction number: %d)' % (self.data.test_size()))\n",
    "        print('=' * 80)\n",
    "\n",
    "    def build(self):\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        pass\n",
    "\n",
    "    def predict(self, u):\n",
    "        pass\n",
    "\n",
    "    def test(self, user_emb, item_emb):\n",
    "        def process_bar(num, total):\n",
    "            rate = float(num) / total\n",
    "            ratenum = int(50 * rate)\n",
    "            r = '\\rProgress: [{}{}]{}%'.format('+' * ratenum, ' ' * (50 - ratenum), ratenum*2)\n",
    "            sys.stdout.write(r)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        # predict\n",
    "        rec_list = {}\n",
    "        user_count = len(self.data.test_set)\n",
    "        lst_users =  list(self.data_kg.userent.keys())\n",
    "        lst_items =  list(self.data_kg.itement.keys())\n",
    "        \n",
    "        for i, user in enumerate(self.data.test_set):\n",
    "            user_id  = lst_users.index(user)\n",
    "            score = torch.matmul(user_emb[user_id], item_emb.transpose(0, 1))\n",
    "            candidates = score.cpu().numpy()\n",
    "            \n",
    "            # e_find_candidates = time.time()\n",
    "            # print(\"Calculate candidates time: %f s\" % (e_find_candidates - s_find_candidates))\n",
    "            # predictedItems = denormalize(predictedItems, self.data.rScale[-1], self.data.rScale[0])\n",
    "            rated_list, li = self.data.user_rated(user)\n",
    "            for item in rated_list:\n",
    "                candidates[lst_items.index(item)] = -10e8\n",
    "            # s_find_k_largest = time.time()\n",
    "            ids, scores = find_k_largest(self.max_N, candidates)\n",
    "            # e_find_k_largest = time.time()\n",
    "            # print(\"Find k largest candidates: %f s\" % (e_find_k_largest - s_find_k_largest))\n",
    "            item_names = [lst_items[iid] for iid in ids]\n",
    "            rec_list[user] = list(zip(item_names, scores))\n",
    "            if i % 1000 == 0:\n",
    "                process_bar(i, user_count)\n",
    "        process_bar(user_count, user_count)\n",
    "        print('')\n",
    "        return rec_list\n",
    "    \n",
    "    def evaluate(self, rec_list):\n",
    "        self.recOutput.append('userId: recommendations in (itemId, ranking score) pairs, * means the item is hit.\\n')\n",
    "        for user in self.data.test_set:\n",
    "            line = str(user) + ':'\n",
    "            for item in rec_list[user]:\n",
    "                line += ' (' + str(item[0]) + ',' + str(item[1]) + ')'\n",
    "                if item[0] in self.data.test_set[user]:\n",
    "                    line += '*'\n",
    "            line += '\\n'\n",
    "            self.recOutput.append(line)\n",
    "        current_time = strftime(\"%Y-%m-%d %H-%M-%S\", localtime(time.time()))\n",
    "        # output prediction result\n",
    "        out_dir = self.output_path\n",
    "        file_name = self.config['model.name'] + '@' + current_time + '-top-' + str(self.max_N) + 'items' + '.txt'\n",
    "        FileIO.write_file(out_dir, file_name, self.recOutput)\n",
    "        print('The result has been output to ', abspath(out_dir), '.')\n",
    "        file_name = self.config['model.name'] + '@' + current_time + '-performance' + '.txt'\n",
    "        self.result = ranking_evaluation(self.data.test_set, rec_list, self.topN)\n",
    "        self.model_log.add('###Evaluation Results###')\n",
    "        self.model_log.add(self.result)\n",
    "        FileIO.write_file(out_dir, file_name, self.result)\n",
    "        print('The result of %s:\\n%s' % (self.model_name, ''.join(self.result)))\n",
    "\n",
    "    def fast_evaluation(self, model, epoch, user_embed, item_embed, kwargs=None):\n",
    "        print('Evaluating the model...')\n",
    "        s_test = time.time()\n",
    "        rec_list = self.test(user_embed, item_embed)\n",
    "        e_test = time.time() \n",
    "        print(\"Test time: %f s\" % (e_test - s_test))\n",
    "        \n",
    "        s_measure = time.time()\n",
    "        measure = ranking_evaluation(self.data.test_set, rec_list, [self.max_N])\n",
    "        e_measure = time.time()\n",
    "        print(\"Measure time: %f s\" % (e_measure - s_measure))\n",
    "        \n",
    "        if len(self.bestPerformance) > 0:\n",
    "            count = 0\n",
    "            performance = {}\n",
    "            for m in measure[1:]:\n",
    "                k, v = m.strip().split(':')\n",
    "                performance[k] = float(v)\n",
    "            for k in self.bestPerformance[1]:\n",
    "                if self.bestPerformance[1][k] > performance[k]:\n",
    "                    count += 1\n",
    "                else:\n",
    "                    count -= 1\n",
    "            if count < 0:\n",
    "                self.bestPerformance[1] = performance\n",
    "                self.bestPerformance[0] = epoch + 1\n",
    "                # try:\n",
    "                #     self.save(kwargs)\n",
    "                # except:\n",
    "                self.save(model)\n",
    "        else:\n",
    "            self.bestPerformance.append(epoch + 1)\n",
    "            performance = {}\n",
    "            for m in measure[1:]:\n",
    "                k, v = m.strip().split(':')\n",
    "                performance[k] = float(v)\n",
    "            self.bestPerformance.append(performance)\n",
    "            # try:\n",
    "            #     self.save(kwargs)\n",
    "            # except:\n",
    "            self.save(model)\n",
    "        print('-' * 120)\n",
    "        print('Real-Time Ranking Performance ' + ' (Top-' + str(self.max_N) + ' Item Recommendation)')\n",
    "        measure = [m.strip() for m in measure[1:]]\n",
    "        print('*Current Performance*')\n",
    "        print('Epoch:', str(epoch + 1) + ',', '  |  '.join(measure))\n",
    "        bp = ''\n",
    "        # for k in self.bestPerformance[1]:\n",
    "        #     bp+=k+':'+str(self.bestPerformance[1][k])+' | '\n",
    "        bp += 'Hit Ratio' + ':' + str(self.bestPerformance[1]['Hit Ratio']) + '  |  '\n",
    "        bp += 'Precision' + ':' + str(self.bestPerformance[1]['Precision']) + '  |  '\n",
    "        bp += 'Recall' + ':' + str(self.bestPerformance[1]['Recall']) + '  |  '\n",
    "        # bp += 'F1' + ':' + str(self.bestPerformance[1]['F1']) + ' | '\n",
    "        bp += 'NDCG' + ':' + str(self.bestPerformance[1]['NDCG'])\n",
    "        print('*Best Performance* ')\n",
    "        print('Epoch:fast_evaluation', str(self.bestPerformance[0]) + ',', bp)\n",
    "        print('-' * 120)\n",
    "        return measure\n",
    "    \n",
    "    def save(self, model):\n",
    "        with torch.no_grad():\n",
    "            ego_emb =  model.calculate_cf_embedding()\n",
    "            user_emb = ego_emb[list(rec.data_kg.userent.keys())]\n",
    "            item_emb = ego_emb[list(rec.data_kg.itement.keys())]\n",
    "            self.best_user_emb, self.best_item_emb = user_emb, item_emb\n",
    "        self.save_model(model)\n",
    "    \n",
    "    def save_model(self, model):\n",
    "        # save model \n",
    "        current_time = strftime(\"%Y-%m-%d\", localtime(time.time()))\n",
    "        out_dir = self.output_path\n",
    "        file_name =  self.config['model.name'] + '@' + current_time + '-weight' + '.pth'\n",
    "        weight_file = out_dir + '/' + file_name \n",
    "        torch.save(model.state_dict(), weight_file)\n",
    "\n",
    "\n",
    "    def save_performance_row(self, ep, data_ep):\n",
    "        # opening the csv file in 'w' mode\n",
    "        csv_path = self.output_path + 'train_performance.csv'\n",
    "        \n",
    "        # 'Hit Ratio:0.00328', 'Precision:0.00202', 'Recall:0.00337', 'NDCG:0.00292\n",
    "        hit = float(data_ep[0].split(':')[1])\n",
    "        precision = float(data_ep[1].split(':')[1])\n",
    "        recall = float(data_ep[2].split(':')[1])\n",
    "        ndcg = float(data_ep[3].split(':')[1])\n",
    "        \n",
    "        with open(csv_path, 'a+', newline = '') as f:\n",
    "            header = ['ep', 'hit@20', 'prec@20', 'recall@20', 'ndcg@20']\n",
    "            writer = csv.DictWriter(f, fieldnames = header)\n",
    "            # writer.writeheader()\n",
    "            writer.writerow({\n",
    "                 'ep' : ep,\n",
    "                 'hit@20': hit,\n",
    "                 'prec@20': precision,\n",
    "                 'recall@20': recall,\n",
    "                 'ndcg@20': ndcg,\n",
    "            })\n",
    "            \n",
    "    def save_loss_row(self, data_ep):\n",
    "        csv_path = self.output_path + 'loss.csv'\n",
    "        with open(csv_path, 'a+', newline ='') as f:\n",
    "            header = ['ep', 'train_loss', 'cf_loss', 'kg_loss']\n",
    "            writer = csv.DictWriter(f, fieldnames = header)\n",
    "            # writer.writeheader()\n",
    "            writer.writerow({\n",
    "                'ep' : data_ep[0],\n",
    "                'train_loss': data_ep[1],\n",
    "                 'cf_loss': data_ep[2],\n",
    "                 'kg_loss': data_ep[3]\n",
    "            })\n",
    "\n",
    "    def save_loss(self, train_losses, rec_losses, kg_losses):\n",
    "        df_train_loss = pd.DataFrame(train_losses, columns = ['ep', 'loss'])\n",
    "        df_rec_loss = pd.DataFrame(rec_losses, columns = ['ep', 'loss'])\n",
    "        df_kg_loss = pd.DataFrame(kg_losses, columns = ['ep', 'loss'])\n",
    "        df_train_loss.to_csv(self.output_path + '/train_loss.csv')\n",
    "        df_rec_loss.to_csv(self.output_path + '/rec_loss.csv')\n",
    "        df_kg_loss.to_csv(self.output_path + '/kg_loss.csv')\n",
    "    \n",
    "    def save_perfomance_training(self, log_train):\n",
    "        df_train_log = pd.DataFrame(log_train)\n",
    "        df_train_log.to_csv(self.output_path + '/train_performance.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62daafda-3820-49bb-b862-c76b720d0b32",
   "metadata": {},
   "source": [
    "## Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a0c5817-6093-4b16-8dc7-652683f7dbc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Interaction(Data, Graph):\n",
    "    def __init__(self, conf, training, test):\n",
    "        self.conf = conf \n",
    "        Graph.__init__(self)\n",
    "        Data.__init__(self,conf,training,test)\n",
    "\n",
    "        self.user = {}\n",
    "        self.item = {}\n",
    "        self.id2user = {}\n",
    "        self.id2item = {}\n",
    "        self.training_set_u = defaultdict(dict)\n",
    "        self.training_set_i = defaultdict(dict)\n",
    "        self.test_set = defaultdict(dict)\n",
    "        self.user_history_dict = defaultdict(dict)\n",
    "\n",
    "        self.test_set_item = set()\n",
    "        self.__generate_set()\n",
    "\n",
    "        self.n_users = len(self.training_set_u)\n",
    "        self.n_items = len(self.training_set_i) \n",
    "\n",
    "        self.n_cf_train = len(self.training_data)\n",
    "        self.n_cf_test = len(self.test_data)\n",
    "\n",
    "        # self.ui_adj = self.__create_sparse_bipartite_adjacency()\n",
    "        # self.norm_adj = self.normalize_graph_mat(self.ui_adj)\n",
    "        # self.interaction_mat, self.inv_interaction_mat = self.__create_sparse_interaction_matrix()\n",
    "        \n",
    "    def __generate_set(self):\n",
    "        for entry in self.training_data:\n",
    "            user, item, rating = entry\n",
    "            user, item = int(user), int(item)\n",
    "            if user not in self.user:\n",
    "                self.user[user] = len(self.user)\n",
    "                self.id2user[self.user[user]] = user\n",
    "            if item not in self.item:\n",
    "                self.item[item] = len(self.item)\n",
    "                self.id2item[self.item[item]] = item\n",
    "                # userList.append\n",
    "            # construct user_history_dict \n",
    "            if rating == 1.0:\n",
    "                if user not in self.user_history_dict:\n",
    "                    self.user_history_dict[user] = []\n",
    "                self.user_history_dict[user].append(item)\n",
    "            \n",
    "            self.training_set_u[user][item] = rating\n",
    "            self.training_set_i[item][user] = rating\n",
    "        \n",
    "        for entry in self.test_data:\n",
    "            user, item, rating = entry\n",
    "            if user not in self.user:\n",
    "                continue\n",
    "            self.test_set[user][item] = rating\n",
    "            self.test_set_item.add(item)\n",
    "\n",
    "    def __create_sparse_bipartite_adjacency(self, self_connection=False):\n",
    "        '''\n",
    "        return a sparse adjacency matrix with the shape (user number + item number, user number + item number)\n",
    "        '''\n",
    "        n_nodes = self.n_users + self.n_items\n",
    "        row_idx = [int(pair[0]) for pair in self.training_data]\n",
    "        col_idx = [int(pair[1]) for pair in self.training_data]\n",
    "        user_np = np.array(row_idx)\n",
    "        item_np = np.array(col_idx)\n",
    "        ratings = np.ones_like(user_np, dtype=np.float32)\n",
    "        tmp_adj = sp.csr_matrix((ratings, (user_np, item_np + self.n_users)), shape=(n_nodes, n_nodes),dtype=np.float32)\n",
    "        adj_mat = tmp_adj + tmp_adj.T\n",
    "        if self_connection:\n",
    "            adj_mat += sp.eye(n_nodes)\n",
    "        return adj_mat\n",
    "    \n",
    "    def __create_sparse_interaction_matrix(self):\n",
    "        \"\"\"\n",
    "            return a sparse adjacency matrix with the shape (user number, item number)\n",
    "        \"\"\"\n",
    "        row, col, entries = [], [], []\n",
    "        for pair in self.training_data:\n",
    "            row += [int(pair[0])]\n",
    "            col += [int(pair[1])]\n",
    "            entries += [1.0]\n",
    "        interaction_mat = sp.csr_matrix((entries, (row, col)), shape=(self.n_users,self.n_items),dtype=np.float32)\n",
    "        inv_interaction_mat = sp.csr_matrix((entries, (col, row)), shape=(self.n_items, self.n_users), dtype=np.float32)\n",
    "        return interaction_mat, inv_interaction_mat\n",
    "            \n",
    "    def get_user_id(self, u):\n",
    "        if u in self.user:\n",
    "            return self.user[u]\n",
    "\n",
    "    def get_item_id(self, i):\n",
    "        if i in self.item:\n",
    "            return self.item[i]\n",
    "\n",
    "    def training_size(self):\n",
    "        return len(self.user), len(self.item), len(self.training_data)\n",
    "\n",
    "    def test_size(self):\n",
    "        return len(self.test_set), len(self.test_set_item), len(self.test_data)\n",
    "\n",
    "    def contain(self, u, i):\n",
    "        'whether user u rated item i'\n",
    "        if u in self.user and i in self.training_set_u[u]:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def contain_user(self, u):\n",
    "        'whether user is in training set'\n",
    "        if u in self.user:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def contain_item(self, i):\n",
    "        \"\"\"whether item is in training set\"\"\"\n",
    "        if i in self.item:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def user_rated(self, u):\n",
    "        return list(self.training_set_u[u].keys()), list(self.training_set_u[u].values())\n",
    "\n",
    "    def item_rated(self, i):\n",
    "        return list(self.training_set_i[i].keys()), list(self.training_set_i[i].values())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb4bb4a-e750-4360-801b-4a90b4f06bec",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "193ecb06-7fc5-405b-a3e4-cd8063fafc64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Knowledge(Interaction):\n",
    "    def __init__(self, conf, training, test, knowledge):\n",
    "        super().__init__(conf, training, test)\n",
    "        self.conf = conf \n",
    "        self.kg_data = knowledge\n",
    "\n",
    "        self.entity = {}\n",
    "        self.id2ent = {}\n",
    "\n",
    "        self.userent = {}\n",
    "        self.itement = {}\n",
    "        \n",
    "        self.u2id = {}\n",
    "        self.id2u = {}\n",
    "        \n",
    "        self.i2id = {}\n",
    "        self.id2i = {}\n",
    "        \n",
    "        self.relation = {}\n",
    "        self.id2rel = {}\n",
    "\n",
    "        self.cf_train_data = np.array(training)\n",
    "        self.training_set_e = defaultdict(dict)\n",
    "\n",
    "        self.construct_data()\n",
    "        \n",
    "        self.laplacian_type = 'random-walk'\n",
    "        self.create_adjacency_dict()\n",
    "        self.create_laplacian_dict()\n",
    "        \n",
    "        self.kg_interaction_mat = self.__create_sparse_knowledge_interaction_matrix()\n",
    "        self.interaction_mat = self.__create_sparse_interaction_matrix()\n",
    "        \n",
    "        \n",
    "    def construct_data(self):\n",
    "        kg_data = self.kg_data\n",
    "        n_relations = max(kg_data['r']) + 1\n",
    "        inverse_kg_data = kg_data.copy()\n",
    "        inverse_kg_data = inverse_kg_data.rename({'h': 't', 't': 'h'}, axis='columns')\n",
    "        inverse_kg_data['r'] += n_relations\n",
    "\n",
    "        kg_data = pd.concat([kg_data, inverse_kg_data], axis=0, ignore_index=True, sort=False)\n",
    "\n",
    "        # remap user_id \n",
    "        kg_data['r'] += 2\n",
    "        \n",
    "        kg_train_data = pd.concat([kg_data, inverse_kg_data], axis=0, ignore_index=True, sort=False)\n",
    "        self.n_entities = max(max(kg_train_data['h']), max(kg_train_data['t'])) + 1\n",
    "        self.n_relations = max(kg_train_data['r']) + 1\n",
    "\n",
    "        # add interactions to kg data\n",
    "        cf2kg_train_data = pd.DataFrame(np.zeros((self.n_cf_train, 3), dtype=np.int32), columns=['h', 'r', 't'])\n",
    "        cf2kg_train_data['h'] = self.cf_train_data[:,0]\n",
    "        cf2kg_train_data['t'] = self.cf_train_data[:,1]\n",
    "\n",
    "        inverse_cf2kg_train_data = pd.DataFrame(np.ones((self.n_cf_train, 3), dtype=np.int32), columns=['h', 'r', 't'])\n",
    "        inverse_cf2kg_train_data['h'] = self.cf_train_data[:,1]\n",
    "        inverse_cf2kg_train_data['t'] = self.cf_train_data[:,0]\n",
    "\n",
    "        self.kg_train_data = pd.concat([kg_train_data, cf2kg_train_data, inverse_cf2kg_train_data], ignore_index=True)\n",
    "        self.n_kg_train = len(self.kg_train_data)\n",
    "\n",
    "        self.n_users_entities = int(max(max(self.kg_train_data['h']), max(self.kg_train_data['t'])) + 1)\n",
    "\n",
    "        # construct kg dict\n",
    "        h_list = []\n",
    "        t_list = []\n",
    "        r_list = []\n",
    "\n",
    "        self.train_kg_dict = collections.defaultdict(list)\n",
    "        self.train_relation_dict = collections.defaultdict(list)\n",
    "\n",
    "        for idx, row in self.kg_train_data.iterrows():\n",
    "            h, r, t = int(row['h']), int(row['r']), int(row['t'])\n",
    "            h_list.append(h)\n",
    "            t_list.append(t)\n",
    "            r_list.append(r)\n",
    "\n",
    "            if h not in self.entity:\n",
    "                self.entity[h] = len(self.entity)\n",
    "                self.id2ent[self.entity[h]] = h\n",
    "                # check h co phai user hay item k\n",
    "                if h in self.user:\n",
    "                    self.userent[h] = len(self.userent)\n",
    "                #     # self.id2userent[self.userent[h]] = h\n",
    "                if h in self.item:\n",
    "                    self.itement[h] = len(self.itement)\n",
    "                #     # self.id2itement[self.itement[h]] = h\n",
    "\n",
    "            if t not in self.entity:\n",
    "                self.entity[t] = len(self.entity)\n",
    "                self.id2ent[self.entity[t]] = t \n",
    "                # check h co phai user hay item k \n",
    "                if t in self.user:\n",
    "                    self.userent[t] = len(self.userent)\n",
    "                #     # self.id2userent[self.userent[t]] = t\n",
    "                if t in self.item:\n",
    "                    self.itement[t] = len(self.itement)\n",
    "                #     # self.id2itement[self.itement[t]] = t\n",
    "            if r not in self.relation:\n",
    "                self.relation[r] = len(self.relation)\n",
    "                self.id2rel[self.relation[r]] = r \n",
    "            \n",
    "            self.training_set_e[t][h] = r\n",
    "            self.train_kg_dict[h].append((t, r))\n",
    "            self.train_relation_dict[r].append((h, t))\n",
    "        \n",
    "        self.h_list = torch.LongTensor(h_list).cuda()\n",
    "        self.t_list = torch.LongTensor(t_list).cuda()\n",
    "        self.r_list = torch.LongTensor(r_list).cuda()\n",
    "        \n",
    "        lst_user_entities = list(self.userent.keys())\n",
    "        lst_item_entities = list(self.itement.keys())\n",
    "\n",
    "        for idx, u in enumerate(lst_user_entities):\n",
    "            self.u2id[u] = idx\n",
    "            self.id2u[idx] = u\n",
    "        for idx, i in enumerate(lst_item_entities):\n",
    "            self.i2id[i] = idx\n",
    "            self.id2i[idx] = i\n",
    "        \n",
    "    def get_entity_id(self, e):\n",
    "        if e in self.entity:\n",
    "            return self.entity[e]\n",
    "    \n",
    "    def __create_sparse_knowledge_interaction_matrix(self):\n",
    "        \"\"\"\n",
    "            return a sparse adjacency matrix with the shape (entity number, entity number)\n",
    "        \"\"\"\n",
    "        row, col, entries = [], [], []\n",
    "        for idx, pair in self.kg_train_data.iterrows():\n",
    "            head, tail = int(pair['h']), int(pair['t'])\n",
    "            row += [head]\n",
    "            col += [tail]\n",
    "            entries += [1.0]\n",
    "        interaction_mat = sp.csr_matrix((entries, (row, col)), shape=(self.n_users_entities, self.n_users_entities),dtype=np.float32)\n",
    "        return interaction_mat\n",
    "    \n",
    "    def __create_sparse_interaction_matrix(self):\n",
    "        row, col, entries = [], [], []\n",
    "        for pair in self.training_data:\n",
    "            head, tail  = int(pair[0]), int(pair[1])\n",
    "            row += [head]\n",
    "            col += [tail]\n",
    "            entries += [1.0]\n",
    "        interaction_mat = sp.csr_matrix((entries, (row, col)), shape=(self.n_users_entities, self.n_users_entities),dtype=np.float32) \n",
    "        return interaction_mat\n",
    "    \n",
    "    def convert_coo2tensor(self, coo):\n",
    "        values = coo.data\n",
    "        indices = np.vstack((coo.row, coo.col))\n",
    "\n",
    "        i = torch.LongTensor(indices)\n",
    "        v = torch.FloatTensor(values)\n",
    "        shape = coo.shape\n",
    "        return torch.sparse.FloatTensor(i, v, torch.Size(shape))\n",
    "    \n",
    "    def create_adjacency_dict(self):\n",
    "        self.adjacency_dict = {}\n",
    "        for r, ht_list in self.train_relation_dict.items():\n",
    "            rows = [e[0] for e in ht_list]\n",
    "            cols = [e[1] for e in ht_list]\n",
    "            vals = [1] * len(rows)\n",
    "            adj = sp.coo_matrix((vals, (rows, cols)), shape=(self.n_users_entities, self.n_users_entities))\n",
    "            self.adjacency_dict[r] = adj\n",
    "    \n",
    "    def create_laplacian_dict(self):\n",
    "        def symmetric_norm_lap(adj):\n",
    "            rowsum = np.array(adj.sum(axis=1))\n",
    "\n",
    "            d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "            d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0\n",
    "            d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "\n",
    "            norm_adj = d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt)\n",
    "            return norm_adj.tocoo()\n",
    "\n",
    "        def random_walk_norm_lap(adj):\n",
    "            rowsum = np.array(adj.sum(axis=1))\n",
    "\n",
    "            d_inv = np.power(rowsum, -1.0).flatten()\n",
    "            d_inv[np.isinf(d_inv)] = 0\n",
    "            d_mat_inv = sp.diags(d_inv)\n",
    "\n",
    "            norm_adj = d_mat_inv.dot(adj)\n",
    "            return norm_adj.tocoo()\n",
    "\n",
    "        if self.laplacian_type == 'symmetric':\n",
    "            norm_lap_func = symmetric_norm_lap\n",
    "        elif self.laplacian_type == 'random-walk':\n",
    "            norm_lap_func = random_walk_norm_lap\n",
    "        \n",
    "        self.laplacian_dict = {}\n",
    "        for r, adj in self.adjacency_dict.items():\n",
    "            self.laplacian_dict[r] = norm_lap_func(adj)\n",
    "\n",
    "        A_in = sum(self.laplacian_dict.values())\n",
    "        self.A_in = self.convert_coo2tensor(A_in.tocoo())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1216710e-2725-450e-a2e8-116c959bc8f6",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "732e6cd4-c756-4382-a457-104ac8446f66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HGNNConv(nn.Module):\n",
    "    def __init__(self, leaky, input_dim, hyper_dim, bias=False):\n",
    "        super(HGNNConv, self).__init__()\n",
    "        self.hyper_dim = hyper_dim\n",
    "        self.act = nn.LeakyReLU(negative_slope=leaky).cuda()\n",
    "        self.fc1 = nn.Linear(input_dim, hyper_dim ,bias=False).cuda() \n",
    "        self.fc2 = nn.Linear(hyper_dim, hyper_dim ,bias=False).cuda()  \n",
    "        self.fc3 = nn.Linear(hyper_dim, hyper_dim ,bias=False).cuda()  \n",
    "        \n",
    "        self.ln1 = torch.nn.LayerNorm(hyper_dim).cuda()\n",
    "        self.ln2 = torch.nn.LayerNorm(hyper_dim).cuda()\n",
    "        self.ln3 = torch.nn.LayerNorm(hyper_dim).cuda()\n",
    "        self.ln4 = torch.nn.LayerNorm(hyper_dim).cuda()\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(hyper_dim))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, adj, embeds):\n",
    "        lat1 = self.act(torch.spmm(adj.t(), embeds))\n",
    "        lat1 = self.ln1(lat1)\n",
    "\n",
    "        lat2 = self.act(self.fc1(lat1)) +  lat1\n",
    "        lat2 = self.ln2(lat2)\n",
    "        \n",
    "        lat3 = self.act(self.fc2(lat2)) + lat2\n",
    "        lat3 = self.ln3(lat3)\n",
    "        \n",
    "        lat4 = self.act(self.fc3(lat3)) + lat3 \n",
    "        output = torch.spmm(adj, lat4)\n",
    "        if self.bias is not None:\n",
    "            output += self.bias \n",
    "        output = self.ln4(output)\n",
    "        ret = self.act(output)\n",
    "        return ret\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8ac61ab-4ae5-4708-8a7c-f6e1baa0c7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, config, data, data_kg, args):\n",
    "        super(Model, self).__init__()\n",
    "        self.data = data\n",
    "        self.data_kg = data_kg\n",
    "        adj = data_kg.interaction_mat\n",
    "        kg_adj = data_kg.kg_interaction_mat\n",
    "        \n",
    "        self.user_indices = torch.LongTensor(list(rec.data_kg.userent.keys())).cuda()\n",
    "        self.item_indices =  torch.LongTensor(list(rec.data_kg.itement.keys())).cuda()\n",
    "\n",
    "        self.adj  = TorchGraphInterface.convert_sparse_mat_to_tensor(adj).cuda()\n",
    "        self.kg_adj = TorchGraphInterface.convert_sparse_mat_to_tensor(kg_adj).cuda()\n",
    "        \n",
    "        self._parse_args(args)\n",
    "        self.embedding_dict = self._init_model()\n",
    "        \n",
    "        self.fc_e_cf = nn.Linear(self.input_dim, self.hyper_dim)\n",
    "        self.fc_e_kg = nn.Linear(self.input_dim, self.hyper_dim)\n",
    "        \n",
    "        \n",
    "        self.hgnn_e_cf = [HGNNConv(leaky=self.p, input_dim=self.hyper_dim, hyper_dim=self.hyper_dim) for i in range(self.layers)]\n",
    "        self.hgnn_e_kg = [HGNNConv(leaky=self.p, input_dim=self.hyper_dim, hyper_dim=self.hyper_dim) for i in range(self.layers)]   \n",
    "        \n",
    "        self.non_linear = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(self.drop_rate)\n",
    "        \n",
    "    def _parse_args(self, args):\n",
    "        self.input_dim = args['input_dim']\n",
    "        self.hyper_dim = args['hyper_dim']\n",
    "        self.relation_dim = args['relation_dim']\n",
    "        self.p = args['p']\n",
    "        self.drop_rate = args['drop_rate'] \n",
    "        self.layers = args['n_layers']\n",
    "        self.temp = args['temp']\n",
    "        self.aug_type = args['aug_type']\n",
    "        self.alpha = args['alpha']\n",
    "\n",
    "    def _init_model(self):\n",
    "        initializer = init.xavier_uniform_\n",
    "        self.user_entity_emb =  nn.Parameter(initializer(torch.empty(self.data_kg.n_users_entities, self.input_dim))).cuda()\n",
    "        self.relation_emb =   nn.Parameter(initializer(torch.empty(self.data_kg.n_relations, self.input_dim))).cuda()\n",
    "        self.trans_M = nn.Parameter(initializer(torch.empty(self.data_kg.n_relations, self.hyper_dim, self.relation_dim))).cuda()\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            init.normal_(m.weight, std=0.01)\n",
    "            if m.bias is not None:\n",
    "                init.zeros_(m.bias)\n",
    "                \n",
    "    def graph_reconstruction(self):\n",
    "        if self.aug_type==0 or 1:\n",
    "            dropped_adj = self.random_graph_augment()\n",
    "        else:\n",
    "            dropped_adj = [], []\n",
    "            for k in range(self.n_layers):\n",
    "                dropped_adj = self.random_graph_augment()\n",
    "                dropped_adj.append(dropped_adj_)\n",
    "        return dropped_adj\n",
    "\n",
    "    def random_graph_augment(self):\n",
    "        dropped_mat = None\n",
    "        if self.aug_type == 0:\n",
    "            dropped_mat = GraphAugmentor.node_dropout(self.data.interaction_mat, self.drop_rate)\n",
    "        elif self.aug_type == 1 or self.aug_type == 2:\n",
    "            dropped_mat = GraphAugmentor.edge_dropout(self.data.interaction_mat, self.drop_rate)\n",
    "        return TorchGraphInterface.convert_sparse_mat_to_tensor(dropped_mat).cuda()\n",
    "    \n",
    "    def random_graph_augment_kg(self):\n",
    "        dropped_mat = None\n",
    "        if self.aug_type == 0:\n",
    "            dropped_mat = GraphAugmentor.node_dropout(self.data_kg.kg_interaction_mat, self.drop_rate)\n",
    "        elif self.aug_type == 1 or self.aug_type == 2:\n",
    "            dropped_mat = GraphAugmentor.edge_dropout(self.data_kg.kg_interaction_mat, self.drop_rate)\n",
    "        return TorchGraphInterface.convert_sparse_mat_to_tensor(dropped_mat).cuda()\n",
    "\n",
    "    def calculate_cf_embedding(self, perturbed_adj=None):\n",
    "        eEmbed = self.user_entity_emb \n",
    "        eEmbed = self.dropout(self.non_linear(self.fc_e_cf(eEmbed)))\n",
    "        \n",
    "        all_embeddings = [eEmbed]\n",
    "        for k in range(self.layers):     \n",
    "            if perturbed_adj is not None:\n",
    "                if isinstance(perturbed_adj, list):\n",
    "                    hyperELat = self.hgnn_e_cf[k](perturbed_adj[k], eEmbed)\n",
    "                else:\n",
    "                    hyperELat = self.hgnn_e_cf[k](perturbed_adj[k], eEmbed)\n",
    "            else:\n",
    "                hyperELat = self.hgnn_e_cf[k](self.adj, eEmbed)\n",
    "            all_embeddings += [hyperELat]\n",
    "        ego_embeddings = torch.cat(all_embeddings, dim=1)\n",
    "        return ego_embeddings\n",
    "        \n",
    "    def calculate_kg_embedding(self, perturbed_adj_kg=None):\n",
    "        eEmbed = self.user_entity_emb \n",
    "        eEmbed = self.dropout(self.non_linear(self.fc_e_kg(eEmbed)))\n",
    "        \n",
    "        all_embeddings = [eEmbed]\n",
    "        for k in range(self.layers):     \n",
    "            if perturbed_adj_kg is not None:\n",
    "                if isinstance(perturbed_adj_kg, list):\n",
    "                    hyperELat = self.hgnn_e_kg[k](perturbed_adj_kg[k], eEmbed)\n",
    "                else:\n",
    "                    hyperELat = self.hgnn_e_kg[k](perturbed_adj_kg[k], eEmbed)\n",
    "            else:\n",
    "                hyperELat = self.hgnn_e_kg[k](self.kg_adj, eEmbed)\n",
    "            all_embeddings += [hyperELat]\n",
    "            \n",
    "        ego_embeddings = torch.stack(all_embeddings, dim=1)\n",
    "        ego_embeddings = torch.mean(ego_embeddings, dim=1)\n",
    "        return ego_embeddings\n",
    "        \n",
    "    def calculate_cf_loss(self, anchor_emb, pos_emb, neg_emb, reg):\n",
    "        calc_reg_loss = EmbLoss()\n",
    "        rec_loss = bpr_loss(anchor_emb, pos_emb, neg_emb)\n",
    "        reg_loss = reg * calc_reg_loss(anchor_emb, pos_emb, neg_emb)\n",
    "        cf_loss = rec_loss + reg_loss\n",
    "        return cf_loss\n",
    "    \n",
    "    def calculate_kg_loss(self, h_embed, r, pos_t_embed, neg_t_embed, reg_kg):\n",
    "        calc_reg_loss = EmbLoss()\n",
    "        \n",
    "        r_embed = self.relation_emb[r]                                                # (kg_batch_size, relation_dim)\n",
    "        W_r = self.trans_M[r]                                                           # (kg_batch_size, embed_dim, relation_dim)\n",
    "\n",
    "        # h_embed = self.user_entity_emb[h]                                          # (kg_batch_size, embed_dim)\n",
    "        # pos_t_embed = self.user_entity_emb[pos_t]                                    # (kg_batch_size, embed_dim)\n",
    "        # neg_t_embed = self.user_entity_emb[neg_t]                                     # (kg_batch_size, embed_dim)\n",
    "\n",
    "        r_mul_h = torch.bmm(h_embed.unsqueeze(1), W_r).squeeze(1)                       # (kg_batch_size, relation_dim)\n",
    "        r_mul_pos_t = torch.bmm(pos_t_embed.unsqueeze(1), W_r).squeeze(1)               # (kg_batch_size, relation_dim)\n",
    "        r_mul_neg_t = torch.bmm(neg_t_embed.unsqueeze(1), W_r).squeeze(1)               # (kg_batch_size, relation_dim)\n",
    "\n",
    "        # Equation (1)\n",
    "        pos_score = torch.sum(torch.pow(r_mul_h + r_embed - r_mul_pos_t, 2), dim=1)     # (kg_batch_size)\n",
    "        neg_score = torch.sum(torch.pow(r_mul_h + r_embed - r_mul_neg_t, 2), dim=1)     # (kg_batch_size)\n",
    "\n",
    "        # Equation (2)\n",
    "        # kg_loss = F.softplus(pos_score - neg_score)\n",
    "        kg_loss = (-1.0) * F.logsigmoid(neg_score - pos_score)\n",
    "        kg_loss = torch.mean(kg_loss)\n",
    "\n",
    "        reg_loss =  reg_kg * calc_reg_loss(r_mul_h, r_embed, r_mul_pos_t, r_mul_neg_t)\n",
    "        loss = self.alpha *(kg_loss + reg_loss)\n",
    "        return loss\n",
    "    \n",
    "    def cal_cl_loss(self, idxs, perturbed_mat1, perturbed_mat2):\n",
    "        if type(idxs[0]) is not list:\n",
    "            u_idx = torch.unique(idxs[0])\n",
    "        else:\n",
    "            u_idx = torch.unique(torch.Tensor(idxs[0]).cuda().type(torch.long))\n",
    "        if type(idxs[1]) is not list:\n",
    "            i_idx = torch.unique(idxs[1])\n",
    "        else:\n",
    "            i_idx = torch.unique(torch.Tensor(idxs[1]).cuda().type(torch.long))\n",
    "        user_view_1, item_view_1 = self.forward(perturbed_mat1)\n",
    "        user_view_2, item_view_2 = self.forward(perturbed_mat2)\n",
    "        view1 = torch.cat((user_view_1[u_idx],item_view_1[i_idx]),0)\n",
    "        view2 = torch.cat((user_view_2[u_idx],item_view_2[i_idx]),0)\n",
    "        return InfoNCE(view1,view2,self.temp)\n",
    "    \n",
    "    def forward(self):\n",
    "        user_emb, item_emb = self.calculate_cf_embedding()\n",
    "        return user_emb, item_emb\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ac95e8-3eb4-49b0-87c5-c81ab3b2504d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55990a44-beea-4041-b23b-229be826bbab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime \n",
    "def next_batch_pairwise(rec, batch_size, n_negs=1):\n",
    "    training_data = rec.data.training_data\n",
    "    shuffle(training_data)\n",
    "    ptr = 0\n",
    "    data_size = len(training_data)\n",
    "    while ptr < data_size:\n",
    "        if ptr + batch_size < data_size:\n",
    "            batch_end = ptr + batch_size\n",
    "        else:   \n",
    "            batch_end = data_size\n",
    "        users = [training_data[idx][0] for idx in range(ptr, batch_end)]\n",
    "        items = [training_data[idx][1] for idx in range(ptr, batch_end)]\n",
    "        ptr = batch_end\n",
    "        u_idx, i_idx, j_idx = [], [], []\n",
    "        item_list = list(data.item.keys())\n",
    "        for i, user in enumerate(users):\n",
    "            i_idx.append(items[i])\n",
    "            u_idx.append(user)\n",
    "            for m in range(n_negs):\n",
    "                neg_item = choice(item_list)\n",
    "                while neg_item in data.training_set_u[user]:\n",
    "                    neg_item = choice(item_list)\n",
    "                j_idx.append(neg_item)\n",
    "\n",
    "        u_idx  = torch.LongTensor(u_idx).cuda()\n",
    "        i_idx  = torch.LongTensor(i_idx).cuda()\n",
    "        j_idx  = torch.LongTensor(j_idx).cuda()\n",
    "        yield u_idx, i_idx, j_idx\n",
    "    \n",
    "def next_batch_kg(kg_data, kg_dict, batch_size, n_negs=1):\n",
    "    ptr = 0\n",
    "    exist_heads= kg_dict.keys()\n",
    "    h_list = list(exist_heads)\n",
    "    h_dict = {value: idx for idx, value in enumerate(h_list)}\n",
    "    all_tails = list(set(kg_data[:,2]))\n",
    "    data_size = len(kg_data)\n",
    "    # Pre-compute positive tail sets and negative tails for each head\n",
    "    pos_tail_sets = {head: set([it[0] for it in tails]) for head, tails in kg_dict.items()}\n",
    "    # neg_tail_sets = {head: np.random.choice(list(all_tails - pos_tails), size=n_negs) for head, pos_tails in pos_tail_sets.items()}\n",
    "    \n",
    "    while ptr < data_size:\n",
    "        if ptr + batch_size < data_size:\n",
    "            batch_end = ptr + batch_size\n",
    "        else:   \n",
    "            batch_end = data_size\n",
    "        \n",
    "        heads, relations, tails = kg_data[ptr:batch_end, 0], kg_data[ptr:batch_end, 1], kg_data[ptr:batch_end, 2]\n",
    "        \n",
    "        ptr = batch_end\n",
    "        h_idx, r_idx, pos_t_idx, neg_t_idx = [], [], [], []\n",
    "        # time1 = datetime.datetime.now()\n",
    "        h_idx = [h_dict[head] for head in heads]\n",
    "        \n",
    "        r_idx.extend(relations)\n",
    "        pos_t_idx.extend(tails)\n",
    "        for head in heads:\n",
    "            neg_t = random.choice(all_tails)\n",
    "            while neg_t in pos_tail_sets[head]:\n",
    "                neg_t = random.choice(all_tails)\n",
    "            neg_t_idx.append(h_dict[neg_t])\n",
    "\n",
    "        h_idx  = torch.LongTensor(h_idx).cuda()\n",
    "        r_idx  = torch.LongTensor(r_idx).cuda()\n",
    "        pos_t_idx  = torch.LongTensor(pos_t_idx).cuda()\n",
    "        neg_t_idx  = torch.LongTensor(neg_t_idx).cuda()\n",
    "        yield h_idx, r_idx, pos_t_idx, neg_t_idx\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60d4353-61de-4154-bfe9-2e2f6c46b1a9",
   "metadata": {},
   "source": [
    "## Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4a81a3f-94f9-4607-a288-21ed5bde7f54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _L2_loss_mean(x):\n",
    "    return torch.mean(torch.sum(torch.pow(x, 2), dim=1, keepdim=False) / 2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2a94f0-e377-4e3f-ae79-8f7500f4d621",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4038820d-8b17-40c9-ae1b-a5e5e7a6f7f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(train_model, rec, args):\n",
    "    # seed\n",
    "    random.seed(args['seed'])\n",
    "    np.random.seed(args['seed'])\n",
    "    torch.manual_seed(args['seed'])\n",
    "    torch.cuda.manual_seed_all(args['seed'])\n",
    "    \n",
    "    lst_train_losses = []\n",
    "    lst_rec_losses = []\n",
    "    lst_kg_losses = []\n",
    "    lst_performances = []\n",
    "    \n",
    "    reg = args['reg']\n",
    "    reg_kg = args['reg_kg']\n",
    "    alpha = args['alpha']\n",
    "    \n",
    "    \n",
    "    cf_optimizer  = torch.optim.Adam(train_model.parameters(), lr=lRate)\n",
    "    kg_optimizer = torch.optim.Adam(train_model.parameters(), lr=lRateKG)\n",
    "\n",
    "    kg_data = rec.data_kg.kg_train_data.to_numpy()\n",
    "    kg_dict = rec.data_kg.train_kg_dict\n",
    "        \n",
    "    for ep in range(maxEpoch):\n",
    "        train_model.train()\n",
    "        \n",
    "        train_losses = []\n",
    "        cf_losses = []\n",
    "        kg_losses = []\n",
    "        \n",
    "        cf_total_loss = 0\n",
    "        kg_total_loss = 0\n",
    "        \n",
    "        n_cf_batch = int(rec.data.n_cf_train // batchSize + 1)\n",
    "        n_kg_batch = int(rec.data_kg.n_kg_train // batchSizeKG + 1)\n",
    "\n",
    "        shuffle(kg_data)\n",
    "\n",
    "        # Learn cf graph\n",
    "        for n, batch in enumerate(next_batch_pairwise(rec, batchSize)):\n",
    "            user_idx, pos_idx, neg_idx = batch\n",
    "            ego_emb = train_model.calculate_cf_embedding()\n",
    "            \n",
    "            user_emb = ego_emb[user_idx]\n",
    "            pos_item_emb = ego_emb[pos_idx]\n",
    "            neg_item_emb = ego_emb[neg_idx]\n",
    "            \n",
    "            cf_batch_loss = train_model.calculate_cf_loss(user_emb, pos_item_emb, neg_item_emb, reg)\n",
    "            if np.isnan(cf_batch_loss.cpu().detach().numpy()):\n",
    "                print('ERROR (CF Training): Epoch {:04d} Iter {:04d} / {:04d} Loss is nan.'.format(ep, n, n_cf_batch))\n",
    "\n",
    "            cf_batch_loss.backward()\n",
    "            cf_optimizer.step()\n",
    "            cf_optimizer.zero_grad()\n",
    "            cf_total_loss += cf_batch_loss.item()\n",
    "            \n",
    "            if (n % 20) == 0:\n",
    "                print('CF Training: Epoch {:04d} Iter {:04d} / {:04d} | Iter Loss {:.4f} | Iter Mean Loss {:.4f}'.format(ep, n, n_cf_batch,  cf_batch_loss.item(), cf_total_loss / (n+1)))\n",
    "        \n",
    "#         # Learn knowledge grap\n",
    "#         for n, batch in enumerate(next_batch_kg(kg_data, kg_dict, batchSizeKG)):\n",
    "#             kg_batch_head, kg_batch_relation, kg_batch_pos_tail, kg_batch_neg_tail = batch\n",
    "            \n",
    "#             ego_embed = train_model.calculate_kg_embedding()\n",
    "#             kg_batch_head_emb = ego_embed[kg_batch_head]\n",
    "#             kg_batch_pos_tail_emb = ego_embed[kg_batch_pos_tail]\n",
    "#             kg_batch_neg_tail_emb = ego_embed[kg_batch_neg_tail]\n",
    "            \n",
    "#             kg_batch_loss = train_model.calculate_kg_loss(kg_batch_head_emb, kg_batch_relation, kg_batch_pos_tail_emb, kg_batch_neg_tail_emb, reg_kg)\n",
    "#             if np.isnan(kg_batch_loss.cpu().detach().numpy()):\n",
    "#                 print('ERROR (KG Training): Epoch {:04d} Iter {:04d} / {:04d} Loss is nan.'.format(ep, n, n_kg_batch))\n",
    "#             kg_batch_loss.backward()\n",
    "#             kg_optimizer.step()\n",
    "#             kg_optimizer.zero_grad()\n",
    "#             kg_total_loss += kg_batch_loss.item()\n",
    "#             kg_losses.append(kg_batch_loss.item())\n",
    "\n",
    "#             if (n % 10) == 0:\n",
    "#                 print('KG Training: Epoch {:04d} Iter {:04d} / {:04d} | Iter Loss {:.4f} | Iter Mean Loss {:.4f}'.format(ep, n, n_kg_batch,  kg_batch_loss.item(), kg_total_loss / (n+1)))\n",
    "        \n",
    "        # # Learn attention \n",
    "        # h_list = rec.data_kg.h_list.cuda()\n",
    "        # t_list = rec.data_kg.t_list.cuda()\n",
    "        # r_list = rec.data_kg.r_list.cuda()\n",
    "        # relations = list(rec.data_kg.laplacian_dict.keys())\n",
    "        # train_model.update_attention(h_list, t_list, r_list, relations)\n",
    "        \n",
    "        cf_loss = np.mean(cf_losses)\n",
    "        kg_loss = 0\n",
    "        train_loss = cf_loss + kg_loss\n",
    "\n",
    "\n",
    "        train_model.eval()\n",
    "        with torch.no_grad():\n",
    "            ego_emb = train_model.calculate_cf_embedding()       \n",
    "            user_emb = ego_emb[train_model.user_indices]\n",
    "            item_emb = ego_emb[train_model.item_indices]\n",
    "            data_ep = rec.fast_evaluation(train_model, ep, user_emb, item_emb)\n",
    "        \n",
    "        rec.save_performance_row(ep, data_ep)\n",
    "        rec.save_loss_row([ep, train_loss, cf_loss, kg_loss])\n",
    "        \n",
    "        lst_performances.append(data_ep)\n",
    "        lst_train_losses.append([ep, train_loss]) \n",
    "        lst_rec_losses.append([ep, cf_loss])\n",
    "        lst_kg_losses.append([ep, kg_loss])\n",
    "        \n",
    "    rec.save_loss(lst_train_losses, lst_rec_losses, lst_kg_losses)\n",
    "    rec.save_perfomance_training(lst_performances)\n",
    "    user_emb, item_emb = rec.best_user_emb, rec.best_item_emb\n",
    "    return user_emb, item_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed034219-dbe5-407a-9111-873b9aa94e3e",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e277edf0-c61c-49cf-ad9b-80f235ae808d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(rec, user_emb, item_emb):\n",
    "    def process_bar(num, total):\n",
    "        rate = float(num) / total\n",
    "        ratenum = int(50 * rate)\n",
    "        r = '\\rProgress: [{}{}]{}%'.format('+' * ratenum, ' ' * (50 - ratenum), ratenum*2)\n",
    "        sys.stdout.write(r)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    # predict\n",
    "    rec_list = {}\n",
    "    user_count = len(rec.data.test_set)\n",
    "    for i, user in enumerate(rec.data.test_set):\n",
    "        user_id = rec.data_kg.u2id[user]\n",
    "        score = torch.matmul(user_emb[user_id], item_emb.transpose(0, 1))\n",
    "        candidates = score.cpu().numpy()\n",
    "        \n",
    "        rated_list, li = rec.data.user_rated(user)\n",
    "        for item in rated_list:\n",
    "            candidates[rec.data_kg.i2id[item]] = -10e8\n",
    "        # s_find_k_largest = time.time()\n",
    "        ids, scores = find_k_largest(rec.max_N, candidates)\n",
    "\n",
    "        item_names = [rec.data_kg.id2i[iid] for iid in ids]\n",
    "        rec_list[user] = list(zip(item_names, scores))\n",
    "        if i % 1000 == 0:\n",
    "            process_bar(i, user_count)\n",
    "    process_bar(user_count, user_count)\n",
    "    print('')\n",
    "    rec.evaluate(rec_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d52b83-7ba1-4d77-9211-71a8a4932501",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41e7f745-44dd-4936-8165-c895d6b9a5f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = 'HGNN'\n",
    "config = ModelConf('./conf/' + model + '.conf')\n",
    "lRates = [0.1]\n",
    "lRateKGs = [0.01]\n",
    "lrDecays = [0.9]\n",
    "maxEpochs = [10]\n",
    "batchSizes = [2048]\n",
    "batchSizeKGs = [8192]\n",
    "nLayers = [2]\n",
    "regs = [0.1]\n",
    "regkgs = [ 0.01]\n",
    "embeddingSizes = [128]\n",
    "datasets = ['lastfm']\n",
    "dataset = datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87ceac4a-b36b-4035-8200-0be4ef752dcb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27092/183689244.py:181: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n",
      "/tmp/ipykernel_27092/183689244.py:181: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n",
      "/tmp/ipykernel_27092/183689244.py:181: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n",
      "/tmp/ipykernel_27092/183689244.py:181: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n",
      "/tmp/ipykernel_27092/183689244.py:181: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n",
      "/tmp/ipykernel_27092/183689244.py:181: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n",
      "/tmp/ipykernel_27092/183689244.py:181: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n",
      "/tmp/ipykernel_27092/183689244.py:181: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n",
      "/tmp/ipykernel_27092/183689244.py:181: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n",
      "/tmp/ipykernel_27092/183689244.py:181: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n",
      "/tmp/ipykernel_27092/183689244.py:181: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n",
      "/tmp/ipykernel_27092/183689244.py:181: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n",
      "/tmp/ipykernel_27092/183689244.py:181: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n",
      "/tmp/ipykernel_27092/183689244.py:181: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n",
      "/tmp/ipykernel_27092/183689244.py:181: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n",
      "/tmp/ipykernel_27092/183689244.py:181: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n",
      "/tmp/ipykernel_27092/183689244.py:181: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n",
      "/tmp/ipykernel_27092/183689244.py:181: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n",
      "/tmp/ipykernel_27092/183689244.py:181: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n",
      "/tmp/ipykernel_27092/183689244.py:181: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n",
      "/tmp/ipykernel_27092/183689244.py:181: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n",
      "/tmp/ipykernel_27092/183689244.py:181: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n",
      "/tmp/ipykernel_27092/183689244.py:181: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n",
      "/tmp/ipykernel_27092/183689244.py:181: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n",
      "/tmp/ipykernel_27092/183689244.py:181: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n",
      "/tmp/ipykernel_27092/183689244.py:181: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n",
      "/tmp/ipykernel_27092/183689244.py:181: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n",
      "/tmp/ipykernel_27092/183689244.py:181: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n",
      "/tmp/ipykernel_27092/183689244.py:181: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n",
      "/tmp/ipykernel_27092/183689244.py:181: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n",
      "/tmp/ipykernel_27092/183689244.py:181: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n",
      "/tmp/ipykernel_27092/183689244.py:181: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n",
      "/tmp/ipykernel_27092/183689244.py:181: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n",
      "/tmp/ipykernel_27092/183689244.py:181: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n",
      "/tmp/ipykernel_27092/183689244.py:181: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n",
      "/tmp/ipykernel_27092/183689244.py:181: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n",
      "/tmp/ipykernel_27092/183689244.py:181: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n",
      "/tmp/ipykernel_27092/183689244.py:181: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n"
     ]
    }
   ],
   "source": [
    "training_data = FileIO.load_data_set('./dataset/' + dataset + '/' +config['training.set'], config['model.type'])\n",
    "test_data = FileIO.load_data_set('./dataset/' + dataset + '/'  +config['test.set'], config['model.type'])\n",
    "knowledge_set = FileIO.load_kg_data('./dataset/' + dataset +'/'+ dataset +'.kg')\n",
    "data = Interaction(config, training_data, test_data)\n",
    "data_kg = Knowledge(config, training_data, test_data, knowledge_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e55de41-d5fb-431e-a7b2-1cdb94355519",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/hungvv/recommender/SELFRec/base/torch_interface.py:10: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:204.)\n",
      "  i = torch.LongTensor([coo.row, coo.col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CF Training: Epoch 0000 Iter 0000 / 0034 | Iter Loss 0.7277 | Iter Mean Loss 0.7277\n",
      "CF Training: Epoch 0000 Iter 0020 / 0034 | Iter Loss 0.7247 | Iter Mean Loss 0.7261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jun/anaconda3/envs/hungvv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/jun/anaconda3/envs/hungvv/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "Test time: 11.416730 s\n",
      "Measure time: 0.018923 s\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 1, Hit Ratio:0.00056  |  Precision:0.00035  |  Recall:0.00055  |  NDCG:0.00059\n",
      "*Best Performance* \n",
      "Epoch:fast_evaluation 1, Hit Ratio:0.00056  |  Precision:0.00035  |  Recall:0.00055  |  NDCG:0.00059\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "CF Training: Epoch 0001 Iter 0000 / 0034 | Iter Loss 0.7248 | Iter Mean Loss 0.7248\n",
      "CF Training: Epoch 0001 Iter 0020 / 0034 | Iter Loss 0.7252 | Iter Mean Loss 0.7250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jun/anaconda3/envs/hungvv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/jun/anaconda3/envs/hungvv/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "Test time: 8.355564 s\n",
      "Measure time: 0.015016 s\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 2, Hit Ratio:0.00211  |  Precision:0.0013  |  Recall:0.00211  |  NDCG:0.00155\n",
      "*Best Performance* \n",
      "Epoch:fast_evaluation 2, Hit Ratio:0.00211  |  Precision:0.0013  |  Recall:0.00211  |  NDCG:0.00155\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "CF Training: Epoch 0002 Iter 0000 / 0034 | Iter Loss 0.6931 | Iter Mean Loss 0.6931\n",
      "CF Training: Epoch 0002 Iter 0020 / 0034 | Iter Loss 0.6931 | Iter Mean Loss 0.6931\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "Test time: 8.352760 s\n",
      "Measure time: 0.014999 s\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 3, Hit Ratio:0.00211  |  Precision:0.0013  |  Recall:0.00211  |  NDCG:0.00155\n",
      "*Best Performance* \n",
      "Epoch:fast_evaluation 3, Hit Ratio:0.00211  |  Precision:0.0013  |  Recall:0.00211  |  NDCG:0.00155\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "CF Training: Epoch 0003 Iter 0000 / 0034 | Iter Loss 0.6931 | Iter Mean Loss 0.6931\n",
      "CF Training: Epoch 0003 Iter 0020 / 0034 | Iter Loss 0.6931 | Iter Mean Loss 0.6931\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "Test time: 8.375128 s\n",
      "Measure time: 0.015091 s\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 4, Hit Ratio:0.00211  |  Precision:0.0013  |  Recall:0.00211  |  NDCG:0.00155\n",
      "*Best Performance* \n",
      "Epoch:fast_evaluation 4, Hit Ratio:0.00211  |  Precision:0.0013  |  Recall:0.00211  |  NDCG:0.00155\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "CF Training: Epoch 0004 Iter 0000 / 0034 | Iter Loss 0.6931 | Iter Mean Loss 0.6931\n",
      "CF Training: Epoch 0004 Iter 0020 / 0034 | Iter Loss 0.6931 | Iter Mean Loss 0.6931\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "Test time: 8.369030 s\n",
      "Measure time: 0.015227 s\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 5, Hit Ratio:0.00211  |  Precision:0.0013  |  Recall:0.00211  |  NDCG:0.00155\n",
      "*Best Performance* \n",
      "Epoch:fast_evaluation 5, Hit Ratio:0.00211  |  Precision:0.0013  |  Recall:0.00211  |  NDCG:0.00155\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "CF Training: Epoch 0005 Iter 0000 / 0034 | Iter Loss 0.6931 | Iter Mean Loss 0.6931\n",
      "CF Training: Epoch 0005 Iter 0020 / 0034 | Iter Loss 0.6931 | Iter Mean Loss 0.6931\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "Test time: 8.359495 s\n",
      "Measure time: 0.015504 s\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 6, Hit Ratio:0.00211  |  Precision:0.0013  |  Recall:0.00211  |  NDCG:0.00155\n",
      "*Best Performance* \n",
      "Epoch:fast_evaluation 6, Hit Ratio:0.00211  |  Precision:0.0013  |  Recall:0.00211  |  NDCG:0.00155\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "CF Training: Epoch 0006 Iter 0000 / 0034 | Iter Loss 0.6931 | Iter Mean Loss 0.6931\n",
      "CF Training: Epoch 0006 Iter 0020 / 0034 | Iter Loss 0.6931 | Iter Mean Loss 0.6931\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "Test time: 8.528878 s\n",
      "Measure time: 0.015122 s\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 7, Hit Ratio:0.00211  |  Precision:0.0013  |  Recall:0.00211  |  NDCG:0.00155\n",
      "*Best Performance* \n",
      "Epoch:fast_evaluation 7, Hit Ratio:0.00211  |  Precision:0.0013  |  Recall:0.00211  |  NDCG:0.00155\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "CF Training: Epoch 0007 Iter 0000 / 0034 | Iter Loss 0.6931 | Iter Mean Loss 0.6931\n",
      "CF Training: Epoch 0007 Iter 0020 / 0034 | Iter Loss 0.6931 | Iter Mean Loss 0.6931\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "Test time: 8.387925 s\n",
      "Measure time: 0.015157 s\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 8, Hit Ratio:0.00211  |  Precision:0.0013  |  Recall:0.00211  |  NDCG:0.00155\n",
      "*Best Performance* \n",
      "Epoch:fast_evaluation 8, Hit Ratio:0.00211  |  Precision:0.0013  |  Recall:0.00211  |  NDCG:0.00155\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "CF Training: Epoch 0008 Iter 0000 / 0034 | Iter Loss 0.6931 | Iter Mean Loss 0.6931\n",
      "CF Training: Epoch 0008 Iter 0020 / 0034 | Iter Loss 0.6931 | Iter Mean Loss 0.6931\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "Test time: 8.333770 s\n",
      "Measure time: 0.015062 s\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 9, Hit Ratio:0.00211  |  Precision:0.0013  |  Recall:0.00211  |  NDCG:0.00155\n",
      "*Best Performance* \n",
      "Epoch:fast_evaluation 9, Hit Ratio:0.00211  |  Precision:0.0013  |  Recall:0.00211  |  NDCG:0.00155\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "CF Training: Epoch 0009 Iter 0000 / 0034 | Iter Loss 0.6931 | Iter Mean Loss 0.6931\n",
      "CF Training: Epoch 0009 Iter 0020 / 0034 | Iter Loss 0.6931 | Iter Mean Loss 0.6931\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "Test time: 8.258171 s\n",
      "Measure time: 0.015201 s\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 10, Hit Ratio:0.00211  |  Precision:0.0013  |  Recall:0.00211  |  NDCG:0.00155\n",
      "*Best Performance* \n",
      "Epoch:fast_evaluation 10, Hit Ratio:0.00211  |  Precision:0.0013  |  Recall:0.00211  |  NDCG:0.00155\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (10, 1), indices imply (10, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# A_in = TorchGraphInterface.convert_sparse_mat_to_tensor(rec.data_kg.kg_interaction_mat).cuda()\u001b[39;00m\n\u001b[1;32m     31\u001b[0m train_model \u001b[38;5;241m=\u001b[39m Model(config, data, data_kg, args)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m---> 32\u001b[0m user_emb, item_emb \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m   \n\u001b[1;32m     33\u001b[0m test(rec, user_emb, item_emb)\n",
      "Cell \u001b[0;32mIn[9], line 109\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_model, rec, args)\u001b[0m\n\u001b[1;32m    106\u001b[0m     lst_rec_losses\u001b[38;5;241m.\u001b[39mappend(cf_loss)\n\u001b[1;32m    107\u001b[0m     lst_kg_losses\u001b[38;5;241m.\u001b[39mappend(kg_loss)\n\u001b[0;32m--> 109\u001b[0m \u001b[43mrec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlst_train_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlst_rec_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlst_kg_losses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m rec\u001b[38;5;241m.\u001b[39msave_perfomance_training(lst_performances)\n\u001b[1;32m    111\u001b[0m user_emb, item_emb \u001b[38;5;241m=\u001b[39m rec\u001b[38;5;241m.\u001b[39mbest_user_emb, rec\u001b[38;5;241m.\u001b[39mbest_item_emb\n",
      "Cell \u001b[0;32mIn[2], line 203\u001b[0m, in \u001b[0;36mGraphRecommender.save_loss\u001b[0;34m(self, train_losses, rec_losses, kg_losses)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_losses, rec_losses, kg_losses):\n\u001b[0;32m--> 203\u001b[0m     df_train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mep\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m     df_rec_loss \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(rec_losses, columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mep\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    205\u001b[0m     df_kg_loss \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(kg_losses, columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mep\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/hungvv/lib/python3.8/site-packages/pandas/core/frame.py:762\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    754\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    755\u001b[0m             arrays,\n\u001b[1;32m    756\u001b[0m             columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    759\u001b[0m             typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    760\u001b[0m         )\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 762\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    766\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    771\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[1;32m    772\u001b[0m         {},\n\u001b[1;32m    773\u001b[0m         index,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    776\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    777\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/hungvv/lib/python3.8/site-packages/pandas/core/internals/construction.py:349\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[1;32m    345\u001b[0m index, columns \u001b[38;5;241m=\u001b[39m _get_axes(\n\u001b[1;32m    346\u001b[0m     values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns\n\u001b[1;32m    347\u001b[0m )\n\u001b[0;32m--> 349\u001b[0m \u001b[43m_check_values_indices_shape_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/hungvv/lib/python3.8/site-packages/pandas/core/internals/construction.py:420\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[0;34m(values, index, columns)\u001b[0m\n\u001b[1;32m    418\u001b[0m passed \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    419\u001b[0m implied \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[0;32m--> 420\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (10, 1), indices imply (10, 2)"
     ]
    }
   ],
   "source": [
    "hyperparameters = [lRates, lRateKGs, lrDecays, maxEpochs, batchSizes, batchSizeKGs, nLayers, regs, regkgs, embeddingSizes, datasets]\n",
    "\n",
    "for params in product(*hyperparameters):\n",
    "    lRate, lRateKG, lrDecay, maxEpoch, batchSize, batchSizeKG, nLayer, reg, reg_kg, embeddingSize, dataset = params\n",
    "    args = {\n",
    "        'lr': lRate,\n",
    "        'lr_kg': lRateKG,\n",
    "        'max_epoch': maxEpoch,\n",
    "        'batch_size': batchSize, \n",
    "        'lr_decay': lrDecay,\n",
    "        'dataset': dataset,\n",
    "        'n_layers': nLayer,\n",
    "        'use_pretrain': 0,\n",
    "        'input_dim': 32,\n",
    "        'hyper_dim': embeddingSize,\n",
    "        'relation_dim': 32,\n",
    "        'reg': reg,\n",
    "        'reg_kg': reg_kg,\n",
    "        'seed': 123,\n",
    "        'alpha': 0.1,\n",
    "        'p': 0.3,\n",
    "        'drop_rate': 0.4,\n",
    "        'cl_rate': 0.1,\n",
    "        'aug_type': 1,\n",
    "        'temp': 0.3\n",
    "    }\n",
    "    args['output_path'] =  f\"./results/HGNN_unified/{dataset}/@KGAT-inp_emb:{args['input_dim']}-emb:{args['hyper_dim']}-bs:{args['batch_size']}-lr:{args['lr']}-lr_kg:{args['lr_kg']}-n_layers:{args['n_layers']}/\"\n",
    "    rec = GraphRecommender(config, data, data_kg, knowledge_set, **args)\n",
    "    # A_in = TorchGraphInterface.convert_sparse_mat_to_tensor(rec.data_kg.kg_interaction_mat).cuda()\n",
    "    \n",
    "    train_model = Model(config, data, data_kg, args).cuda()\n",
    "    user_emb, item_emb = train(train_model, rec, args)   \n",
    "    test(rec, user_emb, item_emb)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hungvv",
   "language": "python",
   "name": "hungvv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
