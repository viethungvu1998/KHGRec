{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7fcb158-996c-47b5-ad77-d27bc87611d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import collections\n",
    "import scipy.sparse as sp\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from util.loss_torch import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e158198-b4ba-4c4d-8b58-542012287406",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from data.ui_graph_new import DataLoaderKG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391743fd-3c68-4398-9800-f631bd674367",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3a9e8db-b5e3-4938-8727-5bccebf973b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataLoader(object):\n",
    "    def __init__(self, args):\n",
    "        self.args = args \n",
    "        self.data_name = args.data_name \n",
    "        \n",
    "        self.data_dir = os.path.join(args.data_dir, args.data_name)\n",
    "        self.train_file = os.path.join(self.data_dir, 'train.txt')\n",
    "        self.test_file = os.path.join(self.data_dir, 'test.txt')\n",
    "        self.kg_file = os.path.join(self.data_dir, \"kg_final.txt\")\n",
    "\n",
    "        self.cf_train_data, self.train_user_dict = self.load_cf(self.train_file)\n",
    "        self.cf_test_data, self.test_user_dict = self.load_cf(self.test_file)\n",
    "        self.statistic_cf()\n",
    "\n",
    "    def load_cf(self, filename):\n",
    "        # load user and item from file \n",
    "        user = []\n",
    "        item = []\n",
    "        user_dict = dict()\n",
    "\n",
    "        lines = open(filename, 'r').readlines()\n",
    "        for l in lines:\n",
    "            tmp = l.strip()\n",
    "            inter = [int(i) for i in tmp.split()]\n",
    "\n",
    "            if len(inter) > 1:\n",
    "                user_id, item_ids = inter[0], inter[1:]\n",
    "                item_ids = list(set(item_ids))\n",
    "\n",
    "                for item_id in item_ids:\n",
    "                    user.append(user_id)\n",
    "                    item.append(item_id)\n",
    "                user_dict[user_id] = item_ids\n",
    "\n",
    "        user = np.array(user, dtype=np.int32)\n",
    "        item = np.array(item, dtype=np.int32)\n",
    "        return (user, item), user_dict\n",
    "\n",
    "\n",
    "    def statistic_cf(self):\n",
    "        self.n_users = max(max(self.cf_train_data[0]), max(self.cf_test_data[0])) + 1\n",
    "        self.n_items = max(max(self.cf_train_data[1]), max(self.cf_test_data[1])) + 1\n",
    "        self.n_cf_train = len(self.cf_train_data[0])\n",
    "        self.n_cf_test = len(self.cf_test_data[0])\n",
    "\n",
    "\n",
    "    def load_kg(self, filename):\n",
    "        # load Kg from file \n",
    "        kg_data = pd.read_csv(filename, sep=' ', names=['h', 'r', 't'], engine='python')\n",
    "        kg_data = kg_data.drop_duplicates()\n",
    "        return kg_data\n",
    "\n",
    "\n",
    "    def sample_pos_items_for_u(self, user_dict, user_id, n_sample_pos_items):\n",
    "        pos_items = user_dict[user_id]\n",
    "        n_pos_items = len(pos_items)\n",
    "\n",
    "        sample_pos_items = []\n",
    "        while True:\n",
    "            if len(sample_pos_items) == n_sample_pos_items:\n",
    "                break\n",
    "\n",
    "            pos_item_idx = np.random.randint(low=0, high=n_pos_items, size=1)[0]\n",
    "            pos_item_id = pos_items[pos_item_idx]\n",
    "            if pos_item_id not in sample_pos_items:\n",
    "                sample_pos_items.append(pos_item_id)\n",
    "        return sample_pos_items\n",
    "\n",
    "\n",
    "    def sample_neg_items_for_u(self, user_dict, user_id, n_sample_neg_items):\n",
    "        pos_items = user_dict[user_id]\n",
    "\n",
    "        sample_neg_items = []\n",
    "        while True:\n",
    "            if len(sample_neg_items) == n_sample_neg_items:\n",
    "                break\n",
    "\n",
    "            neg_item_id = np.random.randint(low=0, high=self.n_items, size=1)[0]\n",
    "            if neg_item_id not in pos_items and neg_item_id not in sample_neg_items:\n",
    "                sample_neg_items.append(neg_item_id)\n",
    "        return sample_neg_items\n",
    "\n",
    "    def generate_cf_batch(self, user_dict, batch_size):\n",
    "        # sample data for user, item \n",
    "        exist_users = user_dict.keys()\n",
    "        if batch_size <= len(exist_users):\n",
    "            batch_user = random.sample(exist_users, batch_size)\n",
    "        else:\n",
    "            batch_user = [random.choice(exist_users) for _ in range(batch_size)]\n",
    "        batch_pos_item, batch_neg_item = [], []\n",
    "        for u in batch_user:\n",
    "            batch_pos_item += self.sample_pos_items_for_u(user_dict, u, 1)\n",
    "            batch_neg_item += self.sample_neg_items_for_u(user_dict, u, 1)\n",
    "\n",
    "        batch_user = torch.LongTensor(batch_user)\n",
    "        batch_pos_item = torch.LongTensor(batch_pos_item)\n",
    "        batch_neg_item = torch.LongTensor(batch_neg_item)\n",
    "        return batch_user, batch_pos_item, batch_neg_item\n",
    "\n",
    "    def sample_pos_triples_for_h(self, kg_dict, head, n_sample_pos_triples):\n",
    "        pos_triples = kg_dict[head]\n",
    "        n_pos_triples = len(pos_triples)\n",
    "\n",
    "        sample_relations, sample_pos_tails = [], []\n",
    "        while True:\n",
    "            if len(sample_relations) == n_sample_pos_triples:\n",
    "                break\n",
    "\n",
    "            pos_triple_idx = np.random.randint(low=0, high=n_pos_triples, size=1)[0]\n",
    "            tail = pos_triples[pos_triple_idx][0]\n",
    "            relation = pos_triples[pos_triple_idx][1]\n",
    "\n",
    "            if relation not in sample_relations and tail not in sample_pos_tails:\n",
    "                sample_relations.append(relation)\n",
    "                sample_pos_tails.append(tail)\n",
    "        return sample_relations, sample_pos_tails\n",
    "\n",
    "\n",
    "    def sample_neg_triples_for_h(self, kg_dict, head, relation, n_sample_neg_triples, highest_neg_idx):\n",
    "        pos_triples = kg_dict[head]\n",
    "\n",
    "        sample_neg_tails = []\n",
    "        while True:\n",
    "            if len(sample_neg_tails) == n_sample_neg_triples:\n",
    "                break\n",
    "\n",
    "            tail = np.random.randint(low=0, high=highest_neg_idx, size=1)[0]\n",
    "            if (tail, relation) not in pos_triples and tail not in sample_neg_tails:\n",
    "                sample_neg_tails.append(tail)\n",
    "        return sample_neg_tails\n",
    "\n",
    "\n",
    "    def generate_kg_batch(self, kg_dict, batch_size, highest_neg_idx):\n",
    "        # sample data for KG \n",
    "        exist_heads = kg_dict.keys()\n",
    "        if batch_size <= len(exist_heads):\n",
    "            batch_head = random.sample(exist_heads, batch_size)\n",
    "        else:\n",
    "            batch_head = [random.choice(exist_heads) for _ in range(batch_size)]\n",
    "\n",
    "        batch_relation, batch_pos_tail, batch_neg_tail = [], [], []\n",
    "        for h in batch_head:\n",
    "            relation, pos_tail = self.sample_pos_triples_for_h(kg_dict, h, 1)\n",
    "            batch_relation += relation\n",
    "            batch_pos_tail += pos_tail\n",
    "\n",
    "            neg_tail = self.sample_neg_triples_for_h(kg_dict, h, relation[0], 1, highest_neg_idx)\n",
    "            batch_neg_tail += neg_tail\n",
    "\n",
    "        batch_head = torch.LongTensor(batch_head)\n",
    "        batch_relation = torch.LongTensor(batch_relation)\n",
    "        batch_pos_tail = torch.LongTensor(batch_pos_tail)\n",
    "        batch_neg_tail = torch.LongTensor(batch_neg_tail)\n",
    "        return batch_head, batch_relation, batch_pos_tail, batch_neg_tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "211e9660-7e1a-4eb4-b3bd-a14f5b4fbb37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataLoaderKG(DataLoader):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super().__init__(args)\n",
    "        self.cf_batch_size = args.cf_batch_size\n",
    "        self.kg_batch_size = args.kg_batch_size\n",
    "        self.test_batch_size = args.test_batch_size\n",
    "\n",
    "        kg_data = self.load_kg(self.kg_file)\n",
    "        self.construct_data(kg_data)\n",
    "        # self.print_info(logging)\n",
    "\n",
    "        self.laplacian_type = args.laplacian_type\n",
    "        self.create_adjacency_dict()\n",
    "        self.create_laplacian_dict()\n",
    "\n",
    "\n",
    "    def construct_data(self, kg_data):\n",
    "        # add inverse kg data\n",
    "        n_relations = max(kg_data['r']) + 1\n",
    "        inverse_kg_data = kg_data.copy()\n",
    "        inverse_kg_data = inverse_kg_data.rename({'h': 't', 't': 'h'}, axis='columns')\n",
    "        inverse_kg_data['r'] += n_relations\n",
    "        kg_data = pd.concat([kg_data, inverse_kg_data], axis=0, ignore_index=True, sort=False)\n",
    "\n",
    "        # re-map user id\n",
    "        kg_data['r'] += 2\n",
    "        self.n_relations = max(kg_data['r']) + 1\n",
    "        self.n_entities = max(max(kg_data['h']), max(kg_data['t'])) + 1\n",
    "        self.n_users_entities = self.n_users + self.n_entities\n",
    "\n",
    "        self.cf_train_data = (np.array(list(map(lambda d: d + self.n_entities, self.cf_train_data[0]))).astype(np.int32), self.cf_train_data[1].astype(np.int32))\n",
    "        self.cf_test_data = (np.array(list(map(lambda d: d + self.n_entities, self.cf_test_data[0]))).astype(np.int32), self.cf_test_data[1].astype(np.int32))\n",
    "\n",
    "        self.train_user_dict = {k + self.n_entities: np.unique(v).astype(np.int32) for k, v in self.train_user_dict.items()}\n",
    "        self.test_user_dict = {k + self.n_entities: np.unique(v).astype(np.int32) for k, v in self.test_user_dict.items()}\n",
    "\n",
    "        # add interactions to kg data\n",
    "        cf2kg_train_data = pd.DataFrame(np.zeros((self.n_cf_train, 3), dtype=np.int32), columns=['h', 'r', 't'])\n",
    "        cf2kg_train_data['h'] = self.cf_train_data[0]\n",
    "        cf2kg_train_data['t'] = self.cf_train_data[1]\n",
    "\n",
    "        inverse_cf2kg_train_data = pd.DataFrame(np.ones((self.n_cf_train, 3), dtype=np.int32), columns=['h', 'r', 't'])\n",
    "        inverse_cf2kg_train_data['h'] = self.cf_train_data[1]\n",
    "        inverse_cf2kg_train_data['t'] = self.cf_train_data[0]\n",
    "\n",
    "        self.kg_train_data = pd.concat([kg_data, cf2kg_train_data, inverse_cf2kg_train_data], ignore_index=True)\n",
    "        self.n_kg_train = len(self.kg_train_data)\n",
    "\n",
    "        # construct kg dict\n",
    "        h_list = []\n",
    "        t_list = []\n",
    "        r_list = []\n",
    "\n",
    "        self.train_kg_dict = collections.defaultdict(list)\n",
    "        self.train_relation_dict = collections.defaultdict(list)\n",
    "\n",
    "        for row in self.kg_train_data.iterrows():\n",
    "            h, r, t = row[1]\n",
    "            h_list.append(h)\n",
    "            t_list.append(t)\n",
    "            r_list.append(r)\n",
    "\n",
    "            self.train_kg_dict[h].append((t, r))\n",
    "            self.train_relation_dict[r].append((h, t))\n",
    "\n",
    "        self.h_list = torch.LongTensor(h_list)\n",
    "        self.t_list = torch.LongTensor(t_list)\n",
    "        self.r_list = torch.LongTensor(r_list)\n",
    "\n",
    "\n",
    "    def convert_coo2tensor(self, coo):\n",
    "        values = coo.data\n",
    "        indices = np.vstack((coo.row, coo.col))\n",
    "\n",
    "        i = torch.LongTensor(indices)\n",
    "        v = torch.FloatTensor(values)\n",
    "        shape = coo.shape\n",
    "        return torch.sparse.FloatTensor(i, v, torch.Size(shape))\n",
    "\n",
    "\n",
    "    def create_adjacency_dict(self):\n",
    "        # create adjacency matrix from head and tail \n",
    "        self.adjacency_dict = {}\n",
    "        for r, ht_list in self.train_relation_dict.items():\n",
    "            rows = [e[0] for e in ht_list]\n",
    "            cols = [e[1] for e in ht_list]\n",
    "            vals = [1] * len(rows)\n",
    "            adj = sp.coo_matrix((vals, (rows, cols)), shape=(self.n_users_entities, self.n_users_entities))\n",
    "            self.adjacency_dict[r] = adj\n",
    "\n",
    "\n",
    "    def create_laplacian_dict(self):\n",
    "        # create lapalacian adjacencyy matrix\n",
    "        def symmetric_norm_lap(adj):\n",
    "            rowsum = np.array(adj.sum(axis=1))\n",
    "\n",
    "            d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "            d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0\n",
    "            d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "\n",
    "            norm_adj = d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt)\n",
    "            return norm_adj.tocoo()\n",
    "\n",
    "        def random_walk_norm_lap(adj):\n",
    "            rowsum = np.array(adj.sum(axis=1))\n",
    "\n",
    "            d_inv = np.power(rowsum, -1.0).flatten()\n",
    "            d_inv[np.isinf(d_inv)] = 0\n",
    "            d_mat_inv = sp.diags(d_inv)\n",
    "\n",
    "            norm_adj = d_mat_inv.dot(adj)\n",
    "            return norm_adj.tocoo()\n",
    "\n",
    "        if self.laplacian_type == 'symmetric':\n",
    "            norm_lap_func = symmetric_norm_lap\n",
    "        elif self.laplacian_type == 'random-walk':\n",
    "            norm_lap_func = random_walk_norm_lap\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.laplacian_dict = {}\n",
    "        for r, adj in self.adjacency_dict.items():\n",
    "            self.laplacian_dict[r] = norm_lap_func(adj)\n",
    "\n",
    "        A_in = sum(self.laplacian_dict.values())\n",
    "        self.A_in = self.convert_coo2tensor(A_in.tocoo())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acad1f9c-5ae0-42bb-92d2-740cdbf1e1b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6554f172-e655-4244-bda2-8f210691d884",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def parse_kgat_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Run KGAT.\")\n",
    "\n",
    "    parser.add_argument('--seed', type=int, default=2019,\n",
    "                        help='Random seed.')\n",
    "\n",
    "    parser.add_argument('--data_name', nargs='?', default='lastfm',\n",
    "                        help='Choose a dataset from {yelp2018, lastfm, amazon-book}')\n",
    "    parser.add_argument('--data_dir', nargs='?', default='dataset/',\n",
    "                        help='Input data path.')\n",
    "    \n",
    "    parser.add_argument('--use_pretrain', type=int, default=0,\n",
    "                    help='0: No pretrain, 1: Pretrain with the learned embeddings, 2: Pretrain with stored model.')\n",
    "\n",
    "    parser.add_argument('--cf_batch_size', type=int, default=256,\n",
    "                        help='CF batch size.')\n",
    "    parser.add_argument('--kg_batch_size', type=int, default=256,\n",
    "                        help='KG batch size.')\n",
    "    parser.add_argument('--test_batch_size', type=int, default=256,\n",
    "                        help='Test batch size (the user number to test every batch).')\n",
    "\n",
    "    parser.add_argument('--embed_dim', type=int, default=64,\n",
    "                        help='User / entity Embedding size.')\n",
    "    parser.add_argument('--relation_dim', type=int, default=64,\n",
    "                        help='Relation Embedding size.')\n",
    "\n",
    "    parser.add_argument('--laplacian_type', type=str, default='random-walk',\n",
    "                        help='Specify the type of the adjacency (laplacian) matrix from {symmetric, random-walk}.')\n",
    "    parser.add_argument('--aggregation_type', type=str, default='bi-interaction',\n",
    "                        help='Specify the type of the aggregation layer from {gcn, graphsage, bi-interaction}.')\n",
    "    parser.add_argument('--conv_dim_list', nargs='?', default='[64, 32, 16]',\n",
    "                        help='Output sizes of every aggregation layer.')\n",
    "    parser.add_argument('--mess_dropout', nargs='?', default='[0.1, 0.1, 0.1]',\n",
    "                        help='Dropout probability w.r.t. message dropout for each deep layer. 0: no dropout.')\n",
    "\n",
    "    parser.add_argument('--kg_l2loss_lambda', type=float, default=1e-5,\n",
    "                        help='Lambda when calculating KG l2 loss.')\n",
    "    parser.add_argument('--cf_l2loss_lambda', type=float, default=1e-5,\n",
    "                        help='Lambda when calculating CF l2 loss.')\n",
    "\n",
    "    parser.add_argument('--lr', type=float, default=0.001,\n",
    "                        help='Learning rate.')\n",
    "    parser.add_argument('--n_epoch', type=int, default=100,\n",
    "                        help='Number of epoch.')\n",
    "    parser.add_argument('--stopping_steps', type=int, default=10,\n",
    "                        help='Number of epoch for early stopping')\n",
    "\n",
    "    parser.add_argument('--cf_print_every', type=int, default=1,\n",
    "                        help='Iter interval of printing CF loss.')\n",
    "    parser.add_argument('--kg_print_every', type=int, default=1,\n",
    "                        help='Iter interval of printing KG loss.')\n",
    "    parser.add_argument('--evaluate_every', type=int, default=10,\n",
    "                        help='Epoch interval of evaluating CF.')\n",
    "\n",
    "    parser.add_argument('--Ks', nargs='?', default='[10, 20]',\n",
    "                        help='Calculate metric@K when evaluating.')\n",
    "\n",
    "    args, unknown = parser.parse_known_args()\n",
    "\n",
    "    save_dir = 'results/KGAT/{}/embed-dim{}_relation-dim{}_{}_{}_{}_lr{}/'.format(\n",
    "        args.data_name, args.embed_dim, args.relation_dim, args.laplacian_type, args.aggregation_type,\n",
    "        '-'.join([str(i) for i in eval(args.conv_dim_list)]), args.lr)\n",
    "    args.save_dir = save_dir\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5954e8-51b9-457e-8ff1-1efa6fc2e40e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f35ac4d-86a5-460e-914d-ab79b5fe0c12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _L2_loss_mean(x):\n",
    "    return torch.mean(torch.sum(torch.pow(x, 2), dim=1, keepdim=False) / 2.)\n",
    "\n",
    "class Aggregator(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, dropout, aggregator_type):\n",
    "        super(Aggregator, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.dropout = dropout\n",
    "        self.aggregator_type = aggregator_type\n",
    "\n",
    "        self.message_dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.LeakyReLU()\n",
    "\n",
    "        if self.aggregator_type == 'gcn':\n",
    "            self.linear = nn.Linear(self.in_dim, self.out_dim)       # W in Equation (6)\n",
    "            nn.init.xavier_uniform_(self.linear.weight)\n",
    "\n",
    "        elif self.aggregator_type == 'graphsage':\n",
    "            self.linear = nn.Linear(self.in_dim * 2, self.out_dim)   # W in Equation (7)\n",
    "            nn.init.xavier_uniform_(self.linear.weight)\n",
    "\n",
    "        elif self.aggregator_type == 'bi-interaction':\n",
    "            self.linear1 = nn.Linear(self.in_dim, self.out_dim)      # W1 in Equation (8)\n",
    "            self.linear2 = nn.Linear(self.in_dim, self.out_dim)      # W2 in Equation (8)\n",
    "            nn.init.xavier_uniform_(self.linear1.weight)\n",
    "            nn.init.xavier_uniform_(self.linear2.weight)\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "\n",
    "    def forward(self, ego_embeddings, A_in):\n",
    "        \"\"\"\n",
    "        ego_embeddings:  (n_users + n_entities, in_dim)\n",
    "        A_in:            (n_users + n_entities, n_users + n_entities), torch.sparse.FloatTensor\n",
    "        \"\"\"\n",
    "        # Equation (3)\n",
    "        side_embeddings = torch.matmul(A_in, ego_embeddings)\n",
    "\n",
    "        if self.aggregator_type == 'gcn':\n",
    "            # Equation (6) & (9)\n",
    "            embeddings = ego_embeddings + side_embeddings\n",
    "            embeddings = self.activation(self.linear(embeddings))\n",
    "\n",
    "        elif self.aggregator_type == 'graphsage':\n",
    "            # Equation (7) & (9)\n",
    "            embeddings = torch.cat([ego_embeddings, side_embeddings], dim=1)\n",
    "            embeddings = self.activation(self.linear(embeddings))\n",
    "\n",
    "        elif self.aggregator_type == 'bi-interaction':\n",
    "            # Equation (8) & (9)\n",
    "            sum_embeddings = self.activation(self.linear1(ego_embeddings + side_embeddings))\n",
    "            bi_embeddings = self.activation(self.linear2(ego_embeddings * side_embeddings))\n",
    "            embeddings = bi_embeddings + sum_embeddings\n",
    "\n",
    "        embeddings = self.message_dropout(embeddings)           # (n_users + n_entities, out_dim)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "678d4e89-6c94-4c49-ba09-66a7998568d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class KGAT(nn.Module):\n",
    "\n",
    "    def __init__(self, args,\n",
    "                 n_users, n_entities, n_relations, A_in=None,\n",
    "                 user_pre_embed=None, item_pre_embed=None):\n",
    "\n",
    "        super(KGAT, self).__init__()\n",
    "        self.use_pretrain = args.use_pretrain\n",
    "\n",
    "        self.n_users = n_users\n",
    "        self.n_entities = n_entities\n",
    "        self.n_relations = n_relations\n",
    "\n",
    "        self.embed_dim = args.embed_dim\n",
    "        self.relation_dim = args.relation_dim\n",
    "\n",
    "        self.aggregation_type = args.aggregation_type\n",
    "        self.conv_dim_list = [args.embed_dim] + eval(args.conv_dim_list)\n",
    "        self.mess_dropout = eval(args.mess_dropout)\n",
    "        self.n_layers = len(eval(args.conv_dim_list))\n",
    "\n",
    "        self.kg_l2loss_lambda = args.kg_l2loss_lambda\n",
    "        self.cf_l2loss_lambda = args.cf_l2loss_lambda\n",
    "\n",
    "        self.entity_user_embed = nn.Embedding(self.n_entities + self.n_users, self.embed_dim)\n",
    "        self.relation_embed = nn.Embedding(self.n_relations, self.relation_dim)\n",
    "        self.trans_M = nn.Parameter(torch.Tensor(self.n_relations, self.embed_dim, self.relation_dim))\n",
    "\n",
    "        if (self.use_pretrain == 1) and (user_pre_embed is not None) and (item_pre_embed is not None):\n",
    "            other_entity_embed = nn.Parameter(torch.Tensor(self.n_entities - item_pre_embed.shape[0], self.embed_dim))\n",
    "            nn.init.xavier_uniform_(other_entity_embed)\n",
    "            entity_user_embed = torch.cat([item_pre_embed, other_entity_embed, user_pre_embed], dim=0)\n",
    "            self.entity_user_embed.weight = nn.Parameter(entity_user_embed)\n",
    "        else:\n",
    "            nn.init.xavier_uniform_(self.entity_user_embed.weight)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.relation_embed.weight)\n",
    "        nn.init.xavier_uniform_(self.trans_M)\n",
    "\n",
    "        self.aggregator_layers = nn.ModuleList()\n",
    "        for k in range(self.n_layers):\n",
    "            self.aggregator_layers.append(Aggregator(self.conv_dim_list[k], self.conv_dim_list[k + 1], self.mess_dropout[k], self.aggregation_type))\n",
    "\n",
    "        self.A_in = nn.Parameter(torch.sparse.FloatTensor(self.n_users + self.n_entities, self.n_users + self.n_entities))\n",
    "        if A_in is not None:\n",
    "            self.A_in.data = A_in\n",
    "        self.A_in.requires_grad = False\n",
    "\n",
    "    def calc_cf_embeddings(self):\n",
    "        ego_embed = self.entity_user_embed.weight\n",
    "        all_embed = [ego_embed]\n",
    "\n",
    "        for idx, layer in enumerate(self.aggregator_layers):\n",
    "            ego_embed = layer(ego_embed, self.A_in)\n",
    "            norm_embed = F.normalize(ego_embed, p=2, dim=1)\n",
    "            all_embed.append(norm_embed)\n",
    "\n",
    "        # Equation (11)\n",
    "        all_embed = torch.cat(all_embed, dim=1)         # (n_users + n_entities, concat_dim)\n",
    "        return all_embed\n",
    "\n",
    "\n",
    "    def calc_cf_loss(self, user_ids, item_pos_ids, item_neg_ids):\n",
    "        \"\"\"\n",
    "        user_ids:       (cf_batch_size)\n",
    "        item_pos_ids:   (cf_batch_size)\n",
    "        item_neg_ids:   (cf_batch_size)\n",
    "        \"\"\"\n",
    "        all_embed = self.calc_cf_embeddings()                       # (n_users + n_entities, concat_dim)\n",
    "        user_embed = all_embed[user_ids]                            # (cf_batch_size, concat_dim)\n",
    "        item_pos_embed = all_embed[item_pos_ids]                    # (cf_batch_size, concat_dim)\n",
    "        item_neg_embed = all_embed[item_neg_ids]                    # (cf_batch_size, concat_dim)\n",
    "\n",
    "        # Equation (12)\n",
    "        pos_score = torch.sum(user_embed * item_pos_embed, dim=1)   # (cf_batch_size)\n",
    "        neg_score = torch.sum(user_embed * item_neg_embed, dim=1)   # (cf_batch_size)\n",
    "\n",
    "        # Equation (13)\n",
    "        # cf_loss = F.softplus(neg_score - pos_score)\n",
    "        cf_loss = (-1.0) * F.logsigmoid(pos_score - neg_score)\n",
    "        cf_loss = torch.mean(cf_loss)\n",
    "\n",
    "        l2_loss = _L2_loss_mean(user_embed) + _L2_loss_mean(item_pos_embed) + _L2_loss_mean(item_neg_embed)\n",
    "        loss = cf_loss + self.cf_l2loss_lambda * l2_loss\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def calc_kg_loss(self, h, r, pos_t, neg_t):\n",
    "        \"\"\"\n",
    "        h:      (kg_batch_size)\n",
    "        r:      (kg_batch_size)\n",
    "        pos_t:  (kg_batch_size)\n",
    "        neg_t:  (kg_batch_size)\n",
    "        \"\"\"\n",
    "        r_embed = self.relation_embed(r)                                                # (kg_batch_size, relation_dim)\n",
    "        W_r = self.trans_M[r]                                                           # (kg_batch_size, embed_dim, relation_dim)\n",
    "\n",
    "        h_embed = self.entity_user_embed(h)                                             # (kg_batch_size, embed_dim)\n",
    "        pos_t_embed = self.entity_user_embed(pos_t)                                     # (kg_batch_size, embed_dim)\n",
    "        neg_t_embed = self.entity_user_embed(neg_t)                                     # (kg_batch_size, embed_dim)\n",
    "\n",
    "        r_mul_h = torch.bmm(h_embed.unsqueeze(1), W_r).squeeze(1)                       # (kg_batch_size, relation_dim)\n",
    "        r_mul_pos_t = torch.bmm(pos_t_embed.unsqueeze(1), W_r).squeeze(1)               # (kg_batch_size, relation_dim)\n",
    "        r_mul_neg_t = torch.bmm(neg_t_embed.unsqueeze(1), W_r).squeeze(1)               # (kg_batch_size, relation_dim)\n",
    "\n",
    "        # Equation (1)\n",
    "        pos_score = torch.sum(torch.pow(r_mul_h + r_embed - r_mul_pos_t, 2), dim=1)     # (kg_batch_size)\n",
    "        neg_score = torch.sum(torch.pow(r_mul_h + r_embed - r_mul_neg_t, 2), dim=1)     # (kg_batch_size)\n",
    "\n",
    "        # Equation (2)\n",
    "        # kg_loss = F.softplus(pos_score - neg_score)\n",
    "        kg_loss = (-1.0) * F.logsigmoid(neg_score - pos_score)\n",
    "        kg_loss = torch.mean(kg_loss)\n",
    "\n",
    "        l2_loss = _L2_loss_mean(r_mul_h) + _L2_loss_mean(r_embed) + _L2_loss_mean(r_mul_pos_t) + _L2_loss_mean(r_mul_neg_t)\n",
    "        loss = kg_loss + self.kg_l2loss_lambda * l2_loss\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def update_attention_batch(self, h_list, t_list, r_idx):\n",
    "        r_embed = self.relation_embed.weight[r_idx]\n",
    "        W_r = self.trans_M[r_idx]\n",
    "\n",
    "        h_embed = self.entity_user_embed.weight[h_list]\n",
    "        t_embed = self.entity_user_embed.weight[t_list]\n",
    "\n",
    "        # Equation (4)\n",
    "        r_mul_h = torch.matmul(h_embed, W_r)\n",
    "        r_mul_t = torch.matmul(t_embed, W_r)\n",
    "        v_list = torch.sum(r_mul_t * torch.tanh(r_mul_h + r_embed), dim=1)\n",
    "        return v_list\n",
    "\n",
    "\n",
    "    def update_attention(self, h_list, t_list, r_list, relations):\n",
    "        device = self.A_in.device\n",
    "\n",
    "        rows = []\n",
    "        cols = []\n",
    "        values = []\n",
    "\n",
    "        for r_idx in relations:\n",
    "            index_list = torch.where(r_list == r_idx)\n",
    "            batch_h_list = h_list[index_list]\n",
    "            batch_t_list = t_list[index_list]\n",
    "\n",
    "            batch_v_list = self.update_attention_batch(batch_h_list, batch_t_list, r_idx)\n",
    "            rows.append(batch_h_list)\n",
    "            cols.append(batch_t_list)\n",
    "            values.append(batch_v_list)\n",
    "\n",
    "        rows = torch.cat(rows)\n",
    "        cols = torch.cat(cols)\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        indices = torch.stack([rows, cols])\n",
    "        shape = self.A_in.shape\n",
    "        A_in = torch.sparse.FloatTensor(indices, values, torch.Size(shape))\n",
    "\n",
    "        # Equation (5)\n",
    "        A_in = torch.sparse.softmax(A_in.cpu(), dim=1)\n",
    "        self.A_in.data = A_in.to(device)\n",
    "\n",
    "\n",
    "    def calc_score(self, user_ids, item_ids):\n",
    "        \"\"\"\n",
    "        user_ids:  (n_users)\n",
    "        item_ids:  (n_items)\n",
    "        \"\"\"\n",
    "        all_embed = self.calc_cf_embeddings()           # (n_users + n_entities, concat_dim)\n",
    "        user_embed = all_embed[user_ids]                # (n_users, concat_dim)\n",
    "        item_embed = all_embed[item_ids]                # (n_items, concat_dim)\n",
    "\n",
    "        # Equation (12)\n",
    "        cf_score = torch.matmul(user_embed, item_embed.transpose(0, 1))    # (n_users, n_items)\n",
    "        return cf_score\n",
    "\n",
    "\n",
    "    def forward(self, *input, mode):\n",
    "        if mode == 'train_cf':\n",
    "            return self.calc_cf_loss(*input)\n",
    "        if mode == 'train_kg':\n",
    "            return self.calc_kg_loss(*input)\n",
    "        if mode == 'update_att':\n",
    "            return self.update_attention(*input)\n",
    "        if mode == 'predict':\n",
    "            return self.calc_score(*input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fd232fc-553d-4177-b52c-5dde2189fd8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LightGCN(nn.Module):\n",
    "    def __init__(self, data, emb_size, n_layers):\n",
    "        super(LightGCN, self).__init__()\n",
    "        self.data = data\n",
    "        self.latent_size = emb_size\n",
    "        self.layers = n_layers\n",
    "        self.norm_adj = data.cf_norm_adj\n",
    "        self.embedding_dict = self._init_model()\n",
    "        self.sparse_norm_adj = convert_sparse_mat_to_tensor(self.norm_adj).cuda()\n",
    "\n",
    "    def _init_model(self):\n",
    "        initializer = nn.init.xavier_uniform_\n",
    "        embedding_dict = nn.ParameterDict({\n",
    "            'user_emb': nn.Parameter(initializer(torch.empty(self.data.n_users, self.latent_size))).cuda(),\n",
    "            'item_emb': nn.Parameter(initializer(torch.empty(self.data.n_items, self.latent_size))).cuda()\n",
    "        })\n",
    "        return embedding_dict\n",
    "\n",
    "    def forward(self):\n",
    "        ego_embeddings = torch.cat([self.embedding_dict['user_emb'], self.embedding_dict['item_emb']], 0)\n",
    "        all_embeddings = [ego_embeddings]\n",
    "        for k in range(self.layers):\n",
    "            ego_embeddings = torch.sparse.mm(self.sparse_norm_adj, ego_embeddings)\n",
    "            all_embeddings += [ego_embeddings]\n",
    "        all_embeddings = torch.stack(all_embeddings, dim=1)\n",
    "        all_embeddings = torch.mean(all_embeddings, dim=1)\n",
    "        user_all_embeddings = all_embeddings[:self.data.n_users]\n",
    "        item_all_embeddings = all_embeddings[self.data.n_users:]\n",
    "        return user_all_embeddings, item_all_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c957a073-20f1-4d00-878a-6c35a11e76dc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69c3146b-a763-481b-85aa-ec9e55fb839c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, log_loss, mean_squared_error\n",
    "\n",
    "\n",
    "def calc_recall(rank, ground_truth, k):\n",
    "    \"\"\"\n",
    "    calculate recall of one example\n",
    "    \"\"\"\n",
    "    return len(set(rank[:k]) & set(ground_truth)) / float(len(set(ground_truth)))\n",
    "\n",
    "def hit_ratio_at_k(hit, k):\n",
    "    '''\n",
    "    calc hit ratio of one example \n",
    "    '''\n",
    "    hit_k = np.asarray(hit)[:k]\n",
    "    return np.sum(hit_k) / np.sum(hit)\n",
    "\n",
    "def hit_ratio_at_k_batch(hits, k):\n",
    "    res = hits[:, :k].sum(axis=1) / hits.sum(axis=1)  \n",
    "    return res \n",
    "\n",
    "def precision_at_k(hit, k):\n",
    "    \"\"\"\n",
    "    calculate Precision@k\n",
    "    hit: list, element is binary (0 / 1)\n",
    "    \"\"\"\n",
    "    hit = np.asarray(hit)[:k]\n",
    "    return np.mean(hit)\n",
    "\n",
    "\n",
    "def precision_at_k_batch(hits, k):\n",
    "    \"\"\"\n",
    "    calculate Precision@k\n",
    "    hits: array, element is binary (0 / 1), 2-dim\n",
    "    \"\"\"\n",
    "    res = hits[:, :k].mean(axis=1)\n",
    "    return res\n",
    "\n",
    "\n",
    "def average_precision(hit, cut):\n",
    "    \"\"\"\n",
    "    calculate average precision (area under PR curve)\n",
    "    hit: list, element is binary (0 / 1)\n",
    "    \"\"\"\n",
    "    hit = np.asarray(hit)\n",
    "    precisions = [precision_at_k(hit, k + 1) for k in range(cut) if len(hit) >= k]\n",
    "    if not precisions:\n",
    "        return 0.\n",
    "    return np.sum(precisions) / float(min(cut, np.sum(hit)))\n",
    "\n",
    "\n",
    "def dcg_at_k(rel, k):\n",
    "    \"\"\"\n",
    "    calculate discounted cumulative gain (dcg)\n",
    "    rel: list, element is positive real values, can be binary\n",
    "    \"\"\"\n",
    "    rel = np.asfarray(rel)[:k]\n",
    "    dcg = np.sum((2 ** rel - 1) / np.log2(np.arange(2, rel.size + 2)))\n",
    "    return dcg\n",
    "\n",
    "\n",
    "def ndcg_at_k(rel, k):\n",
    "    \"\"\"\n",
    "    calculate normalized discounted cumulative gain (ndcg)\n",
    "    rel: list, element is positive real values, can be binary\n",
    "    \"\"\"\n",
    "    idcg = dcg_at_k(sorted(rel, reverse=True), k)\n",
    "    if not idcg:\n",
    "        return 0.\n",
    "    return dcg_at_k(rel, k) / idcg\n",
    "\n",
    "\n",
    "def ndcg_at_k_batch(hits, k):\n",
    "    \"\"\"\n",
    "    calculate NDCG@k\n",
    "    hits: array, element is binary (0 / 1), 2-dim\n",
    "    \"\"\"\n",
    "    hits_k = hits[:, :k]\n",
    "    dcg = np.sum((2 ** hits_k - 1) / np.log2(np.arange(2, k + 2)), axis=1)\n",
    "\n",
    "    sorted_hits_k = np.flip(np.sort(hits), axis=1)[:, :k]\n",
    "    idcg = np.sum((2 ** sorted_hits_k - 1) / np.log2(np.arange(2, k + 2)), axis=1)\n",
    "\n",
    "    idcg[idcg == 0] = np.inf\n",
    "    ndcg = (dcg / idcg)\n",
    "    return ndcg\n",
    "\n",
    "\n",
    "def recall_at_k(hit, k, all_pos_num):\n",
    "    \"\"\"\n",
    "    calculate Recall@k\n",
    "    hit: list, element is binary (0 / 1)\n",
    "    \"\"\"\n",
    "    hit = np.asfarray(hit)[:k]\n",
    "    return np.sum(hit) / all_pos_num\n",
    "\n",
    "\n",
    "def recall_at_k_batch(hits, k):\n",
    "    \"\"\"\n",
    "    calculate Recall@k\n",
    "    hits: array, element is binary (0 / 1), 2-dim\n",
    "    \"\"\"\n",
    "    res = (hits[:, :k].sum(axis=1) / hits.sum(axis=1))\n",
    "    return res\n",
    "\n",
    "\n",
    "def F1(pre, rec):\n",
    "    if pre + rec > 0:\n",
    "        return (2.0 * pre * rec) / (pre + rec)\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "\n",
    "def calc_auc(ground_truth, prediction):\n",
    "    try:\n",
    "        res = roc_auc_score(y_true=ground_truth, y_score=prediction)\n",
    "    except Exception:\n",
    "        res = 0.\n",
    "    return res\n",
    "\n",
    "\n",
    "def logloss(ground_truth, prediction):\n",
    "    logloss = log_loss(np.asarray(ground_truth), np.asarray(prediction))\n",
    "    return logloss\n",
    "\n",
    "\n",
    "def calc_metrics_at_k(cf_scores, train_user_dict, test_user_dict, user_ids, item_ids, Ks):\n",
    "    \"\"\"\n",
    "    cf_scores: (n_users, n_items)\n",
    "    \"\"\"\n",
    "    test_pos_item_binary = np.zeros([len(user_ids), len(item_ids)], dtype=np.float32)\n",
    "    for idx, u in enumerate(user_ids):\n",
    "        train_pos_item_list = train_user_dict[u]\n",
    "        test_pos_item_list = test_user_dict[u]\n",
    "        cf_scores[idx][train_pos_item_list] = -np.inf\n",
    "        test_pos_item_binary[idx][test_pos_item_list] = 1\n",
    "\n",
    "    try:\n",
    "        _, rank_indices = torch.sort(cf_scores.cuda(), descending=True)    # try to speed up the sorting process\n",
    "    except:\n",
    "        _, rank_indices = torch.sort(cf_scores, descending=True)\n",
    "    rank_indices = rank_indices.cpu()\n",
    "\n",
    "    binary_hit = []\n",
    "    for i in range(len(user_ids)):\n",
    "        binary_hit.append(test_pos_item_binary[i][rank_indices[i]])\n",
    "    binary_hit = np.array(binary_hit, dtype=np.float32)\n",
    "\n",
    "    metrics_dict = {}\n",
    "    for k in Ks:\n",
    "        metrics_dict[k] = {}\n",
    "        metrics_dict[k]['hit'] = hit_ratio_at_k_batch(binary_hit, k)\n",
    "        metrics_dict[k]['precision'] = precision_at_k_batch(binary_hit, k)\n",
    "        metrics_dict[k]['recall']    = recall_at_k_batch(binary_hit, k)\n",
    "        metrics_dict[k]['ndcg']      = ndcg_at_k_batch(binary_hit, k)\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa27eafc-49ae-415a-a6e4-f2f7d865fab5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "735ef40b-7d9d-4054-ae7c-9cb359d526ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def early_stopping(recall_list, stopping_steps):\n",
    "    best_recall = max(recall_list)\n",
    "    best_step = recall_list.index(best_recall)\n",
    "    if len(recall_list) - best_step - 1 >= stopping_steps:\n",
    "        should_stop = True\n",
    "    else:\n",
    "        should_stop = False\n",
    "    return best_recall, should_stop\n",
    "\n",
    "\n",
    "def save_model(model, model_dir, current_epoch, last_best_epoch=None):\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    model_state_file = os.path.join(model_dir, 'model_epoch{}.pth'.format(current_epoch))\n",
    "    torch.save({'model_state_dict': model.state_dict(), 'epoch': current_epoch}, model_state_file)\n",
    "\n",
    "    if last_best_epoch is not None and current_epoch != last_best_epoch:\n",
    "        old_model_state_file = os.path.join(model_dir, 'model_epoch{}.pth'.format(last_best_epoch))\n",
    "        if os.path.exists(old_model_state_file):\n",
    "            os.system('rm {}'.format(old_model_state_file))\n",
    "\n",
    "def convert_sparse_mat_to_tensor(X):\n",
    "    coo = X.tocoo()\n",
    "    i = torch.LongTensor([coo.row, coo.col])\n",
    "    v = torch.from_numpy(coo.data).float()\n",
    "    return torch.sparse.FloatTensor(i, v, coo.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bf447b-769f-4958-98e2-20c4a54f53cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a62b15b4-1670-404e-bbf6-cd1d37640bfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, Ks, device):\n",
    "    test_batch_size = dataloader.test_batch_size\n",
    "    train_user_dict = dataloader.train_user_dict\n",
    "    test_user_dict = dataloader.test_user_dict\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    user_ids = list(test_user_dict.keys())\n",
    "    user_ids_batches = [user_ids[i: i + test_batch_size] for i in range(0, len(user_ids), test_batch_size)]\n",
    "    user_ids_batches = [torch.LongTensor(d) for d in user_ids_batches]\n",
    "\n",
    "    n_items = dataloader.n_items\n",
    "    item_ids = torch.arange(n_items, dtype=torch.long).to(device)\n",
    "\n",
    "    cf_scores = []\n",
    "    metric_names = ['hit', 'precision', 'recall', 'ndcg']\n",
    "    metrics_dict = {k: {m: [] for m in metric_names} for k in Ks}\n",
    "\n",
    "    with tqdm(total=len(user_ids_batches), desc='Evaluating Iteration') as pbar:\n",
    "        for batch_user_ids in user_ids_batches:\n",
    "            batch_user_ids = batch_user_ids.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                batch_scores = model(batch_user_ids, item_ids, mode='predict')       # (n_batch_users, n_items)\n",
    "\n",
    "            batch_scores = batch_scores.cpu()\n",
    "            batch_metrics = calc_metrics_at_k(batch_scores, train_user_dict, test_user_dict, batch_user_ids.cpu().numpy(), item_ids.cpu().numpy(), Ks)\n",
    "\n",
    "            cf_scores.append(batch_scores.numpy())\n",
    "            for k in Ks:\n",
    "                for m in metric_names:\n",
    "                    metrics_dict[k][m].append(batch_metrics[k][m])\n",
    "            pbar.update(1)\n",
    "\n",
    "    cf_scores = np.concatenate(cf_scores, axis=0)\n",
    "    for k in Ks:\n",
    "        for m in metric_names:\n",
    "            metrics_dict[k][m] = np.concatenate(metrics_dict[k][m]).mean()\n",
    "    return cf_scores, metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "524fc10d-032d-4e15-8877-fa7dea867ae2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_loss(anchor_emb, pos_emb, neg_emb, batch_size, reg):\n",
    "    calc_reg_loss = EmbLoss()\n",
    "    rec_loss = bpr_loss(anchor_emb, pos_emb, neg_emb)\n",
    "    reg_loss = reg * calc_reg_loss(anchor_emb, pos_emb, neg_emb) / batch_size\n",
    "    return rec_loss, reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eaec0b6d-e59c-4265-a38f-1e54721305c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, data, args):\n",
    "    device = torch.device(\"cuda\" if  torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    cf_optimizer = optim.Adam(model.parameters(), lr = args.lr)\n",
    "    kg_optimizer = optim.Adam(model.parameters(), lr = args.lr)    \n",
    "    \n",
    "    # initialize metrics\n",
    "    best_epoch = -1\n",
    "    best_recall = 0\n",
    "    reg = 0.1\n",
    "    Ks = eval(args.Ks)\n",
    "    k_min = min(Ks)\n",
    "    k_max = max(Ks)\n",
    "\n",
    "    epoch_list = []\n",
    "    metrics_list = {k: {'hit': [],'precision': [], 'recall': [], 'ndcg': []} for k in Ks}\n",
    "\n",
    "    # train model\n",
    "    for epoch in range(1, args.n_epoch + 1):\n",
    "        time0 = time()\n",
    "        model.train()\n",
    "\n",
    "        # train cf\n",
    "        time1 = time()\n",
    "        cf_total_loss = 0\n",
    "        n_cf_batch = data.n_cf_train // data.cf_batch_size + 1\n",
    "\n",
    "        for iter in range(1, n_cf_batch + 1):\n",
    "            time2 = time()\n",
    "            cf_batch_user, cf_batch_pos_item, cf_batch_neg_item = data.generate_cf_batch(data.train_user_dict, data.cf_batch_size)\n",
    "            cf_batch_user = cf_batch_user.cuda()\n",
    "            cf_batch_pos_item = cf_batch_pos_item.cuda()\n",
    "            cf_batch_neg_item = cf_batch_neg_item.cuda()\n",
    "            \n",
    "#             user_emb, item_emb = model()\n",
    "#             anchor_emb = user_emb[cf_batch_user]\n",
    "#             pos_emb = item_emb[cf_batch_pos_item]\n",
    "#             neg_emb = item_emb[cf_batch_neg_item]\n",
    "            \n",
    "            # loss_rec, loss_reg = calculate_loss(anchor_emb, pos_emb, neg_emb, data.cf_batch_size, reg)\n",
    "            # cf_batch_loss = loss_rec + loss_reg \n",
    "            cf_batch_loss = model(cf_batch_user, cf_batch_pos_item, cf_batch_neg_item, mode='train_cf')\n",
    "\n",
    "            if np.isnan(cf_batch_loss.cpu().detach().numpy()):\n",
    "                print('ERROR (CF Training): Epoch {:04d} Iter {:04d} / {:04d} Loss is nan.'.format(epoch, iter, n_cf_batch))\n",
    "                return \n",
    "            \n",
    "            cf_batch_loss.backward()\n",
    "            cf_optimizer.step()\n",
    "            cf_optimizer.zero_grad()\n",
    "            cf_total_loss += cf_batch_loss.item()\n",
    "\n",
    "            if (iter % args.cf_print_every) == 0:\n",
    "                print('CF Training: Epoch {:04d} Iter {:04d} / {:04d} | Time {:.1f}s | Iter Loss {:.4f} | Iter Mean Loss {:.4f}'.format(epoch, iter, n_cf_batch, time() - time2, cf_batch_loss.item(), cf_total_loss / iter))\n",
    "        print('CF Training: Epoch {:04d} Total Iter {:04d} | Total Time {:.1f}s | Iter Mean Loss {:.4f}'.format(epoch, n_cf_batch, time() - time1, cf_total_loss / n_cf_batch))\n",
    "\n",
    "        # train kg\n",
    "        time3 = time()\n",
    "        kg_total_loss = 0\n",
    "        n_kg_batch = data.n_kg_train // data.kg_batch_size + 1\n",
    "\n",
    "        for iter in range(1, n_kg_batch + 1):\n",
    "            time4 = time()\n",
    "            kg_batch_head, kg_batch_relation, kg_batch_pos_tail, kg_batch_neg_tail = data.generate_kg_batch(data.train_kg_dict, data.kg_batch_size, data.n_users_entities)\n",
    "            kg_batch_head = kg_batch_head.to(device)\n",
    "            kg_batch_relation = kg_batch_relation.to(device)\n",
    "            kg_batch_pos_tail = kg_batch_pos_tail.to(device)\n",
    "            kg_batch_neg_tail = kg_batch_neg_tail.to(device)\n",
    "\n",
    "            kg_batch_loss = model(kg_batch_head, kg_batch_relation, kg_batch_pos_tail, kg_batch_neg_tail, mode='train_kg')\n",
    "\n",
    "            if np.isnan(kg_batch_loss.cpu().detach().numpy()):\n",
    "                print('ERROR (KG Training): Epoch {:04d} Iter {:04d} / {:04d} Loss is nan.'.format(epoch, iter, n_kg_batch))\n",
    "                return \n",
    "\n",
    "            kg_batch_loss.backward()\n",
    "            kg_optimizer.step()\n",
    "            kg_optimizer.zero_grad()\n",
    "            kg_total_loss += kg_batch_loss.item()\n",
    "\n",
    "            # if (iter % args.kg_print_every) == 0:\n",
    "            #     print('KG Training: Epoch {:04d} Iter {:04d} / {:04d} | Time {:.1f}s | Iter Loss {:.4f} | Iter Mean Loss {:.4f}'.format(epoch, iter, n_kg_batch, time() - time4, kg_batch_loss.item(), kg_total_loss / iter))\n",
    "        print('KG Training: Epoch {:04d} Total Iter {:04d} | Total Time {:.1f}s | Iter Mean Loss {:.4f}'.format(epoch, n_kg_batch, time() - time3, kg_total_loss / n_kg_batch))\n",
    "\n",
    "        # update attention\n",
    "        time5 = time()\n",
    "        h_list = data.h_list.to(device)\n",
    "        t_list = data.t_list.to(device)\n",
    "        r_list = data.r_list.to(device)\n",
    "        relations = list(data.laplacian_dict.keys())\n",
    "        model(h_list, t_list, r_list, relations, mode='update_att')\n",
    "        print('Update Attention: Epoch {:04d} | Total Time {:.1f}s'.format(epoch, time() - time5))\n",
    "\n",
    "        print('CF + KG Training: Epoch {:04d} | Total Time {:.1f}s'.format(epoch, time() - time0))\n",
    "\n",
    "        # evaluate cf\n",
    "        if (epoch % args.evaluate_every) == 0 or epoch == args.n_epoch:\n",
    "            time6 = time()\n",
    "            _, metrics_dict = evaluate(model, data, Ks, device)\n",
    "            print('CF Evaluation: Epoch {:04d} | Total Time {:.1f}s | Precision [{:.4f}, {:.4f}], Recall [{:.4f}, {:.4f}], NDCG [{:.4f}, {:.4f}]'.format(\n",
    "                epoch, time() - time6, metrics_dict[k_min]['precision'], metrics_dict[k_max]['precision'], metrics_dict[k_min]['recall'], metrics_dict[k_max]['recall'], metrics_dict[k_min]['ndcg'], metrics_dict[k_max]['ndcg']))\n",
    "\n",
    "            epoch_list.append(epoch)\n",
    "            for k in Ks:\n",
    "                for m in ['hit', 'precision', 'recall', 'ndcg']:\n",
    "                    metrics_list[k][m].append(metrics_dict[k][m])\n",
    "            best_recall, should_stop = early_stopping(metrics_list[k_min]['recall'], args.stopping_steps)\n",
    "\n",
    "            if should_stop:\n",
    "                break\n",
    "\n",
    "            if metrics_list[k_min]['recall'].index(best_recall) == len(epoch_list) - 1:\n",
    "                save_model(model, args.save_dir, epoch, best_epoch)\n",
    "                print('Save model on epoch {:04d}!'.format(epoch))\n",
    "                best_epoch = epoch\n",
    "\n",
    "    # save metrics\n",
    "    metrics_df = [epoch_list]\n",
    "    metrics_cols = ['epoch_idx']\n",
    "    for k in Ks:\n",
    "        for m in ['hit', 'precision', 'recall', 'ndcg']:\n",
    "            metrics_df.append(metrics_list[k][m])\n",
    "            metrics_cols.append('{}@{}'.format(m, k))\n",
    "            \n",
    "    metrics_df = pd.DataFrame(metrics_df).transpose()\n",
    "    metrics_df.columns = metrics_cols\n",
    "    metrics_df.to_csv(args.save_dir + '/metrics.csv', sep='\\t', index=False)\n",
    "\n",
    "    # print best metrics\n",
    "    best_metrics = metrics_df.loc[metrics_df['epoch_idx'] == best_epoch].iloc[0].to_dict()\n",
    "    print('Best CF Evaluation: Epoch {:04d} | Hit [{:.4f}, {:.4f}], Precision [{:.4f}, {:.4f}], Recall [{:.4f}, {:.4f}], NDCG [{:.4f}, {:.4f}]'.format(\n",
    "        int(best_metrics['epoch_idx']), best_metrics['hit@{}'.format(k_min)], best_metrics['hit@{}'.format(k_max)], best_metrics['precision@{}'.format(k_min)], best_metrics['precision@{}'.format(k_max)], best_metrics['recall@{}'.format(k_min)], best_metrics['recall@{}'.format(k_max)], best_metrics['ndcg@{}'.format(k_min)], best_metrics['ndcg@{}'.format(k_max)]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac08798-895e-4ef2-b096-5d31d6bf3eec",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54f4a5fe-5907-4f28-8822-735f34d1ee00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = parse_kgat_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cfaa9e8c-a492-401f-bb62-bf0197d4f8bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14070/3007558589.py:92: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -0.5).flatten()\n"
     ]
    }
   ],
   "source": [
    "data = DataLoader(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f28f4eba-3438-4a98-b18b-874af0be6f35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0,   0,   0, ..., 999, 999, 999], dtype=int32),\n",
       " array([    0,     1,     2, ..., 36342, 24408,   991], dtype=int32))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.cf_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "270bd880-5e10-4e39-a8bf-3870e8fc8e66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_pre_embed, item_pre_embed = None, None\n",
    "model = KGAT(args, data.n_users, data.n_entities, data.n_relations, data.A_in, user_pre_embed, item_pre_embed).cuda()\n",
    "# model = LightGCN(data, emb_size=128, n_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8760a37b-fd9c-43df-a401-c9db6d2bc1b5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CF Training: Epoch 0001 Iter 0001 / 0490 | Time 0.1s | Iter Loss 0.3682 | Iter Mean Loss 0.3682\n",
      "CF Training: Epoch 0001 Iter 0002 / 0490 | Time 0.1s | Iter Loss 0.3388 | Iter Mean Loss 0.3535\n",
      "CF Training: Epoch 0001 Iter 0003 / 0490 | Time 0.1s | Iter Loss 0.3554 | Iter Mean Loss 0.3541\n",
      "CF Training: Epoch 0001 Iter 0004 / 0490 | Time 0.1s | Iter Loss 0.3510 | Iter Mean Loss 0.3534\n",
      "CF Training: Epoch 0001 Iter 0005 / 0490 | Time 0.1s | Iter Loss 0.3412 | Iter Mean Loss 0.3509\n",
      "CF Training: Epoch 0001 Iter 0006 / 0490 | Time 0.1s | Iter Loss 0.3391 | Iter Mean Loss 0.3490\n",
      "CF Training: Epoch 0001 Iter 0007 / 0490 | Time 0.1s | Iter Loss 0.3492 | Iter Mean Loss 0.3490\n",
      "CF Training: Epoch 0001 Iter 0008 / 0490 | Time 0.1s | Iter Loss 0.3679 | Iter Mean Loss 0.3514\n",
      "CF Training: Epoch 0001 Iter 0009 / 0490 | Time 0.1s | Iter Loss 0.3350 | Iter Mean Loss 0.3496\n",
      "CF Training: Epoch 0001 Iter 0010 / 0490 | Time 0.1s | Iter Loss 0.3554 | Iter Mean Loss 0.3501\n",
      "CF Training: Epoch 0001 Iter 0011 / 0490 | Time 0.1s | Iter Loss 0.3162 | Iter Mean Loss 0.3470\n",
      "CF Training: Epoch 0001 Iter 0012 / 0490 | Time 0.1s | Iter Loss 0.3200 | Iter Mean Loss 0.3448\n",
      "CF Training: Epoch 0001 Iter 0013 / 0490 | Time 0.1s | Iter Loss 0.3389 | Iter Mean Loss 0.3443\n",
      "CF Training: Epoch 0001 Iter 0014 / 0490 | Time 0.1s | Iter Loss 0.3545 | Iter Mean Loss 0.3451\n",
      "CF Training: Epoch 0001 Iter 0015 / 0490 | Time 0.1s | Iter Loss 0.3553 | Iter Mean Loss 0.3457\n",
      "CF Training: Epoch 0001 Iter 0016 / 0490 | Time 0.1s | Iter Loss 0.3684 | Iter Mean Loss 0.3472\n",
      "CF Training: Epoch 0001 Iter 0017 / 0490 | Time 0.1s | Iter Loss 0.3870 | Iter Mean Loss 0.3495\n",
      "CF Training: Epoch 0001 Iter 0018 / 0490 | Time 0.1s | Iter Loss 0.3297 | Iter Mean Loss 0.3484\n",
      "CF Training: Epoch 0001 Iter 0019 / 0490 | Time 0.1s | Iter Loss 0.3197 | Iter Mean Loss 0.3469\n",
      "CF Training: Epoch 0001 Iter 0020 / 0490 | Time 0.1s | Iter Loss 0.3291 | Iter Mean Loss 0.3460\n",
      "CF Training: Epoch 0001 Iter 0021 / 0490 | Time 0.1s | Iter Loss 0.3342 | Iter Mean Loss 0.3454\n",
      "CF Training: Epoch 0001 Iter 0022 / 0490 | Time 0.1s | Iter Loss 0.3555 | Iter Mean Loss 0.3459\n",
      "CF Training: Epoch 0001 Iter 0023 / 0490 | Time 0.1s | Iter Loss 0.3439 | Iter Mean Loss 0.3458\n",
      "CF Training: Epoch 0001 Iter 0024 / 0490 | Time 0.1s | Iter Loss 0.2967 | Iter Mean Loss 0.3438\n",
      "CF Training: Epoch 0001 Iter 0025 / 0490 | Time 0.1s | Iter Loss 0.3300 | Iter Mean Loss 0.3432\n",
      "CF Training: Epoch 0001 Iter 0026 / 0490 | Time 0.1s | Iter Loss 0.3176 | Iter Mean Loss 0.3422\n",
      "CF Training: Epoch 0001 Iter 0027 / 0490 | Time 0.1s | Iter Loss 0.2910 | Iter Mean Loss 0.3403\n",
      "CF Training: Epoch 0001 Iter 0028 / 0490 | Time 0.1s | Iter Loss 0.3346 | Iter Mean Loss 0.3401\n",
      "CF Training: Epoch 0001 Iter 0029 / 0490 | Time 0.1s | Iter Loss 0.3291 | Iter Mean Loss 0.3397\n",
      "CF Training: Epoch 0001 Iter 0030 / 0490 | Time 0.1s | Iter Loss 0.3135 | Iter Mean Loss 0.3389\n",
      "CF Training: Epoch 0001 Iter 0031 / 0490 | Time 0.1s | Iter Loss 0.2955 | Iter Mean Loss 0.3375\n",
      "CF Training: Epoch 0001 Iter 0032 / 0490 | Time 0.1s | Iter Loss 0.2979 | Iter Mean Loss 0.3362\n",
      "CF Training: Epoch 0001 Iter 0033 / 0490 | Time 0.1s | Iter Loss 0.3420 | Iter Mean Loss 0.3364\n",
      "CF Training: Epoch 0001 Iter 0034 / 0490 | Time 0.1s | Iter Loss 0.3303 | Iter Mean Loss 0.3362\n",
      "CF Training: Epoch 0001 Iter 0035 / 0490 | Time 0.1s | Iter Loss 0.2712 | Iter Mean Loss 0.3344\n",
      "CF Training: Epoch 0001 Iter 0036 / 0490 | Time 0.1s | Iter Loss 0.3249 | Iter Mean Loss 0.3341\n",
      "CF Training: Epoch 0001 Iter 0037 / 0490 | Time 0.1s | Iter Loss 0.3398 | Iter Mean Loss 0.3343\n",
      "CF Training: Epoch 0001 Iter 0038 / 0490 | Time 0.1s | Iter Loss 0.3805 | Iter Mean Loss 0.3355\n",
      "CF Training: Epoch 0001 Iter 0039 / 0490 | Time 0.1s | Iter Loss 0.3107 | Iter Mean Loss 0.3348\n",
      "CF Training: Epoch 0001 Iter 0040 / 0490 | Time 0.1s | Iter Loss 0.3005 | Iter Mean Loss 0.3340\n",
      "CF Training: Epoch 0001 Iter 0041 / 0490 | Time 0.1s | Iter Loss 0.3123 | Iter Mean Loss 0.3335\n",
      "CF Training: Epoch 0001 Iter 0042 / 0490 | Time 0.1s | Iter Loss 0.3428 | Iter Mean Loss 0.3337\n",
      "CF Training: Epoch 0001 Iter 0043 / 0490 | Time 0.1s | Iter Loss 0.3529 | Iter Mean Loss 0.3341\n",
      "CF Training: Epoch 0001 Iter 0044 / 0490 | Time 0.1s | Iter Loss 0.3278 | Iter Mean Loss 0.3340\n",
      "CF Training: Epoch 0001 Iter 0045 / 0490 | Time 0.1s | Iter Loss 0.3371 | Iter Mean Loss 0.3341\n",
      "CF Training: Epoch 0001 Iter 0046 / 0490 | Time 0.1s | Iter Loss 0.3380 | Iter Mean Loss 0.3341\n",
      "CF Training: Epoch 0001 Iter 0047 / 0490 | Time 0.1s | Iter Loss 0.2971 | Iter Mean Loss 0.3334\n",
      "CF Training: Epoch 0001 Iter 0048 / 0490 | Time 0.1s | Iter Loss 0.3137 | Iter Mean Loss 0.3329\n",
      "CF Training: Epoch 0001 Iter 0049 / 0490 | Time 0.1s | Iter Loss 0.3344 | Iter Mean Loss 0.3330\n",
      "CF Training: Epoch 0001 Iter 0050 / 0490 | Time 0.1s | Iter Loss 0.3055 | Iter Mean Loss 0.3324\n",
      "CF Training: Epoch 0001 Iter 0051 / 0490 | Time 0.1s | Iter Loss 0.2990 | Iter Mean Loss 0.3318\n",
      "CF Training: Epoch 0001 Iter 0052 / 0490 | Time 0.1s | Iter Loss 0.3614 | Iter Mean Loss 0.3323\n",
      "CF Training: Epoch 0001 Iter 0053 / 0490 | Time 0.0s | Iter Loss 0.3050 | Iter Mean Loss 0.3318\n",
      "CF Training: Epoch 0001 Iter 0054 / 0490 | Time 0.1s | Iter Loss 0.3308 | Iter Mean Loss 0.3318\n",
      "CF Training: Epoch 0001 Iter 0055 / 0490 | Time 0.1s | Iter Loss 0.3200 | Iter Mean Loss 0.3316\n",
      "CF Training: Epoch 0001 Iter 0056 / 0490 | Time 0.1s | Iter Loss 0.3369 | Iter Mean Loss 0.3317\n",
      "CF Training: Epoch 0001 Iter 0057 / 0490 | Time 0.1s | Iter Loss 0.3374 | Iter Mean Loss 0.3318\n",
      "CF Training: Epoch 0001 Iter 0058 / 0490 | Time 0.1s | Iter Loss 0.3398 | Iter Mean Loss 0.3319\n",
      "CF Training: Epoch 0001 Iter 0059 / 0490 | Time 0.1s | Iter Loss 0.3339 | Iter Mean Loss 0.3320\n",
      "CF Training: Epoch 0001 Iter 0060 / 0490 | Time 0.1s | Iter Loss 0.3221 | Iter Mean Loss 0.3318\n",
      "CF Training: Epoch 0001 Iter 0061 / 0490 | Time 0.1s | Iter Loss 0.3189 | Iter Mean Loss 0.3316\n",
      "CF Training: Epoch 0001 Iter 0062 / 0490 | Time 0.0s | Iter Loss 0.3276 | Iter Mean Loss 0.3315\n",
      "CF Training: Epoch 0001 Iter 0063 / 0490 | Time 0.1s | Iter Loss 0.2940 | Iter Mean Loss 0.3309\n",
      "CF Training: Epoch 0001 Iter 0064 / 0490 | Time 0.1s | Iter Loss 0.3330 | Iter Mean Loss 0.3310\n",
      "CF Training: Epoch 0001 Iter 0065 / 0490 | Time 0.1s | Iter Loss 0.3032 | Iter Mean Loss 0.3305\n",
      "CF Training: Epoch 0001 Iter 0066 / 0490 | Time 0.1s | Iter Loss 0.2682 | Iter Mean Loss 0.3296\n",
      "CF Training: Epoch 0001 Iter 0067 / 0490 | Time 0.1s | Iter Loss 0.3101 | Iter Mean Loss 0.3293\n",
      "CF Training: Epoch 0001 Iter 0068 / 0490 | Time 0.1s | Iter Loss 0.3077 | Iter Mean Loss 0.3290\n",
      "CF Training: Epoch 0001 Iter 0069 / 0490 | Time 0.1s | Iter Loss 0.3308 | Iter Mean Loss 0.3290\n",
      "CF Training: Epoch 0001 Iter 0070 / 0490 | Time 0.1s | Iter Loss 0.3328 | Iter Mean Loss 0.3291\n",
      "CF Training: Epoch 0001 Iter 0071 / 0490 | Time 0.1s | Iter Loss 0.2896 | Iter Mean Loss 0.3285\n",
      "CF Training: Epoch 0001 Iter 0072 / 0490 | Time 0.1s | Iter Loss 0.3118 | Iter Mean Loss 0.3283\n",
      "CF Training: Epoch 0001 Iter 0073 / 0490 | Time 0.1s | Iter Loss 0.3728 | Iter Mean Loss 0.3289\n",
      "CF Training: Epoch 0001 Iter 0074 / 0490 | Time 0.1s | Iter Loss 0.3090 | Iter Mean Loss 0.3286\n",
      "CF Training: Epoch 0001 Iter 0075 / 0490 | Time 0.1s | Iter Loss 0.3026 | Iter Mean Loss 0.3283\n",
      "CF Training: Epoch 0001 Iter 0076 / 0490 | Time 0.1s | Iter Loss 0.3180 | Iter Mean Loss 0.3281\n",
      "CF Training: Epoch 0001 Iter 0077 / 0490 | Time 0.1s | Iter Loss 0.2936 | Iter Mean Loss 0.3277\n",
      "CF Training: Epoch 0001 Iter 0078 / 0490 | Time 0.1s | Iter Loss 0.3321 | Iter Mean Loss 0.3277\n",
      "CF Training: Epoch 0001 Iter 0079 / 0490 | Time 0.1s | Iter Loss 0.3074 | Iter Mean Loss 0.3275\n",
      "CF Training: Epoch 0001 Iter 0080 / 0490 | Time 0.1s | Iter Loss 0.2883 | Iter Mean Loss 0.3270\n",
      "CF Training: Epoch 0001 Iter 0081 / 0490 | Time 0.1s | Iter Loss 0.3368 | Iter Mean Loss 0.3271\n",
      "CF Training: Epoch 0001 Iter 0082 / 0490 | Time 0.1s | Iter Loss 0.2950 | Iter Mean Loss 0.3267\n",
      "CF Training: Epoch 0001 Iter 0083 / 0490 | Time 0.1s | Iter Loss 0.2983 | Iter Mean Loss 0.3264\n",
      "CF Training: Epoch 0001 Iter 0084 / 0490 | Time 0.1s | Iter Loss 0.3134 | Iter Mean Loss 0.3262\n",
      "CF Training: Epoch 0001 Iter 0085 / 0490 | Time 0.1s | Iter Loss 0.2757 | Iter Mean Loss 0.3256\n",
      "CF Training: Epoch 0001 Iter 0086 / 0490 | Time 0.1s | Iter Loss 0.2856 | Iter Mean Loss 0.3252\n",
      "CF Training: Epoch 0001 Iter 0087 / 0490 | Time 0.1s | Iter Loss 0.2732 | Iter Mean Loss 0.3246\n",
      "CF Training: Epoch 0001 Iter 0088 / 0490 | Time 0.1s | Iter Loss 0.3251 | Iter Mean Loss 0.3246\n",
      "CF Training: Epoch 0001 Iter 0089 / 0490 | Time 0.1s | Iter Loss 0.3046 | Iter Mean Loss 0.3243\n",
      "CF Training: Epoch 0001 Iter 0090 / 0490 | Time 0.1s | Iter Loss 0.3337 | Iter Mean Loss 0.3244\n",
      "CF Training: Epoch 0001 Iter 0091 / 0490 | Time 0.1s | Iter Loss 0.3039 | Iter Mean Loss 0.3242\n",
      "CF Training: Epoch 0001 Iter 0092 / 0490 | Time 0.1s | Iter Loss 0.3091 | Iter Mean Loss 0.3241\n",
      "CF Training: Epoch 0001 Iter 0093 / 0490 | Time 0.1s | Iter Loss 0.3178 | Iter Mean Loss 0.3240\n",
      "CF Training: Epoch 0001 Iter 0094 / 0490 | Time 0.1s | Iter Loss 0.2847 | Iter Mean Loss 0.3236\n",
      "CF Training: Epoch 0001 Iter 0095 / 0490 | Time 0.1s | Iter Loss 0.2754 | Iter Mean Loss 0.3231\n",
      "CF Training: Epoch 0001 Iter 0096 / 0490 | Time 0.1s | Iter Loss 0.3061 | Iter Mean Loss 0.3229\n",
      "CF Training: Epoch 0001 Iter 0097 / 0490 | Time 0.1s | Iter Loss 0.3101 | Iter Mean Loss 0.3228\n",
      "CF Training: Epoch 0001 Iter 0098 / 0490 | Time 0.1s | Iter Loss 0.3290 | Iter Mean Loss 0.3228\n",
      "CF Training: Epoch 0001 Iter 0099 / 0490 | Time 0.1s | Iter Loss 0.3469 | Iter Mean Loss 0.3231\n",
      "CF Training: Epoch 0001 Iter 0100 / 0490 | Time 0.1s | Iter Loss 0.3421 | Iter Mean Loss 0.3233\n",
      "CF Training: Epoch 0001 Iter 0101 / 0490 | Time 0.1s | Iter Loss 0.2541 | Iter Mean Loss 0.3226\n",
      "CF Training: Epoch 0001 Iter 0102 / 0490 | Time 0.1s | Iter Loss 0.2546 | Iter Mean Loss 0.3219\n",
      "CF Training: Epoch 0001 Iter 0103 / 0490 | Time 0.1s | Iter Loss 0.3021 | Iter Mean Loss 0.3217\n",
      "CF Training: Epoch 0001 Iter 0104 / 0490 | Time 0.1s | Iter Loss 0.2547 | Iter Mean Loss 0.3211\n",
      "CF Training: Epoch 0001 Iter 0105 / 0490 | Time 0.1s | Iter Loss 0.3015 | Iter Mean Loss 0.3209\n",
      "CF Training: Epoch 0001 Iter 0106 / 0490 | Time 0.1s | Iter Loss 0.2885 | Iter Mean Loss 0.3206\n",
      "CF Training: Epoch 0001 Iter 0107 / 0490 | Time 0.1s | Iter Loss 0.3143 | Iter Mean Loss 0.3205\n",
      "CF Training: Epoch 0001 Iter 0108 / 0490 | Time 0.1s | Iter Loss 0.3136 | Iter Mean Loss 0.3205\n",
      "CF Training: Epoch 0001 Iter 0109 / 0490 | Time 0.1s | Iter Loss 0.2931 | Iter Mean Loss 0.3202\n",
      "CF Training: Epoch 0001 Iter 0110 / 0490 | Time 0.1s | Iter Loss 0.2947 | Iter Mean Loss 0.3200\n",
      "CF Training: Epoch 0001 Iter 0111 / 0490 | Time 0.1s | Iter Loss 0.2696 | Iter Mean Loss 0.3195\n",
      "CF Training: Epoch 0001 Iter 0112 / 0490 | Time 0.1s | Iter Loss 0.2829 | Iter Mean Loss 0.3192\n",
      "CF Training: Epoch 0001 Iter 0113 / 0490 | Time 0.1s | Iter Loss 0.2574 | Iter Mean Loss 0.3186\n",
      "CF Training: Epoch 0001 Iter 0114 / 0490 | Time 0.1s | Iter Loss 0.2827 | Iter Mean Loss 0.3183\n",
      "CF Training: Epoch 0001 Iter 0115 / 0490 | Time 0.1s | Iter Loss 0.2487 | Iter Mean Loss 0.3177\n",
      "CF Training: Epoch 0001 Iter 0116 / 0490 | Time 0.1s | Iter Loss 0.2790 | Iter Mean Loss 0.3174\n",
      "CF Training: Epoch 0001 Iter 0117 / 0490 | Time 0.1s | Iter Loss 0.3694 | Iter Mean Loss 0.3178\n",
      "CF Training: Epoch 0001 Iter 0118 / 0490 | Time 0.1s | Iter Loss 0.2555 | Iter Mean Loss 0.3173\n",
      "CF Training: Epoch 0001 Iter 0119 / 0490 | Time 0.1s | Iter Loss 0.2910 | Iter Mean Loss 0.3171\n",
      "CF Training: Epoch 0001 Iter 0120 / 0490 | Time 0.1s | Iter Loss 0.2507 | Iter Mean Loss 0.3165\n",
      "CF Training: Epoch 0001 Iter 0121 / 0490 | Time 0.1s | Iter Loss 0.2882 | Iter Mean Loss 0.3163\n",
      "CF Training: Epoch 0001 Iter 0122 / 0490 | Time 0.1s | Iter Loss 0.2943 | Iter Mean Loss 0.3161\n",
      "CF Training: Epoch 0001 Iter 0123 / 0490 | Time 0.1s | Iter Loss 0.2762 | Iter Mean Loss 0.3158\n",
      "CF Training: Epoch 0001 Iter 0124 / 0490 | Time 0.1s | Iter Loss 0.3137 | Iter Mean Loss 0.3158\n",
      "CF Training: Epoch 0001 Iter 0125 / 0490 | Time 0.1s | Iter Loss 0.3330 | Iter Mean Loss 0.3159\n",
      "CF Training: Epoch 0001 Iter 0126 / 0490 | Time 0.1s | Iter Loss 0.2736 | Iter Mean Loss 0.3156\n",
      "CF Training: Epoch 0001 Iter 0127 / 0490 | Time 0.1s | Iter Loss 0.2745 | Iter Mean Loss 0.3153\n",
      "CF Training: Epoch 0001 Iter 0128 / 0490 | Time 0.1s | Iter Loss 0.3084 | Iter Mean Loss 0.3152\n",
      "CF Training: Epoch 0001 Iter 0129 / 0490 | Time 0.1s | Iter Loss 0.3222 | Iter Mean Loss 0.3153\n",
      "CF Training: Epoch 0001 Iter 0130 / 0490 | Time 0.1s | Iter Loss 0.2595 | Iter Mean Loss 0.3148\n",
      "CF Training: Epoch 0001 Iter 0131 / 0490 | Time 0.1s | Iter Loss 0.3141 | Iter Mean Loss 0.3148\n",
      "CF Training: Epoch 0001 Iter 0132 / 0490 | Time 0.1s | Iter Loss 0.3026 | Iter Mean Loss 0.3147\n",
      "CF Training: Epoch 0001 Iter 0133 / 0490 | Time 0.1s | Iter Loss 0.3381 | Iter Mean Loss 0.3149\n",
      "CF Training: Epoch 0001 Iter 0134 / 0490 | Time 0.1s | Iter Loss 0.2940 | Iter Mean Loss 0.3147\n",
      "CF Training: Epoch 0001 Iter 0135 / 0490 | Time 0.1s | Iter Loss 0.3128 | Iter Mean Loss 0.3147\n",
      "CF Training: Epoch 0001 Iter 0136 / 0490 | Time 0.1s | Iter Loss 0.3053 | Iter Mean Loss 0.3147\n",
      "CF Training: Epoch 0001 Iter 0137 / 0490 | Time 0.1s | Iter Loss 0.2932 | Iter Mean Loss 0.3145\n",
      "CF Training: Epoch 0001 Iter 0138 / 0490 | Time 0.1s | Iter Loss 0.3040 | Iter Mean Loss 0.3144\n",
      "CF Training: Epoch 0001 Iter 0139 / 0490 | Time 0.1s | Iter Loss 0.2969 | Iter Mean Loss 0.3143\n",
      "CF Training: Epoch 0001 Iter 0140 / 0490 | Time 0.1s | Iter Loss 0.2850 | Iter Mean Loss 0.3141\n",
      "CF Training: Epoch 0001 Iter 0141 / 0490 | Time 0.1s | Iter Loss 0.2790 | Iter Mean Loss 0.3138\n",
      "CF Training: Epoch 0001 Iter 0142 / 0490 | Time 0.1s | Iter Loss 0.2481 | Iter Mean Loss 0.3134\n",
      "CF Training: Epoch 0001 Iter 0143 / 0490 | Time 0.1s | Iter Loss 0.2787 | Iter Mean Loss 0.3131\n",
      "CF Training: Epoch 0001 Iter 0144 / 0490 | Time 0.1s | Iter Loss 0.2800 | Iter Mean Loss 0.3129\n",
      "CF Training: Epoch 0001 Iter 0145 / 0490 | Time 0.1s | Iter Loss 0.2878 | Iter Mean Loss 0.3127\n",
      "CF Training: Epoch 0001 Iter 0146 / 0490 | Time 0.1s | Iter Loss 0.3167 | Iter Mean Loss 0.3128\n",
      "CF Training: Epoch 0001 Iter 0147 / 0490 | Time 0.1s | Iter Loss 0.2715 | Iter Mean Loss 0.3125\n",
      "CF Training: Epoch 0001 Iter 0148 / 0490 | Time 0.1s | Iter Loss 0.2551 | Iter Mean Loss 0.3121\n",
      "CF Training: Epoch 0001 Iter 0149 / 0490 | Time 0.1s | Iter Loss 0.3085 | Iter Mean Loss 0.3121\n",
      "CF Training: Epoch 0001 Iter 0150 / 0490 | Time 0.1s | Iter Loss 0.3355 | Iter Mean Loss 0.3122\n",
      "CF Training: Epoch 0001 Iter 0151 / 0490 | Time 0.1s | Iter Loss 0.2706 | Iter Mean Loss 0.3120\n",
      "CF Training: Epoch 0001 Iter 0152 / 0490 | Time 0.1s | Iter Loss 0.2819 | Iter Mean Loss 0.3118\n",
      "CF Training: Epoch 0001 Iter 0153 / 0490 | Time 0.1s | Iter Loss 0.2607 | Iter Mean Loss 0.3114\n",
      "CF Training: Epoch 0001 Iter 0154 / 0490 | Time 0.1s | Iter Loss 0.2683 | Iter Mean Loss 0.3111\n",
      "CF Training: Epoch 0001 Iter 0155 / 0490 | Time 0.1s | Iter Loss 0.2946 | Iter Mean Loss 0.3110\n",
      "CF Training: Epoch 0001 Iter 0156 / 0490 | Time 0.1s | Iter Loss 0.2720 | Iter Mean Loss 0.3108\n",
      "CF Training: Epoch 0001 Iter 0157 / 0490 | Time 0.1s | Iter Loss 0.2881 | Iter Mean Loss 0.3106\n",
      "CF Training: Epoch 0001 Iter 0158 / 0490 | Time 0.1s | Iter Loss 0.3302 | Iter Mean Loss 0.3108\n",
      "CF Training: Epoch 0001 Iter 0159 / 0490 | Time 0.1s | Iter Loss 0.3099 | Iter Mean Loss 0.3108\n",
      "CF Training: Epoch 0001 Iter 0160 / 0490 | Time 0.1s | Iter Loss 0.2878 | Iter Mean Loss 0.3106\n",
      "CF Training: Epoch 0001 Iter 0161 / 0490 | Time 0.1s | Iter Loss 0.3063 | Iter Mean Loss 0.3106\n",
      "CF Training: Epoch 0001 Iter 0162 / 0490 | Time 0.1s | Iter Loss 0.2498 | Iter Mean Loss 0.3102\n",
      "CF Training: Epoch 0001 Iter 0163 / 0490 | Time 0.1s | Iter Loss 0.2625 | Iter Mean Loss 0.3099\n",
      "CF Training: Epoch 0001 Iter 0164 / 0490 | Time 0.1s | Iter Loss 0.3172 | Iter Mean Loss 0.3100\n",
      "CF Training: Epoch 0001 Iter 0165 / 0490 | Time 0.1s | Iter Loss 0.3058 | Iter Mean Loss 0.3099\n",
      "CF Training: Epoch 0001 Iter 0166 / 0490 | Time 0.1s | Iter Loss 0.2591 | Iter Mean Loss 0.3096\n",
      "CF Training: Epoch 0001 Iter 0167 / 0490 | Time 0.0s | Iter Loss 0.2962 | Iter Mean Loss 0.3096\n",
      "CF Training: Epoch 0001 Iter 0168 / 0490 | Time 0.1s | Iter Loss 0.2954 | Iter Mean Loss 0.3095\n",
      "CF Training: Epoch 0001 Iter 0169 / 0490 | Time 0.1s | Iter Loss 0.2682 | Iter Mean Loss 0.3092\n",
      "CF Training: Epoch 0001 Iter 0170 / 0490 | Time 0.1s | Iter Loss 0.2939 | Iter Mean Loss 0.3091\n",
      "CF Training: Epoch 0001 Iter 0171 / 0490 | Time 0.1s | Iter Loss 0.2552 | Iter Mean Loss 0.3088\n",
      "CF Training: Epoch 0001 Iter 0172 / 0490 | Time 0.1s | Iter Loss 0.2598 | Iter Mean Loss 0.3085\n",
      "CF Training: Epoch 0001 Iter 0173 / 0490 | Time 0.1s | Iter Loss 0.3137 | Iter Mean Loss 0.3086\n",
      "CF Training: Epoch 0001 Iter 0174 / 0490 | Time 0.1s | Iter Loss 0.2698 | Iter Mean Loss 0.3083\n",
      "CF Training: Epoch 0001 Iter 0175 / 0490 | Time 0.1s | Iter Loss 0.2626 | Iter Mean Loss 0.3081\n",
      "CF Training: Epoch 0001 Iter 0176 / 0490 | Time 0.1s | Iter Loss 0.2946 | Iter Mean Loss 0.3080\n",
      "CF Training: Epoch 0001 Iter 0177 / 0490 | Time 0.1s | Iter Loss 0.2942 | Iter Mean Loss 0.3079\n",
      "CF Training: Epoch 0001 Iter 0178 / 0490 | Time 0.1s | Iter Loss 0.2710 | Iter Mean Loss 0.3077\n",
      "CF Training: Epoch 0001 Iter 0179 / 0490 | Time 0.1s | Iter Loss 0.2366 | Iter Mean Loss 0.3073\n",
      "CF Training: Epoch 0001 Iter 0180 / 0490 | Time 0.1s | Iter Loss 0.2771 | Iter Mean Loss 0.3072\n",
      "CF Training: Epoch 0001 Iter 0181 / 0490 | Time 0.1s | Iter Loss 0.2260 | Iter Mean Loss 0.3067\n",
      "CF Training: Epoch 0001 Iter 0182 / 0490 | Time 0.1s | Iter Loss 0.3301 | Iter Mean Loss 0.3068\n",
      "CF Training: Epoch 0001 Iter 0183 / 0490 | Time 0.1s | Iter Loss 0.3282 | Iter Mean Loss 0.3069\n",
      "CF Training: Epoch 0001 Iter 0184 / 0490 | Time 0.1s | Iter Loss 0.2651 | Iter Mean Loss 0.3067\n",
      "CF Training: Epoch 0001 Iter 0185 / 0490 | Time 0.1s | Iter Loss 0.2510 | Iter Mean Loss 0.3064\n",
      "CF Training: Epoch 0001 Iter 0186 / 0490 | Time 0.0s | Iter Loss 0.2594 | Iter Mean Loss 0.3062\n",
      "CF Training: Epoch 0001 Iter 0187 / 0490 | Time 0.1s | Iter Loss 0.2665 | Iter Mean Loss 0.3060\n",
      "CF Training: Epoch 0001 Iter 0188 / 0490 | Time 0.1s | Iter Loss 0.3281 | Iter Mean Loss 0.3061\n",
      "CF Training: Epoch 0001 Iter 0189 / 0490 | Time 0.1s | Iter Loss 0.2653 | Iter Mean Loss 0.3059\n",
      "CF Training: Epoch 0001 Iter 0190 / 0490 | Time 0.1s | Iter Loss 0.2773 | Iter Mean Loss 0.3057\n",
      "CF Training: Epoch 0001 Iter 0191 / 0490 | Time 0.1s | Iter Loss 0.2677 | Iter Mean Loss 0.3055\n",
      "CF Training: Epoch 0001 Iter 0192 / 0490 | Time 0.1s | Iter Loss 0.2901 | Iter Mean Loss 0.3054\n",
      "CF Training: Epoch 0001 Iter 0193 / 0490 | Time 0.1s | Iter Loss 0.2985 | Iter Mean Loss 0.3054\n",
      "CF Training: Epoch 0001 Iter 0194 / 0490 | Time 0.1s | Iter Loss 0.2638 | Iter Mean Loss 0.3052\n",
      "CF Training: Epoch 0001 Iter 0195 / 0490 | Time 0.1s | Iter Loss 0.2666 | Iter Mean Loss 0.3050\n",
      "CF Training: Epoch 0001 Iter 0196 / 0490 | Time 0.1s | Iter Loss 0.2476 | Iter Mean Loss 0.3047\n",
      "CF Training: Epoch 0001 Iter 0197 / 0490 | Time 0.1s | Iter Loss 0.2513 | Iter Mean Loss 0.3044\n",
      "CF Training: Epoch 0001 Iter 0198 / 0490 | Time 0.1s | Iter Loss 0.2736 | Iter Mean Loss 0.3043\n",
      "CF Training: Epoch 0001 Iter 0199 / 0490 | Time 0.1s | Iter Loss 0.2784 | Iter Mean Loss 0.3041\n",
      "CF Training: Epoch 0001 Iter 0200 / 0490 | Time 0.1s | Iter Loss 0.2333 | Iter Mean Loss 0.3038\n",
      "CF Training: Epoch 0001 Iter 0201 / 0490 | Time 0.1s | Iter Loss 0.2826 | Iter Mean Loss 0.3037\n",
      "CF Training: Epoch 0001 Iter 0202 / 0490 | Time 0.1s | Iter Loss 0.2833 | Iter Mean Loss 0.3036\n",
      "CF Training: Epoch 0001 Iter 0203 / 0490 | Time 0.0s | Iter Loss 0.2815 | Iter Mean Loss 0.3035\n",
      "CF Training: Epoch 0001 Iter 0204 / 0490 | Time 0.1s | Iter Loss 0.2729 | Iter Mean Loss 0.3033\n",
      "CF Training: Epoch 0001 Iter 0205 / 0490 | Time 0.1s | Iter Loss 0.2598 | Iter Mean Loss 0.3031\n",
      "CF Training: Epoch 0001 Iter 0206 / 0490 | Time 0.1s | Iter Loss 0.2963 | Iter Mean Loss 0.3031\n",
      "CF Training: Epoch 0001 Iter 0207 / 0490 | Time 0.1s | Iter Loss 0.2534 | Iter Mean Loss 0.3028\n",
      "CF Training: Epoch 0001 Iter 0208 / 0490 | Time 0.1s | Iter Loss 0.3187 | Iter Mean Loss 0.3029\n",
      "CF Training: Epoch 0001 Iter 0209 / 0490 | Time 0.1s | Iter Loss 0.2652 | Iter Mean Loss 0.3027\n",
      "CF Training: Epoch 0001 Iter 0210 / 0490 | Time 0.1s | Iter Loss 0.2579 | Iter Mean Loss 0.3025\n",
      "CF Training: Epoch 0001 Iter 0211 / 0490 | Time 0.1s | Iter Loss 0.2850 | Iter Mean Loss 0.3024\n",
      "CF Training: Epoch 0001 Iter 0212 / 0490 | Time 0.1s | Iter Loss 0.2477 | Iter Mean Loss 0.3022\n",
      "CF Training: Epoch 0001 Iter 0213 / 0490 | Time 0.1s | Iter Loss 0.2522 | Iter Mean Loss 0.3019\n",
      "CF Training: Epoch 0001 Iter 0214 / 0490 | Time 0.1s | Iter Loss 0.2646 | Iter Mean Loss 0.3018\n",
      "CF Training: Epoch 0001 Iter 0215 / 0490 | Time 0.1s | Iter Loss 0.2694 | Iter Mean Loss 0.3016\n",
      "CF Training: Epoch 0001 Iter 0216 / 0490 | Time 0.1s | Iter Loss 0.2355 | Iter Mean Loss 0.3013\n",
      "CF Training: Epoch 0001 Iter 0217 / 0490 | Time 0.1s | Iter Loss 0.2941 | Iter Mean Loss 0.3013\n",
      "CF Training: Epoch 0001 Iter 0218 / 0490 | Time 0.1s | Iter Loss 0.2948 | Iter Mean Loss 0.3012\n",
      "CF Training: Epoch 0001 Iter 0219 / 0490 | Time 0.1s | Iter Loss 0.3318 | Iter Mean Loss 0.3014\n",
      "CF Training: Epoch 0001 Iter 0220 / 0490 | Time 0.1s | Iter Loss 0.2544 | Iter Mean Loss 0.3012\n",
      "CF Training: Epoch 0001 Iter 0221 / 0490 | Time 0.1s | Iter Loss 0.2031 | Iter Mean Loss 0.3007\n",
      "CF Training: Epoch 0001 Iter 0222 / 0490 | Time 0.1s | Iter Loss 0.2610 | Iter Mean Loss 0.3005\n",
      "CF Training: Epoch 0001 Iter 0223 / 0490 | Time 0.1s | Iter Loss 0.2965 | Iter Mean Loss 0.3005\n",
      "CF Training: Epoch 0001 Iter 0224 / 0490 | Time 0.1s | Iter Loss 0.3127 | Iter Mean Loss 0.3006\n",
      "CF Training: Epoch 0001 Iter 0225 / 0490 | Time 0.1s | Iter Loss 0.2938 | Iter Mean Loss 0.3005\n",
      "CF Training: Epoch 0001 Iter 0226 / 0490 | Time 0.1s | Iter Loss 0.2699 | Iter Mean Loss 0.3004\n",
      "CF Training: Epoch 0001 Iter 0227 / 0490 | Time 0.1s | Iter Loss 0.2476 | Iter Mean Loss 0.3002\n",
      "CF Training: Epoch 0001 Iter 0228 / 0490 | Time 0.1s | Iter Loss 0.3015 | Iter Mean Loss 0.3002\n",
      "CF Training: Epoch 0001 Iter 0229 / 0490 | Time 0.1s | Iter Loss 0.2878 | Iter Mean Loss 0.3001\n",
      "CF Training: Epoch 0001 Iter 0230 / 0490 | Time 0.1s | Iter Loss 0.2523 | Iter Mean Loss 0.2999\n",
      "CF Training: Epoch 0001 Iter 0231 / 0490 | Time 0.1s | Iter Loss 0.2498 | Iter Mean Loss 0.2997\n",
      "CF Training: Epoch 0001 Iter 0232 / 0490 | Time 0.1s | Iter Loss 0.2258 | Iter Mean Loss 0.2994\n",
      "CF Training: Epoch 0001 Iter 0233 / 0490 | Time 0.1s | Iter Loss 0.2690 | Iter Mean Loss 0.2993\n",
      "CF Training: Epoch 0001 Iter 0234 / 0490 | Time 0.1s | Iter Loss 0.2570 | Iter Mean Loss 0.2991\n",
      "CF Training: Epoch 0001 Iter 0235 / 0490 | Time 0.1s | Iter Loss 0.2438 | Iter Mean Loss 0.2988\n",
      "CF Training: Epoch 0001 Iter 0236 / 0490 | Time 0.1s | Iter Loss 0.2773 | Iter Mean Loss 0.2988\n",
      "CF Training: Epoch 0001 Iter 0237 / 0490 | Time 0.1s | Iter Loss 0.2559 | Iter Mean Loss 0.2986\n",
      "CF Training: Epoch 0001 Iter 0238 / 0490 | Time 0.1s | Iter Loss 0.2299 | Iter Mean Loss 0.2983\n",
      "CF Training: Epoch 0001 Iter 0239 / 0490 | Time 0.1s | Iter Loss 0.2307 | Iter Mean Loss 0.2980\n",
      "CF Training: Epoch 0001 Iter 0240 / 0490 | Time 0.1s | Iter Loss 0.2599 | Iter Mean Loss 0.2978\n",
      "CF Training: Epoch 0001 Iter 0241 / 0490 | Time 0.1s | Iter Loss 0.2649 | Iter Mean Loss 0.2977\n",
      "CF Training: Epoch 0001 Iter 0242 / 0490 | Time 0.1s | Iter Loss 0.2909 | Iter Mean Loss 0.2977\n",
      "CF Training: Epoch 0001 Iter 0243 / 0490 | Time 0.1s | Iter Loss 0.2569 | Iter Mean Loss 0.2975\n",
      "CF Training: Epoch 0001 Iter 0244 / 0490 | Time 0.1s | Iter Loss 0.2833 | Iter Mean Loss 0.2974\n",
      "CF Training: Epoch 0001 Iter 0245 / 0490 | Time 0.1s | Iter Loss 0.2614 | Iter Mean Loss 0.2973\n",
      "CF Training: Epoch 0001 Iter 0246 / 0490 | Time 0.1s | Iter Loss 0.2673 | Iter Mean Loss 0.2972\n",
      "CF Training: Epoch 0001 Iter 0247 / 0490 | Time 0.1s | Iter Loss 0.3281 | Iter Mean Loss 0.2973\n",
      "CF Training: Epoch 0001 Iter 0248 / 0490 | Time 0.1s | Iter Loss 0.2658 | Iter Mean Loss 0.2972\n",
      "CF Training: Epoch 0001 Iter 0249 / 0490 | Time 0.1s | Iter Loss 0.2649 | Iter Mean Loss 0.2970\n",
      "CF Training: Epoch 0001 Iter 0250 / 0490 | Time 0.1s | Iter Loss 0.2570 | Iter Mean Loss 0.2969\n",
      "CF Training: Epoch 0001 Iter 0251 / 0490 | Time 0.1s | Iter Loss 0.2194 | Iter Mean Loss 0.2966\n",
      "CF Training: Epoch 0001 Iter 0252 / 0490 | Time 0.1s | Iter Loss 0.2543 | Iter Mean Loss 0.2964\n",
      "CF Training: Epoch 0001 Iter 0253 / 0490 | Time 0.1s | Iter Loss 0.2225 | Iter Mean Loss 0.2961\n",
      "CF Training: Epoch 0001 Iter 0254 / 0490 | Time 0.1s | Iter Loss 0.2725 | Iter Mean Loss 0.2960\n",
      "CF Training: Epoch 0001 Iter 0255 / 0490 | Time 0.1s | Iter Loss 0.2455 | Iter Mean Loss 0.2958\n",
      "CF Training: Epoch 0001 Iter 0256 / 0490 | Time 0.1s | Iter Loss 0.2635 | Iter Mean Loss 0.2957\n",
      "CF Training: Epoch 0001 Iter 0257 / 0490 | Time 0.1s | Iter Loss 0.2881 | Iter Mean Loss 0.2957\n",
      "CF Training: Epoch 0001 Iter 0258 / 0490 | Time 0.1s | Iter Loss 0.2695 | Iter Mean Loss 0.2956\n",
      "CF Training: Epoch 0001 Iter 0259 / 0490 | Time 0.1s | Iter Loss 0.2506 | Iter Mean Loss 0.2954\n",
      "CF Training: Epoch 0001 Iter 0260 / 0490 | Time 0.1s | Iter Loss 0.2648 | Iter Mean Loss 0.2953\n",
      "CF Training: Epoch 0001 Iter 0261 / 0490 | Time 0.1s | Iter Loss 0.2358 | Iter Mean Loss 0.2951\n",
      "CF Training: Epoch 0001 Iter 0262 / 0490 | Time 0.1s | Iter Loss 0.2777 | Iter Mean Loss 0.2950\n",
      "CF Training: Epoch 0001 Iter 0263 / 0490 | Time 0.1s | Iter Loss 0.2562 | Iter Mean Loss 0.2948\n",
      "CF Training: Epoch 0001 Iter 0264 / 0490 | Time 0.1s | Iter Loss 0.3070 | Iter Mean Loss 0.2949\n",
      "CF Training: Epoch 0001 Iter 0265 / 0490 | Time 0.1s | Iter Loss 0.2586 | Iter Mean Loss 0.2947\n",
      "CF Training: Epoch 0001 Iter 0266 / 0490 | Time 0.1s | Iter Loss 0.2782 | Iter Mean Loss 0.2947\n",
      "CF Training: Epoch 0001 Iter 0267 / 0490 | Time 0.1s | Iter Loss 0.2312 | Iter Mean Loss 0.2944\n",
      "CF Training: Epoch 0001 Iter 0268 / 0490 | Time 0.1s | Iter Loss 0.2605 | Iter Mean Loss 0.2943\n",
      "CF Training: Epoch 0001 Iter 0269 / 0490 | Time 0.1s | Iter Loss 0.2423 | Iter Mean Loss 0.2941\n",
      "CF Training: Epoch 0001 Iter 0270 / 0490 | Time 0.1s | Iter Loss 0.3145 | Iter Mean Loss 0.2942\n",
      "CF Training: Epoch 0001 Iter 0271 / 0490 | Time 0.1s | Iter Loss 0.2779 | Iter Mean Loss 0.2941\n",
      "CF Training: Epoch 0001 Iter 0272 / 0490 | Time 0.1s | Iter Loss 0.2412 | Iter Mean Loss 0.2939\n",
      "CF Training: Epoch 0001 Iter 0273 / 0490 | Time 0.1s | Iter Loss 0.2628 | Iter Mean Loss 0.2938\n",
      "CF Training: Epoch 0001 Iter 0274 / 0490 | Time 0.1s | Iter Loss 0.2348 | Iter Mean Loss 0.2936\n",
      "CF Training: Epoch 0001 Iter 0275 / 0490 | Time 0.1s | Iter Loss 0.2777 | Iter Mean Loss 0.2936\n",
      "CF Training: Epoch 0001 Iter 0276 / 0490 | Time 0.1s | Iter Loss 0.2739 | Iter Mean Loss 0.2935\n",
      "CF Training: Epoch 0001 Iter 0277 / 0490 | Time 0.1s | Iter Loss 0.2373 | Iter Mean Loss 0.2933\n",
      "CF Training: Epoch 0001 Iter 0278 / 0490 | Time 0.1s | Iter Loss 0.3044 | Iter Mean Loss 0.2933\n",
      "CF Training: Epoch 0001 Iter 0279 / 0490 | Time 0.1s | Iter Loss 0.2590 | Iter Mean Loss 0.2932\n",
      "CF Training: Epoch 0001 Iter 0280 / 0490 | Time 0.1s | Iter Loss 0.2369 | Iter Mean Loss 0.2930\n",
      "CF Training: Epoch 0001 Iter 0281 / 0490 | Time 0.1s | Iter Loss 0.2757 | Iter Mean Loss 0.2929\n",
      "CF Training: Epoch 0001 Iter 0282 / 0490 | Time 0.1s | Iter Loss 0.2428 | Iter Mean Loss 0.2928\n",
      "CF Training: Epoch 0001 Iter 0283 / 0490 | Time 0.1s | Iter Loss 0.2748 | Iter Mean Loss 0.2927\n",
      "CF Training: Epoch 0001 Iter 0284 / 0490 | Time 0.1s | Iter Loss 0.2639 | Iter Mean Loss 0.2926\n",
      "CF Training: Epoch 0001 Iter 0285 / 0490 | Time 0.1s | Iter Loss 0.2591 | Iter Mean Loss 0.2925\n",
      "CF Training: Epoch 0001 Iter 0286 / 0490 | Time 0.1s | Iter Loss 0.2459 | Iter Mean Loss 0.2923\n",
      "CF Training: Epoch 0001 Iter 0287 / 0490 | Time 0.1s | Iter Loss 0.2655 | Iter Mean Loss 0.2922\n",
      "CF Training: Epoch 0001 Iter 0288 / 0490 | Time 0.1s | Iter Loss 0.2739 | Iter Mean Loss 0.2922\n",
      "CF Training: Epoch 0001 Iter 0289 / 0490 | Time 0.1s | Iter Loss 0.2529 | Iter Mean Loss 0.2920\n",
      "CF Training: Epoch 0001 Iter 0290 / 0490 | Time 0.1s | Iter Loss 0.2052 | Iter Mean Loss 0.2917\n",
      "CF Training: Epoch 0001 Iter 0291 / 0490 | Time 0.1s | Iter Loss 0.2450 | Iter Mean Loss 0.2916\n",
      "CF Training: Epoch 0001 Iter 0292 / 0490 | Time 0.1s | Iter Loss 0.2588 | Iter Mean Loss 0.2915\n",
      "CF Training: Epoch 0001 Iter 0293 / 0490 | Time 0.1s | Iter Loss 0.2433 | Iter Mean Loss 0.2913\n",
      "CF Training: Epoch 0001 Iter 0294 / 0490 | Time 0.1s | Iter Loss 0.2655 | Iter Mean Loss 0.2912\n",
      "CF Training: Epoch 0001 Iter 0295 / 0490 | Time 0.1s | Iter Loss 0.2450 | Iter Mean Loss 0.2910\n",
      "CF Training: Epoch 0001 Iter 0296 / 0490 | Time 0.1s | Iter Loss 0.2883 | Iter Mean Loss 0.2910\n",
      "CF Training: Epoch 0001 Iter 0297 / 0490 | Time 0.1s | Iter Loss 0.2404 | Iter Mean Loss 0.2909\n",
      "CF Training: Epoch 0001 Iter 0298 / 0490 | Time 0.1s | Iter Loss 0.2545 | Iter Mean Loss 0.2907\n",
      "CF Training: Epoch 0001 Iter 0299 / 0490 | Time 0.1s | Iter Loss 0.2844 | Iter Mean Loss 0.2907\n",
      "CF Training: Epoch 0001 Iter 0300 / 0490 | Time 0.1s | Iter Loss 0.3115 | Iter Mean Loss 0.2908\n",
      "CF Training: Epoch 0001 Iter 0301 / 0490 | Time 0.1s | Iter Loss 0.2503 | Iter Mean Loss 0.2907\n",
      "CF Training: Epoch 0001 Iter 0302 / 0490 | Time 0.1s | Iter Loss 0.2280 | Iter Mean Loss 0.2904\n",
      "CF Training: Epoch 0001 Iter 0303 / 0490 | Time 0.1s | Iter Loss 0.2447 | Iter Mean Loss 0.2903\n",
      "CF Training: Epoch 0001 Iter 0304 / 0490 | Time 0.1s | Iter Loss 0.2350 | Iter Mean Loss 0.2901\n",
      "CF Training: Epoch 0001 Iter 0305 / 0490 | Time 0.1s | Iter Loss 0.2879 | Iter Mean Loss 0.2901\n",
      "CF Training: Epoch 0001 Iter 0306 / 0490 | Time 0.1s | Iter Loss 0.3129 | Iter Mean Loss 0.2902\n",
      "CF Training: Epoch 0001 Iter 0307 / 0490 | Time 0.1s | Iter Loss 0.2457 | Iter Mean Loss 0.2900\n",
      "CF Training: Epoch 0001 Iter 0308 / 0490 | Time 0.1s | Iter Loss 0.2404 | Iter Mean Loss 0.2899\n",
      "CF Training: Epoch 0001 Iter 0309 / 0490 | Time 0.1s | Iter Loss 0.2311 | Iter Mean Loss 0.2897\n",
      "CF Training: Epoch 0001 Iter 0310 / 0490 | Time 0.1s | Iter Loss 0.2872 | Iter Mean Loss 0.2897\n",
      "CF Training: Epoch 0001 Iter 0311 / 0490 | Time 0.1s | Iter Loss 0.2443 | Iter Mean Loss 0.2895\n",
      "CF Training: Epoch 0001 Iter 0312 / 0490 | Time 0.1s | Iter Loss 0.2661 | Iter Mean Loss 0.2895\n",
      "CF Training: Epoch 0001 Iter 0313 / 0490 | Time 0.1s | Iter Loss 0.2617 | Iter Mean Loss 0.2894\n",
      "CF Training: Epoch 0001 Iter 0314 / 0490 | Time 0.1s | Iter Loss 0.2099 | Iter Mean Loss 0.2891\n",
      "CF Training: Epoch 0001 Iter 0315 / 0490 | Time 0.1s | Iter Loss 0.2408 | Iter Mean Loss 0.2890\n",
      "CF Training: Epoch 0001 Iter 0316 / 0490 | Time 0.1s | Iter Loss 0.1981 | Iter Mean Loss 0.2887\n",
      "CF Training: Epoch 0001 Iter 0317 / 0490 | Time 0.1s | Iter Loss 0.2616 | Iter Mean Loss 0.2886\n",
      "CF Training: Epoch 0001 Iter 0318 / 0490 | Time 0.1s | Iter Loss 0.2377 | Iter Mean Loss 0.2884\n",
      "CF Training: Epoch 0001 Iter 0319 / 0490 | Time 0.1s | Iter Loss 0.2485 | Iter Mean Loss 0.2883\n",
      "CF Training: Epoch 0001 Iter 0320 / 0490 | Time 0.1s | Iter Loss 0.2188 | Iter Mean Loss 0.2881\n",
      "CF Training: Epoch 0001 Iter 0321 / 0490 | Time 0.1s | Iter Loss 0.2366 | Iter Mean Loss 0.2879\n",
      "CF Training: Epoch 0001 Iter 0322 / 0490 | Time 0.1s | Iter Loss 0.2204 | Iter Mean Loss 0.2877\n",
      "CF Training: Epoch 0001 Iter 0323 / 0490 | Time 0.1s | Iter Loss 0.2007 | Iter Mean Loss 0.2874\n",
      "CF Training: Epoch 0001 Iter 0324 / 0490 | Time 0.1s | Iter Loss 0.2392 | Iter Mean Loss 0.2873\n",
      "CF Training: Epoch 0001 Iter 0325 / 0490 | Time 0.1s | Iter Loss 0.2981 | Iter Mean Loss 0.2873\n",
      "CF Training: Epoch 0001 Iter 0326 / 0490 | Time 0.1s | Iter Loss 0.2394 | Iter Mean Loss 0.2872\n",
      "CF Training: Epoch 0001 Iter 0327 / 0490 | Time 0.1s | Iter Loss 0.2540 | Iter Mean Loss 0.2871\n",
      "CF Training: Epoch 0001 Iter 0328 / 0490 | Time 0.1s | Iter Loss 0.2440 | Iter Mean Loss 0.2870\n",
      "CF Training: Epoch 0001 Iter 0329 / 0490 | Time 0.1s | Iter Loss 0.2327 | Iter Mean Loss 0.2868\n",
      "CF Training: Epoch 0001 Iter 0330 / 0490 | Time 0.1s | Iter Loss 0.2829 | Iter Mean Loss 0.2868\n",
      "CF Training: Epoch 0001 Iter 0331 / 0490 | Time 0.1s | Iter Loss 0.2506 | Iter Mean Loss 0.2867\n",
      "CF Training: Epoch 0001 Iter 0332 / 0490 | Time 0.1s | Iter Loss 0.2388 | Iter Mean Loss 0.2865\n",
      "CF Training: Epoch 0001 Iter 0333 / 0490 | Time 0.1s | Iter Loss 0.2316 | Iter Mean Loss 0.2864\n",
      "CF Training: Epoch 0001 Iter 0334 / 0490 | Time 0.1s | Iter Loss 0.2627 | Iter Mean Loss 0.2863\n",
      "CF Training: Epoch 0001 Iter 0335 / 0490 | Time 0.1s | Iter Loss 0.2616 | Iter Mean Loss 0.2862\n",
      "CF Training: Epoch 0001 Iter 0336 / 0490 | Time 0.1s | Iter Loss 0.2569 | Iter Mean Loss 0.2861\n",
      "CF Training: Epoch 0001 Iter 0337 / 0490 | Time 0.1s | Iter Loss 0.2816 | Iter Mean Loss 0.2861\n",
      "CF Training: Epoch 0001 Iter 0338 / 0490 | Time 0.1s | Iter Loss 0.2626 | Iter Mean Loss 0.2860\n",
      "CF Training: Epoch 0001 Iter 0339 / 0490 | Time 0.1s | Iter Loss 0.2484 | Iter Mean Loss 0.2859\n",
      "CF Training: Epoch 0001 Iter 0340 / 0490 | Time 0.1s | Iter Loss 0.2796 | Iter Mean Loss 0.2859\n",
      "CF Training: Epoch 0001 Iter 0341 / 0490 | Time 0.1s | Iter Loss 0.2415 | Iter Mean Loss 0.2858\n",
      "CF Training: Epoch 0001 Iter 0342 / 0490 | Time 0.1s | Iter Loss 0.2652 | Iter Mean Loss 0.2857\n",
      "CF Training: Epoch 0001 Iter 0343 / 0490 | Time 0.1s | Iter Loss 0.2225 | Iter Mean Loss 0.2855\n",
      "CF Training: Epoch 0001 Iter 0344 / 0490 | Time 0.1s | Iter Loss 0.2309 | Iter Mean Loss 0.2854\n",
      "CF Training: Epoch 0001 Iter 0345 / 0490 | Time 0.1s | Iter Loss 0.2254 | Iter Mean Loss 0.2852\n",
      "CF Training: Epoch 0001 Iter 0346 / 0490 | Time 0.1s | Iter Loss 0.2475 | Iter Mean Loss 0.2851\n",
      "CF Training: Epoch 0001 Iter 0347 / 0490 | Time 0.1s | Iter Loss 0.2894 | Iter Mean Loss 0.2851\n",
      "CF Training: Epoch 0001 Iter 0348 / 0490 | Time 0.1s | Iter Loss 0.2482 | Iter Mean Loss 0.2850\n",
      "CF Training: Epoch 0001 Iter 0349 / 0490 | Time 0.1s | Iter Loss 0.2025 | Iter Mean Loss 0.2848\n",
      "CF Training: Epoch 0001 Iter 0350 / 0490 | Time 0.1s | Iter Loss 0.2318 | Iter Mean Loss 0.2846\n",
      "CF Training: Epoch 0001 Iter 0351 / 0490 | Time 0.1s | Iter Loss 0.2485 | Iter Mean Loss 0.2845\n",
      "CF Training: Epoch 0001 Iter 0352 / 0490 | Time 0.1s | Iter Loss 0.2366 | Iter Mean Loss 0.2844\n",
      "CF Training: Epoch 0001 Iter 0353 / 0490 | Time 0.1s | Iter Loss 0.2535 | Iter Mean Loss 0.2843\n",
      "CF Training: Epoch 0001 Iter 0354 / 0490 | Time 0.1s | Iter Loss 0.2257 | Iter Mean Loss 0.2841\n",
      "CF Training: Epoch 0001 Iter 0355 / 0490 | Time 0.1s | Iter Loss 0.2314 | Iter Mean Loss 0.2840\n",
      "CF Training: Epoch 0001 Iter 0356 / 0490 | Time 0.1s | Iter Loss 0.2665 | Iter Mean Loss 0.2839\n",
      "CF Training: Epoch 0001 Iter 0357 / 0490 | Time 0.1s | Iter Loss 0.1933 | Iter Mean Loss 0.2837\n",
      "CF Training: Epoch 0001 Iter 0358 / 0490 | Time 0.1s | Iter Loss 0.2021 | Iter Mean Loss 0.2834\n",
      "CF Training: Epoch 0001 Iter 0359 / 0490 | Time 0.1s | Iter Loss 0.2172 | Iter Mean Loss 0.2833\n",
      "CF Training: Epoch 0001 Iter 0360 / 0490 | Time 0.1s | Iter Loss 0.2433 | Iter Mean Loss 0.2831\n",
      "CF Training: Epoch 0001 Iter 0361 / 0490 | Time 0.1s | Iter Loss 0.2393 | Iter Mean Loss 0.2830\n",
      "CF Training: Epoch 0001 Iter 0362 / 0490 | Time 0.1s | Iter Loss 0.2581 | Iter Mean Loss 0.2830\n",
      "CF Training: Epoch 0001 Iter 0363 / 0490 | Time 0.1s | Iter Loss 0.2422 | Iter Mean Loss 0.2828\n",
      "CF Training: Epoch 0001 Iter 0364 / 0490 | Time 0.1s | Iter Loss 0.2402 | Iter Mean Loss 0.2827\n",
      "CF Training: Epoch 0001 Iter 0365 / 0490 | Time 0.1s | Iter Loss 0.2648 | Iter Mean Loss 0.2827\n",
      "CF Training: Epoch 0001 Iter 0366 / 0490 | Time 0.1s | Iter Loss 0.1765 | Iter Mean Loss 0.2824\n",
      "CF Training: Epoch 0001 Iter 0367 / 0490 | Time 0.1s | Iter Loss 0.1993 | Iter Mean Loss 0.2822\n",
      "CF Training: Epoch 0001 Iter 0368 / 0490 | Time 0.1s | Iter Loss 0.2049 | Iter Mean Loss 0.2820\n",
      "CF Training: Epoch 0001 Iter 0369 / 0490 | Time 0.1s | Iter Loss 0.2689 | Iter Mean Loss 0.2819\n",
      "CF Training: Epoch 0001 Iter 0370 / 0490 | Time 0.1s | Iter Loss 0.2303 | Iter Mean Loss 0.2818\n",
      "CF Training: Epoch 0001 Iter 0371 / 0490 | Time 0.1s | Iter Loss 0.2534 | Iter Mean Loss 0.2817\n",
      "CF Training: Epoch 0001 Iter 0372 / 0490 | Time 0.1s | Iter Loss 0.2217 | Iter Mean Loss 0.2815\n",
      "CF Training: Epoch 0001 Iter 0373 / 0490 | Time 0.1s | Iter Loss 0.2590 | Iter Mean Loss 0.2815\n",
      "CF Training: Epoch 0001 Iter 0374 / 0490 | Time 0.1s | Iter Loss 0.2164 | Iter Mean Loss 0.2813\n",
      "CF Training: Epoch 0001 Iter 0375 / 0490 | Time 0.1s | Iter Loss 0.2362 | Iter Mean Loss 0.2812\n",
      "CF Training: Epoch 0001 Iter 0376 / 0490 | Time 0.1s | Iter Loss 0.2364 | Iter Mean Loss 0.2811\n",
      "CF Training: Epoch 0001 Iter 0377 / 0490 | Time 0.1s | Iter Loss 0.2447 | Iter Mean Loss 0.2810\n",
      "CF Training: Epoch 0001 Iter 0378 / 0490 | Time 0.1s | Iter Loss 0.2369 | Iter Mean Loss 0.2809\n",
      "CF Training: Epoch 0001 Iter 0379 / 0490 | Time 0.1s | Iter Loss 0.2436 | Iter Mean Loss 0.2808\n",
      "CF Training: Epoch 0001 Iter 0380 / 0490 | Time 0.1s | Iter Loss 0.2503 | Iter Mean Loss 0.2807\n",
      "CF Training: Epoch 0001 Iter 0381 / 0490 | Time 0.1s | Iter Loss 0.2657 | Iter Mean Loss 0.2806\n",
      "CF Training: Epoch 0001 Iter 0382 / 0490 | Time 0.1s | Iter Loss 0.2355 | Iter Mean Loss 0.2805\n",
      "CF Training: Epoch 0001 Iter 0383 / 0490 | Time 0.1s | Iter Loss 0.2558 | Iter Mean Loss 0.2805\n",
      "CF Training: Epoch 0001 Iter 0384 / 0490 | Time 0.1s | Iter Loss 0.2703 | Iter Mean Loss 0.2804\n",
      "CF Training: Epoch 0001 Iter 0385 / 0490 | Time 0.1s | Iter Loss 0.2174 | Iter Mean Loss 0.2803\n",
      "CF Training: Epoch 0001 Iter 0386 / 0490 | Time 0.1s | Iter Loss 0.2475 | Iter Mean Loss 0.2802\n",
      "CF Training: Epoch 0001 Iter 0387 / 0490 | Time 0.1s | Iter Loss 0.2409 | Iter Mean Loss 0.2801\n",
      "CF Training: Epoch 0001 Iter 0388 / 0490 | Time 0.1s | Iter Loss 0.2674 | Iter Mean Loss 0.2800\n",
      "CF Training: Epoch 0001 Iter 0389 / 0490 | Time 0.1s | Iter Loss 0.2549 | Iter Mean Loss 0.2800\n",
      "CF Training: Epoch 0001 Iter 0390 / 0490 | Time 0.1s | Iter Loss 0.2114 | Iter Mean Loss 0.2798\n",
      "CF Training: Epoch 0001 Iter 0391 / 0490 | Time 0.1s | Iter Loss 0.2000 | Iter Mean Loss 0.2796\n",
      "CF Training: Epoch 0001 Iter 0392 / 0490 | Time 0.1s | Iter Loss 0.2266 | Iter Mean Loss 0.2795\n",
      "CF Training: Epoch 0001 Iter 0393 / 0490 | Time 0.1s | Iter Loss 0.2153 | Iter Mean Loss 0.2793\n",
      "CF Training: Epoch 0001 Iter 0394 / 0490 | Time 0.1s | Iter Loss 0.2514 | Iter Mean Loss 0.2792\n",
      "CF Training: Epoch 0001 Iter 0395 / 0490 | Time 0.1s | Iter Loss 0.2304 | Iter Mean Loss 0.2791\n",
      "CF Training: Epoch 0001 Iter 0396 / 0490 | Time 0.1s | Iter Loss 0.2222 | Iter Mean Loss 0.2790\n",
      "CF Training: Epoch 0001 Iter 0397 / 0490 | Time 0.1s | Iter Loss 0.2258 | Iter Mean Loss 0.2788\n",
      "CF Training: Epoch 0001 Iter 0398 / 0490 | Time 0.1s | Iter Loss 0.2559 | Iter Mean Loss 0.2788\n",
      "CF Training: Epoch 0001 Iter 0399 / 0490 | Time 0.1s | Iter Loss 0.2560 | Iter Mean Loss 0.2787\n",
      "CF Training: Epoch 0001 Iter 0400 / 0490 | Time 0.1s | Iter Loss 0.1903 | Iter Mean Loss 0.2785\n",
      "CF Training: Epoch 0001 Iter 0401 / 0490 | Time 0.1s | Iter Loss 0.2206 | Iter Mean Loss 0.2783\n",
      "CF Training: Epoch 0001 Iter 0402 / 0490 | Time 0.1s | Iter Loss 0.2545 | Iter Mean Loss 0.2783\n",
      "CF Training: Epoch 0001 Iter 0403 / 0490 | Time 0.1s | Iter Loss 0.2452 | Iter Mean Loss 0.2782\n",
      "CF Training: Epoch 0001 Iter 0404 / 0490 | Time 0.1s | Iter Loss 0.2664 | Iter Mean Loss 0.2782\n",
      "CF Training: Epoch 0001 Iter 0405 / 0490 | Time 0.1s | Iter Loss 0.2273 | Iter Mean Loss 0.2781\n",
      "CF Training: Epoch 0001 Iter 0406 / 0490 | Time 0.1s | Iter Loss 0.2912 | Iter Mean Loss 0.2781\n",
      "CF Training: Epoch 0001 Iter 0407 / 0490 | Time 0.1s | Iter Loss 0.1793 | Iter Mean Loss 0.2778\n",
      "CF Training: Epoch 0001 Iter 0408 / 0490 | Time 0.1s | Iter Loss 0.2294 | Iter Mean Loss 0.2777\n",
      "CF Training: Epoch 0001 Iter 0409 / 0490 | Time 0.1s | Iter Loss 0.2565 | Iter Mean Loss 0.2777\n",
      "CF Training: Epoch 0001 Iter 0410 / 0490 | Time 0.1s | Iter Loss 0.2336 | Iter Mean Loss 0.2776\n",
      "CF Training: Epoch 0001 Iter 0411 / 0490 | Time 0.1s | Iter Loss 0.1894 | Iter Mean Loss 0.2773\n",
      "CF Training: Epoch 0001 Iter 0412 / 0490 | Time 0.1s | Iter Loss 0.2441 | Iter Mean Loss 0.2773\n",
      "CF Training: Epoch 0001 Iter 0413 / 0490 | Time 0.1s | Iter Loss 0.2261 | Iter Mean Loss 0.2771\n",
      "CF Training: Epoch 0001 Iter 0414 / 0490 | Time 0.1s | Iter Loss 0.2209 | Iter Mean Loss 0.2770\n",
      "CF Training: Epoch 0001 Iter 0415 / 0490 | Time 0.1s | Iter Loss 0.2252 | Iter Mean Loss 0.2769\n",
      "CF Training: Epoch 0001 Iter 0416 / 0490 | Time 0.1s | Iter Loss 0.2410 | Iter Mean Loss 0.2768\n",
      "CF Training: Epoch 0001 Iter 0417 / 0490 | Time 0.1s | Iter Loss 0.2357 | Iter Mean Loss 0.2767\n",
      "CF Training: Epoch 0001 Iter 0418 / 0490 | Time 0.1s | Iter Loss 0.2521 | Iter Mean Loss 0.2766\n",
      "CF Training: Epoch 0001 Iter 0419 / 0490 | Time 0.0s | Iter Loss 0.2521 | Iter Mean Loss 0.2766\n",
      "CF Training: Epoch 0001 Iter 0420 / 0490 | Time 0.1s | Iter Loss 0.2097 | Iter Mean Loss 0.2764\n",
      "CF Training: Epoch 0001 Iter 0421 / 0490 | Time 0.1s | Iter Loss 0.2544 | Iter Mean Loss 0.2764\n",
      "CF Training: Epoch 0001 Iter 0422 / 0490 | Time 0.1s | Iter Loss 0.2187 | Iter Mean Loss 0.2762\n",
      "CF Training: Epoch 0001 Iter 0423 / 0490 | Time 0.1s | Iter Loss 0.2079 | Iter Mean Loss 0.2761\n",
      "CF Training: Epoch 0001 Iter 0424 / 0490 | Time 0.1s | Iter Loss 0.2793 | Iter Mean Loss 0.2761\n",
      "CF Training: Epoch 0001 Iter 0425 / 0490 | Time 0.1s | Iter Loss 0.1756 | Iter Mean Loss 0.2758\n",
      "CF Training: Epoch 0001 Iter 0426 / 0490 | Time 0.1s | Iter Loss 0.2551 | Iter Mean Loss 0.2758\n",
      "CF Training: Epoch 0001 Iter 0427 / 0490 | Time 0.1s | Iter Loss 0.2760 | Iter Mean Loss 0.2758\n",
      "CF Training: Epoch 0001 Iter 0428 / 0490 | Time 0.1s | Iter Loss 0.2283 | Iter Mean Loss 0.2757\n",
      "CF Training: Epoch 0001 Iter 0429 / 0490 | Time 0.1s | Iter Loss 0.2291 | Iter Mean Loss 0.2756\n",
      "CF Training: Epoch 0001 Iter 0430 / 0490 | Time 0.1s | Iter Loss 0.2092 | Iter Mean Loss 0.2754\n",
      "CF Training: Epoch 0001 Iter 0431 / 0490 | Time 0.1s | Iter Loss 0.2288 | Iter Mean Loss 0.2753\n",
      "CF Training: Epoch 0001 Iter 0432 / 0490 | Time 0.1s | Iter Loss 0.2064 | Iter Mean Loss 0.2752\n",
      "CF Training: Epoch 0001 Iter 0433 / 0490 | Time 0.1s | Iter Loss 0.2103 | Iter Mean Loss 0.2750\n",
      "CF Training: Epoch 0001 Iter 0434 / 0490 | Time 0.1s | Iter Loss 0.2299 | Iter Mean Loss 0.2749\n",
      "CF Training: Epoch 0001 Iter 0435 / 0490 | Time 0.1s | Iter Loss 0.2201 | Iter Mean Loss 0.2748\n",
      "CF Training: Epoch 0001 Iter 0436 / 0490 | Time 0.1s | Iter Loss 0.2260 | Iter Mean Loss 0.2747\n",
      "CF Training: Epoch 0001 Iter 0437 / 0490 | Time 0.1s | Iter Loss 0.2586 | Iter Mean Loss 0.2746\n",
      "CF Training: Epoch 0001 Iter 0438 / 0490 | Time 0.1s | Iter Loss 0.1854 | Iter Mean Loss 0.2744\n",
      "CF Training: Epoch 0001 Iter 0439 / 0490 | Time 0.1s | Iter Loss 0.2597 | Iter Mean Loss 0.2744\n",
      "CF Training: Epoch 0001 Iter 0440 / 0490 | Time 0.1s | Iter Loss 0.2030 | Iter Mean Loss 0.2742\n",
      "CF Training: Epoch 0001 Iter 0441 / 0490 | Time 0.1s | Iter Loss 0.2324 | Iter Mean Loss 0.2741\n",
      "CF Training: Epoch 0001 Iter 0442 / 0490 | Time 0.1s | Iter Loss 0.2647 | Iter Mean Loss 0.2741\n",
      "CF Training: Epoch 0001 Iter 0443 / 0490 | Time 0.1s | Iter Loss 0.2155 | Iter Mean Loss 0.2740\n",
      "CF Training: Epoch 0001 Iter 0444 / 0490 | Time 0.1s | Iter Loss 0.1985 | Iter Mean Loss 0.2738\n",
      "CF Training: Epoch 0001 Iter 0445 / 0490 | Time 0.1s | Iter Loss 0.2367 | Iter Mean Loss 0.2737\n",
      "CF Training: Epoch 0001 Iter 0446 / 0490 | Time 0.1s | Iter Loss 0.1918 | Iter Mean Loss 0.2735\n",
      "CF Training: Epoch 0001 Iter 0447 / 0490 | Time 0.1s | Iter Loss 0.2205 | Iter Mean Loss 0.2734\n",
      "CF Training: Epoch 0001 Iter 0448 / 0490 | Time 0.1s | Iter Loss 0.2107 | Iter Mean Loss 0.2733\n",
      "CF Training: Epoch 0001 Iter 0449 / 0490 | Time 0.1s | Iter Loss 0.2302 | Iter Mean Loss 0.2732\n",
      "CF Training: Epoch 0001 Iter 0450 / 0490 | Time 0.1s | Iter Loss 0.2136 | Iter Mean Loss 0.2731\n",
      "CF Training: Epoch 0001 Iter 0451 / 0490 | Time 0.1s | Iter Loss 0.1964 | Iter Mean Loss 0.2729\n",
      "CF Training: Epoch 0001 Iter 0452 / 0490 | Time 0.1s | Iter Loss 0.1946 | Iter Mean Loss 0.2727\n",
      "CF Training: Epoch 0001 Iter 0453 / 0490 | Time 0.1s | Iter Loss 0.2138 | Iter Mean Loss 0.2726\n",
      "CF Training: Epoch 0001 Iter 0454 / 0490 | Time 0.1s | Iter Loss 0.2368 | Iter Mean Loss 0.2725\n",
      "CF Training: Epoch 0001 Iter 0455 / 0490 | Time 0.1s | Iter Loss 0.2219 | Iter Mean Loss 0.2724\n",
      "CF Training: Epoch 0001 Iter 0456 / 0490 | Time 0.1s | Iter Loss 0.1981 | Iter Mean Loss 0.2722\n",
      "CF Training: Epoch 0001 Iter 0457 / 0490 | Time 0.1s | Iter Loss 0.2171 | Iter Mean Loss 0.2721\n",
      "CF Training: Epoch 0001 Iter 0458 / 0490 | Time 0.0s | Iter Loss 0.1785 | Iter Mean Loss 0.2719\n",
      "CF Training: Epoch 0001 Iter 0459 / 0490 | Time 0.1s | Iter Loss 0.2134 | Iter Mean Loss 0.2718\n",
      "CF Training: Epoch 0001 Iter 0460 / 0490 | Time 0.1s | Iter Loss 0.2619 | Iter Mean Loss 0.2718\n",
      "CF Training: Epoch 0001 Iter 0461 / 0490 | Time 0.1s | Iter Loss 0.2630 | Iter Mean Loss 0.2717\n",
      "CF Training: Epoch 0001 Iter 0462 / 0490 | Time 0.1s | Iter Loss 0.3093 | Iter Mean Loss 0.2718\n",
      "CF Training: Epoch 0001 Iter 0463 / 0490 | Time 0.1s | Iter Loss 0.2370 | Iter Mean Loss 0.2717\n",
      "CF Training: Epoch 0001 Iter 0464 / 0490 | Time 0.1s | Iter Loss 0.2146 | Iter Mean Loss 0.2716\n",
      "CF Training: Epoch 0001 Iter 0465 / 0490 | Time 0.1s | Iter Loss 0.1719 | Iter Mean Loss 0.2714\n",
      "CF Training: Epoch 0001 Iter 0466 / 0490 | Time 0.1s | Iter Loss 0.2536 | Iter Mean Loss 0.2714\n",
      "CF Training: Epoch 0001 Iter 0467 / 0490 | Time 0.1s | Iter Loss 0.2436 | Iter Mean Loss 0.2713\n",
      "CF Training: Epoch 0001 Iter 0468 / 0490 | Time 0.1s | Iter Loss 0.2481 | Iter Mean Loss 0.2713\n",
      "CF Training: Epoch 0001 Iter 0469 / 0490 | Time 0.1s | Iter Loss 0.2352 | Iter Mean Loss 0.2712\n",
      "CF Training: Epoch 0001 Iter 0470 / 0490 | Time 0.1s | Iter Loss 0.2171 | Iter Mean Loss 0.2711\n",
      "CF Training: Epoch 0001 Iter 0471 / 0490 | Time 0.1s | Iter Loss 0.2132 | Iter Mean Loss 0.2709\n",
      "CF Training: Epoch 0001 Iter 0472 / 0490 | Time 0.1s | Iter Loss 0.2095 | Iter Mean Loss 0.2708\n",
      "CF Training: Epoch 0001 Iter 0473 / 0490 | Time 0.1s | Iter Loss 0.2143 | Iter Mean Loss 0.2707\n",
      "CF Training: Epoch 0001 Iter 0474 / 0490 | Time 0.1s | Iter Loss 0.2288 | Iter Mean Loss 0.2706\n",
      "CF Training: Epoch 0001 Iter 0475 / 0490 | Time 0.1s | Iter Loss 0.2603 | Iter Mean Loss 0.2706\n",
      "CF Training: Epoch 0001 Iter 0476 / 0490 | Time 0.1s | Iter Loss 0.2145 | Iter Mean Loss 0.2705\n",
      "CF Training: Epoch 0001 Iter 0477 / 0490 | Time 0.1s | Iter Loss 0.1832 | Iter Mean Loss 0.2703\n",
      "CF Training: Epoch 0001 Iter 0478 / 0490 | Time 0.1s | Iter Loss 0.2429 | Iter Mean Loss 0.2702\n",
      "CF Training: Epoch 0001 Iter 0479 / 0490 | Time 0.1s | Iter Loss 0.1958 | Iter Mean Loss 0.2701\n",
      "CF Training: Epoch 0001 Iter 0480 / 0490 | Time 0.1s | Iter Loss 0.2502 | Iter Mean Loss 0.2700\n",
      "CF Training: Epoch 0001 Iter 0481 / 0490 | Time 0.1s | Iter Loss 0.2263 | Iter Mean Loss 0.2699\n",
      "CF Training: Epoch 0001 Iter 0482 / 0490 | Time 0.1s | Iter Loss 0.2751 | Iter Mean Loss 0.2699\n",
      "CF Training: Epoch 0001 Iter 0483 / 0490 | Time 0.0s | Iter Loss 0.2232 | Iter Mean Loss 0.2698\n",
      "CF Training: Epoch 0001 Iter 0484 / 0490 | Time 0.1s | Iter Loss 0.2404 | Iter Mean Loss 0.2698\n",
      "CF Training: Epoch 0001 Iter 0485 / 0490 | Time 0.1s | Iter Loss 0.2100 | Iter Mean Loss 0.2697\n",
      "CF Training: Epoch 0001 Iter 0486 / 0490 | Time 0.1s | Iter Loss 0.2753 | Iter Mean Loss 0.2697\n",
      "CF Training: Epoch 0001 Iter 0487 / 0490 | Time 0.1s | Iter Loss 0.2688 | Iter Mean Loss 0.2697\n",
      "CF Training: Epoch 0001 Iter 0488 / 0490 | Time 0.1s | Iter Loss 0.1995 | Iter Mean Loss 0.2695\n",
      "CF Training: Epoch 0001 Iter 0489 / 0490 | Time 0.1s | Iter Loss 0.2398 | Iter Mean Loss 0.2695\n",
      "CF Training: Epoch 0001 Iter 0490 / 0490 | Time 0.1s | Iter Loss 0.2271 | Iter Mean Loss 0.2694\n",
      "CF Training: Epoch 0001 Total Iter 0490 | Total Time 48.8s | Iter Mean Loss 0.2694\n",
      "CF Training: Epoch 0002 Iter 0001 / 0490 | Time 0.1s | Iter Loss 0.2306 | Iter Mean Loss 0.2306\n",
      "CF Training: Epoch 0002 Iter 0002 / 0490 | Time 0.1s | Iter Loss 0.2309 | Iter Mean Loss 0.2308\n",
      "CF Training: Epoch 0002 Iter 0003 / 0490 | Time 0.1s | Iter Loss 0.2332 | Iter Mean Loss 0.2316\n",
      "CF Training: Epoch 0002 Iter 0004 / 0490 | Time 0.1s | Iter Loss 0.2391 | Iter Mean Loss 0.2334\n",
      "CF Training: Epoch 0002 Iter 0005 / 0490 | Time 0.1s | Iter Loss 0.2124 | Iter Mean Loss 0.2292\n",
      "CF Training: Epoch 0002 Iter 0006 / 0490 | Time 0.1s | Iter Loss 0.2620 | Iter Mean Loss 0.2347\n",
      "CF Training: Epoch 0002 Iter 0007 / 0490 | Time 0.1s | Iter Loss 0.2556 | Iter Mean Loss 0.2377\n",
      "CF Training: Epoch 0002 Iter 0008 / 0490 | Time 0.1s | Iter Loss 0.1778 | Iter Mean Loss 0.2302\n",
      "CF Training: Epoch 0002 Iter 0009 / 0490 | Time 0.1s | Iter Loss 0.2058 | Iter Mean Loss 0.2275\n",
      "CF Training: Epoch 0002 Iter 0010 / 0490 | Time 0.1s | Iter Loss 0.1985 | Iter Mean Loss 0.2246\n",
      "CF Training: Epoch 0002 Iter 0011 / 0490 | Time 0.1s | Iter Loss 0.2456 | Iter Mean Loss 0.2265\n",
      "CF Training: Epoch 0002 Iter 0012 / 0490 | Time 0.1s | Iter Loss 0.2358 | Iter Mean Loss 0.2273\n",
      "CF Training: Epoch 0002 Iter 0013 / 0490 | Time 0.1s | Iter Loss 0.2142 | Iter Mean Loss 0.2263\n",
      "CF Training: Epoch 0002 Iter 0014 / 0490 | Time 0.1s | Iter Loss 0.1858 | Iter Mean Loss 0.2234\n",
      "CF Training: Epoch 0002 Iter 0015 / 0490 | Time 0.1s | Iter Loss 0.2046 | Iter Mean Loss 0.2221\n",
      "CF Training: Epoch 0002 Iter 0016 / 0490 | Time 0.1s | Iter Loss 0.2264 | Iter Mean Loss 0.2224\n",
      "CF Training: Epoch 0002 Iter 0017 / 0490 | Time 0.1s | Iter Loss 0.2425 | Iter Mean Loss 0.2236\n",
      "CF Training: Epoch 0002 Iter 0018 / 0490 | Time 0.1s | Iter Loss 0.1958 | Iter Mean Loss 0.2220\n",
      "CF Training: Epoch 0002 Iter 0019 / 0490 | Time 0.1s | Iter Loss 0.2101 | Iter Mean Loss 0.2214\n",
      "CF Training: Epoch 0002 Iter 0020 / 0490 | Time 0.1s | Iter Loss 0.2002 | Iter Mean Loss 0.2203\n",
      "CF Training: Epoch 0002 Iter 0021 / 0490 | Time 0.1s | Iter Loss 0.2545 | Iter Mean Loss 0.2220\n",
      "CF Training: Epoch 0002 Iter 0022 / 0490 | Time 0.1s | Iter Loss 0.2036 | Iter Mean Loss 0.2211\n",
      "CF Training: Epoch 0002 Iter 0023 / 0490 | Time 0.1s | Iter Loss 0.2290 | Iter Mean Loss 0.2215\n",
      "CF Training: Epoch 0002 Iter 0024 / 0490 | Time 0.1s | Iter Loss 0.2498 | Iter Mean Loss 0.2227\n",
      "CF Training: Epoch 0002 Iter 0025 / 0490 | Time 0.1s | Iter Loss 0.1799 | Iter Mean Loss 0.2209\n",
      "CF Training: Epoch 0002 Iter 0026 / 0490 | Time 0.1s | Iter Loss 0.1900 | Iter Mean Loss 0.2198\n",
      "CF Training: Epoch 0002 Iter 0027 / 0490 | Time 0.1s | Iter Loss 0.2306 | Iter Mean Loss 0.2202\n",
      "CF Training: Epoch 0002 Iter 0028 / 0490 | Time 0.1s | Iter Loss 0.2143 | Iter Mean Loss 0.2199\n",
      "CF Training: Epoch 0002 Iter 0029 / 0490 | Time 0.1s | Iter Loss 0.2625 | Iter Mean Loss 0.2214\n",
      "CF Training: Epoch 0002 Iter 0030 / 0490 | Time 0.1s | Iter Loss 0.2431 | Iter Mean Loss 0.2221\n",
      "CF Training: Epoch 0002 Iter 0031 / 0490 | Time 0.1s | Iter Loss 0.2209 | Iter Mean Loss 0.2221\n",
      "CF Training: Epoch 0002 Iter 0032 / 0490 | Time 0.1s | Iter Loss 0.1767 | Iter Mean Loss 0.2207\n",
      "CF Training: Epoch 0002 Iter 0033 / 0490 | Time 0.1s | Iter Loss 0.2611 | Iter Mean Loss 0.2219\n",
      "CF Training: Epoch 0002 Iter 0034 / 0490 | Time 0.1s | Iter Loss 0.2180 | Iter Mean Loss 0.2218\n",
      "CF Training: Epoch 0002 Iter 0035 / 0490 | Time 0.1s | Iter Loss 0.2107 | Iter Mean Loss 0.2215\n",
      "CF Training: Epoch 0002 Iter 0036 / 0490 | Time 0.1s | Iter Loss 0.2015 | Iter Mean Loss 0.2209\n",
      "CF Training: Epoch 0002 Iter 0037 / 0490 | Time 0.1s | Iter Loss 0.1941 | Iter Mean Loss 0.2202\n",
      "CF Training: Epoch 0002 Iter 0038 / 0490 | Time 0.1s | Iter Loss 0.2089 | Iter Mean Loss 0.2199\n",
      "CF Training: Epoch 0002 Iter 0039 / 0490 | Time 0.1s | Iter Loss 0.2616 | Iter Mean Loss 0.2210\n",
      "CF Training: Epoch 0002 Iter 0040 / 0490 | Time 0.1s | Iter Loss 0.1740 | Iter Mean Loss 0.2198\n",
      "CF Training: Epoch 0002 Iter 0041 / 0490 | Time 0.1s | Iter Loss 0.2341 | Iter Mean Loss 0.2201\n",
      "CF Training: Epoch 0002 Iter 0042 / 0490 | Time 0.1s | Iter Loss 0.2316 | Iter Mean Loss 0.2204\n",
      "CF Training: Epoch 0002 Iter 0043 / 0490 | Time 0.1s | Iter Loss 0.2486 | Iter Mean Loss 0.2211\n",
      "CF Training: Epoch 0002 Iter 0044 / 0490 | Time 0.1s | Iter Loss 0.2261 | Iter Mean Loss 0.2212\n",
      "CF Training: Epoch 0002 Iter 0045 / 0490 | Time 0.1s | Iter Loss 0.2314 | Iter Mean Loss 0.2214\n",
      "CF Training: Epoch 0002 Iter 0046 / 0490 | Time 0.1s | Iter Loss 0.2334 | Iter Mean Loss 0.2217\n",
      "CF Training: Epoch 0002 Iter 0047 / 0490 | Time 0.1s | Iter Loss 0.1815 | Iter Mean Loss 0.2208\n",
      "CF Training: Epoch 0002 Iter 0048 / 0490 | Time 0.1s | Iter Loss 0.2116 | Iter Mean Loss 0.2206\n",
      "CF Training: Epoch 0002 Iter 0049 / 0490 | Time 0.1s | Iter Loss 0.1915 | Iter Mean Loss 0.2200\n",
      "CF Training: Epoch 0002 Iter 0050 / 0490 | Time 0.1s | Iter Loss 0.2340 | Iter Mean Loss 0.2203\n",
      "CF Training: Epoch 0002 Iter 0051 / 0490 | Time 0.1s | Iter Loss 0.2021 | Iter Mean Loss 0.2200\n",
      "CF Training: Epoch 0002 Iter 0052 / 0490 | Time 0.1s | Iter Loss 0.2666 | Iter Mean Loss 0.2208\n",
      "CF Training: Epoch 0002 Iter 0053 / 0490 | Time 0.1s | Iter Loss 0.2216 | Iter Mean Loss 0.2209\n",
      "CF Training: Epoch 0002 Iter 0054 / 0490 | Time 0.1s | Iter Loss 0.1871 | Iter Mean Loss 0.2202\n",
      "CF Training: Epoch 0002 Iter 0055 / 0490 | Time 0.1s | Iter Loss 0.1874 | Iter Mean Loss 0.2196\n",
      "CF Training: Epoch 0002 Iter 0056 / 0490 | Time 0.1s | Iter Loss 0.1887 | Iter Mean Loss 0.2191\n",
      "CF Training: Epoch 0002 Iter 0057 / 0490 | Time 0.1s | Iter Loss 0.2030 | Iter Mean Loss 0.2188\n",
      "CF Training: Epoch 0002 Iter 0058 / 0490 | Time 0.1s | Iter Loss 0.2114 | Iter Mean Loss 0.2187\n",
      "CF Training: Epoch 0002 Iter 0059 / 0490 | Time 0.1s | Iter Loss 0.2343 | Iter Mean Loss 0.2189\n",
      "CF Training: Epoch 0002 Iter 0060 / 0490 | Time 0.1s | Iter Loss 0.1835 | Iter Mean Loss 0.2184\n",
      "CF Training: Epoch 0002 Iter 0061 / 0490 | Time 0.1s | Iter Loss 0.2536 | Iter Mean Loss 0.2189\n",
      "CF Training: Epoch 0002 Iter 0062 / 0490 | Time 0.1s | Iter Loss 0.2251 | Iter Mean Loss 0.2190\n",
      "CF Training: Epoch 0002 Iter 0063 / 0490 | Time 0.1s | Iter Loss 0.1896 | Iter Mean Loss 0.2186\n",
      "CF Training: Epoch 0002 Iter 0064 / 0490 | Time 0.1s | Iter Loss 0.2673 | Iter Mean Loss 0.2193\n",
      "CF Training: Epoch 0002 Iter 0065 / 0490 | Time 0.1s | Iter Loss 0.1843 | Iter Mean Loss 0.2188\n",
      "CF Training: Epoch 0002 Iter 0066 / 0490 | Time 0.1s | Iter Loss 0.1558 | Iter Mean Loss 0.2178\n",
      "CF Training: Epoch 0002 Iter 0067 / 0490 | Time 0.1s | Iter Loss 0.1932 | Iter Mean Loss 0.2175\n",
      "CF Training: Epoch 0002 Iter 0068 / 0490 | Time 0.1s | Iter Loss 0.2094 | Iter Mean Loss 0.2173\n",
      "CF Training: Epoch 0002 Iter 0069 / 0490 | Time 0.1s | Iter Loss 0.2064 | Iter Mean Loss 0.2172\n",
      "CF Training: Epoch 0002 Iter 0070 / 0490 | Time 0.1s | Iter Loss 0.1779 | Iter Mean Loss 0.2166\n",
      "CF Training: Epoch 0002 Iter 0071 / 0490 | Time 0.1s | Iter Loss 0.2034 | Iter Mean Loss 0.2164\n",
      "CF Training: Epoch 0002 Iter 0072 / 0490 | Time 0.1s | Iter Loss 0.1949 | Iter Mean Loss 0.2161\n",
      "CF Training: Epoch 0002 Iter 0073 / 0490 | Time 0.1s | Iter Loss 0.2315 | Iter Mean Loss 0.2163\n",
      "CF Training: Epoch 0002 Iter 0074 / 0490 | Time 0.1s | Iter Loss 0.2126 | Iter Mean Loss 0.2163\n",
      "CF Training: Epoch 0002 Iter 0075 / 0490 | Time 0.1s | Iter Loss 0.2217 | Iter Mean Loss 0.2164\n",
      "CF Training: Epoch 0002 Iter 0076 / 0490 | Time 0.1s | Iter Loss 0.2166 | Iter Mean Loss 0.2164\n",
      "CF Training: Epoch 0002 Iter 0077 / 0490 | Time 0.1s | Iter Loss 0.2274 | Iter Mean Loss 0.2165\n",
      "CF Training: Epoch 0002 Iter 0078 / 0490 | Time 0.1s | Iter Loss 0.2351 | Iter Mean Loss 0.2168\n",
      "CF Training: Epoch 0002 Iter 0079 / 0490 | Time 0.1s | Iter Loss 0.1701 | Iter Mean Loss 0.2162\n",
      "CF Training: Epoch 0002 Iter 0080 / 0490 | Time 0.1s | Iter Loss 0.1845 | Iter Mean Loss 0.2158\n",
      "CF Training: Epoch 0002 Iter 0081 / 0490 | Time 0.1s | Iter Loss 0.1767 | Iter Mean Loss 0.2153\n",
      "CF Training: Epoch 0002 Iter 0082 / 0490 | Time 0.1s | Iter Loss 0.2338 | Iter Mean Loss 0.2155\n",
      "CF Training: Epoch 0002 Iter 0083 / 0490 | Time 0.1s | Iter Loss 0.2251 | Iter Mean Loss 0.2156\n",
      "CF Training: Epoch 0002 Iter 0084 / 0490 | Time 0.1s | Iter Loss 0.2191 | Iter Mean Loss 0.2157\n",
      "CF Training: Epoch 0002 Iter 0085 / 0490 | Time 0.1s | Iter Loss 0.1765 | Iter Mean Loss 0.2152\n",
      "CF Training: Epoch 0002 Iter 0086 / 0490 | Time 0.1s | Iter Loss 0.2389 | Iter Mean Loss 0.2155\n",
      "CF Training: Epoch 0002 Iter 0087 / 0490 | Time 0.1s | Iter Loss 0.1864 | Iter Mean Loss 0.2151\n",
      "CF Training: Epoch 0002 Iter 0088 / 0490 | Time 0.1s | Iter Loss 0.1928 | Iter Mean Loss 0.2149\n",
      "CF Training: Epoch 0002 Iter 0089 / 0490 | Time 0.1s | Iter Loss 0.2103 | Iter Mean Loss 0.2148\n",
      "CF Training: Epoch 0002 Iter 0090 / 0490 | Time 0.1s | Iter Loss 0.1898 | Iter Mean Loss 0.2146\n",
      "CF Training: Epoch 0002 Iter 0091 / 0490 | Time 0.1s | Iter Loss 0.1573 | Iter Mean Loss 0.2139\n",
      "CF Training: Epoch 0002 Iter 0092 / 0490 | Time 0.1s | Iter Loss 0.2043 | Iter Mean Loss 0.2138\n",
      "CF Training: Epoch 0002 Iter 0093 / 0490 | Time 0.1s | Iter Loss 0.2385 | Iter Mean Loss 0.2141\n",
      "CF Training: Epoch 0002 Iter 0094 / 0490 | Time 0.1s | Iter Loss 0.2332 | Iter Mean Loss 0.2143\n",
      "CF Training: Epoch 0002 Iter 0095 / 0490 | Time 0.1s | Iter Loss 0.2040 | Iter Mean Loss 0.2142\n",
      "CF Training: Epoch 0002 Iter 0096 / 0490 | Time 0.1s | Iter Loss 0.1804 | Iter Mean Loss 0.2138\n",
      "CF Training: Epoch 0002 Iter 0097 / 0490 | Time 0.1s | Iter Loss 0.2020 | Iter Mean Loss 0.2137\n",
      "CF Training: Epoch 0002 Iter 0098 / 0490 | Time 0.1s | Iter Loss 0.2162 | Iter Mean Loss 0.2137\n",
      "CF Training: Epoch 0002 Iter 0099 / 0490 | Time 0.1s | Iter Loss 0.2086 | Iter Mean Loss 0.2137\n",
      "CF Training: Epoch 0002 Iter 0100 / 0490 | Time 0.1s | Iter Loss 0.1875 | Iter Mean Loss 0.2134\n",
      "CF Training: Epoch 0002 Iter 0101 / 0490 | Time 0.1s | Iter Loss 0.1929 | Iter Mean Loss 0.2132\n",
      "CF Training: Epoch 0002 Iter 0102 / 0490 | Time 0.1s | Iter Loss 0.2469 | Iter Mean Loss 0.2136\n",
      "CF Training: Epoch 0002 Iter 0103 / 0490 | Time 0.1s | Iter Loss 0.2583 | Iter Mean Loss 0.2140\n",
      "CF Training: Epoch 0002 Iter 0104 / 0490 | Time 0.1s | Iter Loss 0.1786 | Iter Mean Loss 0.2137\n",
      "CF Training: Epoch 0002 Iter 0105 / 0490 | Time 0.1s | Iter Loss 0.1907 | Iter Mean Loss 0.2134\n",
      "CF Training: Epoch 0002 Iter 0106 / 0490 | Time 0.1s | Iter Loss 0.2382 | Iter Mean Loss 0.2137\n",
      "CF Training: Epoch 0002 Iter 0107 / 0490 | Time 0.1s | Iter Loss 0.2157 | Iter Mean Loss 0.2137\n",
      "CF Training: Epoch 0002 Iter 0108 / 0490 | Time 0.1s | Iter Loss 0.1695 | Iter Mean Loss 0.2133\n",
      "CF Training: Epoch 0002 Iter 0109 / 0490 | Time 0.1s | Iter Loss 0.2033 | Iter Mean Loss 0.2132\n",
      "CF Training: Epoch 0002 Iter 0110 / 0490 | Time 0.1s | Iter Loss 0.2417 | Iter Mean Loss 0.2134\n",
      "CF Training: Epoch 0002 Iter 0111 / 0490 | Time 0.1s | Iter Loss 0.2036 | Iter Mean Loss 0.2134\n",
      "CF Training: Epoch 0002 Iter 0112 / 0490 | Time 0.1s | Iter Loss 0.2079 | Iter Mean Loss 0.2133\n",
      "CF Training: Epoch 0002 Iter 0113 / 0490 | Time 0.1s | Iter Loss 0.2351 | Iter Mean Loss 0.2135\n",
      "CF Training: Epoch 0002 Iter 0114 / 0490 | Time 0.1s | Iter Loss 0.2011 | Iter Mean Loss 0.2134\n",
      "CF Training: Epoch 0002 Iter 0115 / 0490 | Time 0.1s | Iter Loss 0.2373 | Iter Mean Loss 0.2136\n",
      "CF Training: Epoch 0002 Iter 0116 / 0490 | Time 0.1s | Iter Loss 0.2274 | Iter Mean Loss 0.2137\n",
      "CF Training: Epoch 0002 Iter 0117 / 0490 | Time 0.1s | Iter Loss 0.1880 | Iter Mean Loss 0.2135\n",
      "CF Training: Epoch 0002 Iter 0118 / 0490 | Time 0.1s | Iter Loss 0.1763 | Iter Mean Loss 0.2132\n",
      "CF Training: Epoch 0002 Iter 0119 / 0490 | Time 0.1s | Iter Loss 0.2045 | Iter Mean Loss 0.2131\n",
      "CF Training: Epoch 0002 Iter 0120 / 0490 | Time 0.1s | Iter Loss 0.1765 | Iter Mean Loss 0.2128\n",
      "CF Training: Epoch 0002 Iter 0121 / 0490 | Time 0.1s | Iter Loss 0.2371 | Iter Mean Loss 0.2130\n",
      "CF Training: Epoch 0002 Iter 0122 / 0490 | Time 0.1s | Iter Loss 0.2115 | Iter Mean Loss 0.2130\n",
      "CF Training: Epoch 0002 Iter 0123 / 0490 | Time 0.1s | Iter Loss 0.1906 | Iter Mean Loss 0.2128\n",
      "CF Training: Epoch 0002 Iter 0124 / 0490 | Time 0.1s | Iter Loss 0.2375 | Iter Mean Loss 0.2130\n",
      "CF Training: Epoch 0002 Iter 0125 / 0490 | Time 0.1s | Iter Loss 0.2180 | Iter Mean Loss 0.2130\n",
      "CF Training: Epoch 0002 Iter 0126 / 0490 | Time 0.1s | Iter Loss 0.2312 | Iter Mean Loss 0.2132\n",
      "CF Training: Epoch 0002 Iter 0127 / 0490 | Time 0.1s | Iter Loss 0.1949 | Iter Mean Loss 0.2130\n",
      "CF Training: Epoch 0002 Iter 0128 / 0490 | Time 0.1s | Iter Loss 0.2837 | Iter Mean Loss 0.2136\n",
      "CF Training: Epoch 0002 Iter 0129 / 0490 | Time 0.1s | Iter Loss 0.1799 | Iter Mean Loss 0.2133\n",
      "CF Training: Epoch 0002 Iter 0130 / 0490 | Time 0.1s | Iter Loss 0.1947 | Iter Mean Loss 0.2132\n",
      "CF Training: Epoch 0002 Iter 0131 / 0490 | Time 0.1s | Iter Loss 0.2223 | Iter Mean Loss 0.2133\n",
      "CF Training: Epoch 0002 Iter 0132 / 0490 | Time 0.1s | Iter Loss 0.2142 | Iter Mean Loss 0.2133\n",
      "CF Training: Epoch 0002 Iter 0133 / 0490 | Time 0.1s | Iter Loss 0.1792 | Iter Mean Loss 0.2130\n",
      "CF Training: Epoch 0002 Iter 0134 / 0490 | Time 0.1s | Iter Loss 0.2193 | Iter Mean Loss 0.2131\n",
      "CF Training: Epoch 0002 Iter 0135 / 0490 | Time 0.1s | Iter Loss 0.2067 | Iter Mean Loss 0.2130\n",
      "CF Training: Epoch 0002 Iter 0136 / 0490 | Time 0.0s | Iter Loss 0.1779 | Iter Mean Loss 0.2128\n",
      "CF Training: Epoch 0002 Iter 0137 / 0490 | Time 0.1s | Iter Loss 0.1855 | Iter Mean Loss 0.2126\n",
      "CF Training: Epoch 0002 Iter 0138 / 0490 | Time 0.1s | Iter Loss 0.1901 | Iter Mean Loss 0.2124\n",
      "CF Training: Epoch 0002 Iter 0139 / 0490 | Time 0.1s | Iter Loss 0.2070 | Iter Mean Loss 0.2124\n",
      "CF Training: Epoch 0002 Iter 0140 / 0490 | Time 0.1s | Iter Loss 0.1808 | Iter Mean Loss 0.2121\n",
      "CF Training: Epoch 0002 Iter 0141 / 0490 | Time 0.1s | Iter Loss 0.2199 | Iter Mean Loss 0.2122\n",
      "CF Training: Epoch 0002 Iter 0142 / 0490 | Time 0.1s | Iter Loss 0.2151 | Iter Mean Loss 0.2122\n",
      "CF Training: Epoch 0002 Iter 0143 / 0490 | Time 0.1s | Iter Loss 0.1970 | Iter Mean Loss 0.2121\n",
      "CF Training: Epoch 0002 Iter 0144 / 0490 | Time 0.1s | Iter Loss 0.2011 | Iter Mean Loss 0.2120\n",
      "CF Training: Epoch 0002 Iter 0145 / 0490 | Time 0.1s | Iter Loss 0.2030 | Iter Mean Loss 0.2120\n",
      "CF Training: Epoch 0002 Iter 0146 / 0490 | Time 0.1s | Iter Loss 0.2108 | Iter Mean Loss 0.2120\n",
      "CF Training: Epoch 0002 Iter 0147 / 0490 | Time 0.1s | Iter Loss 0.2259 | Iter Mean Loss 0.2121\n",
      "CF Training: Epoch 0002 Iter 0148 / 0490 | Time 0.1s | Iter Loss 0.2036 | Iter Mean Loss 0.2120\n",
      "CF Training: Epoch 0002 Iter 0149 / 0490 | Time 0.1s | Iter Loss 0.2072 | Iter Mean Loss 0.2120\n",
      "CF Training: Epoch 0002 Iter 0150 / 0490 | Time 0.1s | Iter Loss 0.1873 | Iter Mean Loss 0.2118\n",
      "CF Training: Epoch 0002 Iter 0151 / 0490 | Time 0.1s | Iter Loss 0.2039 | Iter Mean Loss 0.2117\n",
      "CF Training: Epoch 0002 Iter 0152 / 0490 | Time 0.1s | Iter Loss 0.2253 | Iter Mean Loss 0.2118\n",
      "CF Training: Epoch 0002 Iter 0153 / 0490 | Time 0.1s | Iter Loss 0.1891 | Iter Mean Loss 0.2117\n",
      "CF Training: Epoch 0002 Iter 0154 / 0490 | Time 0.1s | Iter Loss 0.1848 | Iter Mean Loss 0.2115\n",
      "CF Training: Epoch 0002 Iter 0155 / 0490 | Time 0.1s | Iter Loss 0.1946 | Iter Mean Loss 0.2114\n",
      "CF Training: Epoch 0002 Iter 0156 / 0490 | Time 0.1s | Iter Loss 0.2511 | Iter Mean Loss 0.2117\n",
      "CF Training: Epoch 0002 Iter 0157 / 0490 | Time 0.1s | Iter Loss 0.1719 | Iter Mean Loss 0.2114\n",
      "CF Training: Epoch 0002 Iter 0158 / 0490 | Time 0.1s | Iter Loss 0.2080 | Iter Mean Loss 0.2114\n",
      "CF Training: Epoch 0002 Iter 0159 / 0490 | Time 0.1s | Iter Loss 0.2349 | Iter Mean Loss 0.2115\n",
      "CF Training: Epoch 0002 Iter 0160 / 0490 | Time 0.1s | Iter Loss 0.2420 | Iter Mean Loss 0.2117\n",
      "CF Training: Epoch 0002 Iter 0161 / 0490 | Time 0.1s | Iter Loss 0.2509 | Iter Mean Loss 0.2120\n",
      "CF Training: Epoch 0002 Iter 0162 / 0490 | Time 0.1s | Iter Loss 0.2129 | Iter Mean Loss 0.2120\n",
      "CF Training: Epoch 0002 Iter 0163 / 0490 | Time 0.1s | Iter Loss 0.2323 | Iter Mean Loss 0.2121\n",
      "CF Training: Epoch 0002 Iter 0164 / 0490 | Time 0.1s | Iter Loss 0.2079 | Iter Mean Loss 0.2121\n",
      "CF Training: Epoch 0002 Iter 0165 / 0490 | Time 0.1s | Iter Loss 0.2118 | Iter Mean Loss 0.2121\n",
      "CF Training: Epoch 0002 Iter 0166 / 0490 | Time 0.1s | Iter Loss 0.2214 | Iter Mean Loss 0.2121\n",
      "CF Training: Epoch 0002 Iter 0167 / 0490 | Time 0.1s | Iter Loss 0.2238 | Iter Mean Loss 0.2122\n",
      "CF Training: Epoch 0002 Iter 0168 / 0490 | Time 0.1s | Iter Loss 0.2242 | Iter Mean Loss 0.2123\n",
      "CF Training: Epoch 0002 Iter 0169 / 0490 | Time 0.1s | Iter Loss 0.1833 | Iter Mean Loss 0.2121\n",
      "CF Training: Epoch 0002 Iter 0170 / 0490 | Time 0.1s | Iter Loss 0.1993 | Iter Mean Loss 0.2120\n",
      "CF Training: Epoch 0002 Iter 0171 / 0490 | Time 0.1s | Iter Loss 0.1829 | Iter Mean Loss 0.2118\n",
      "CF Training: Epoch 0002 Iter 0172 / 0490 | Time 0.1s | Iter Loss 0.1943 | Iter Mean Loss 0.2117\n",
      "CF Training: Epoch 0002 Iter 0173 / 0490 | Time 0.1s | Iter Loss 0.2147 | Iter Mean Loss 0.2118\n",
      "CF Training: Epoch 0002 Iter 0174 / 0490 | Time 0.1s | Iter Loss 0.2010 | Iter Mean Loss 0.2117\n",
      "CF Training: Epoch 0002 Iter 0175 / 0490 | Time 0.1s | Iter Loss 0.2455 | Iter Mean Loss 0.2119\n",
      "CF Training: Epoch 0002 Iter 0176 / 0490 | Time 0.1s | Iter Loss 0.2397 | Iter Mean Loss 0.2121\n",
      "CF Training: Epoch 0002 Iter 0177 / 0490 | Time 0.1s | Iter Loss 0.2051 | Iter Mean Loss 0.2120\n",
      "CF Training: Epoch 0002 Iter 0178 / 0490 | Time 0.1s | Iter Loss 0.2005 | Iter Mean Loss 0.2119\n",
      "CF Training: Epoch 0002 Iter 0179 / 0490 | Time 0.1s | Iter Loss 0.2254 | Iter Mean Loss 0.2120\n",
      "CF Training: Epoch 0002 Iter 0180 / 0490 | Time 0.1s | Iter Loss 0.2203 | Iter Mean Loss 0.2121\n",
      "CF Training: Epoch 0002 Iter 0181 / 0490 | Time 0.1s | Iter Loss 0.2074 | Iter Mean Loss 0.2120\n",
      "CF Training: Epoch 0002 Iter 0182 / 0490 | Time 0.1s | Iter Loss 0.2462 | Iter Mean Loss 0.2122\n",
      "CF Training: Epoch 0002 Iter 0183 / 0490 | Time 0.1s | Iter Loss 0.2358 | Iter Mean Loss 0.2124\n",
      "CF Training: Epoch 0002 Iter 0184 / 0490 | Time 0.1s | Iter Loss 0.2344 | Iter Mean Loss 0.2125\n",
      "CF Training: Epoch 0002 Iter 0185 / 0490 | Time 0.1s | Iter Loss 0.1809 | Iter Mean Loss 0.2123\n",
      "CF Training: Epoch 0002 Iter 0186 / 0490 | Time 0.1s | Iter Loss 0.1953 | Iter Mean Loss 0.2122\n",
      "CF Training: Epoch 0002 Iter 0187 / 0490 | Time 0.1s | Iter Loss 0.1582 | Iter Mean Loss 0.2119\n",
      "CF Training: Epoch 0002 Iter 0188 / 0490 | Time 0.1s | Iter Loss 0.2080 | Iter Mean Loss 0.2119\n",
      "CF Training: Epoch 0002 Iter 0189 / 0490 | Time 0.1s | Iter Loss 0.2155 | Iter Mean Loss 0.2119\n",
      "CF Training: Epoch 0002 Iter 0190 / 0490 | Time 0.1s | Iter Loss 0.2072 | Iter Mean Loss 0.2119\n",
      "CF Training: Epoch 0002 Iter 0191 / 0490 | Time 0.1s | Iter Loss 0.1677 | Iter Mean Loss 0.2117\n",
      "CF Training: Epoch 0002 Iter 0192 / 0490 | Time 0.1s | Iter Loss 0.2057 | Iter Mean Loss 0.2116\n",
      "CF Training: Epoch 0002 Iter 0193 / 0490 | Time 0.1s | Iter Loss 0.1772 | Iter Mean Loss 0.2115\n",
      "CF Training: Epoch 0002 Iter 0194 / 0490 | Time 0.1s | Iter Loss 0.2813 | Iter Mean Loss 0.2118\n",
      "CF Training: Epoch 0002 Iter 0195 / 0490 | Time 0.1s | Iter Loss 0.1465 | Iter Mean Loss 0.2115\n",
      "CF Training: Epoch 0002 Iter 0196 / 0490 | Time 0.1s | Iter Loss 0.2072 | Iter Mean Loss 0.2115\n",
      "CF Training: Epoch 0002 Iter 0197 / 0490 | Time 0.1s | Iter Loss 0.2023 | Iter Mean Loss 0.2114\n",
      "CF Training: Epoch 0002 Iter 0198 / 0490 | Time 0.1s | Iter Loss 0.2227 | Iter Mean Loss 0.2115\n",
      "CF Training: Epoch 0002 Iter 0199 / 0490 | Time 0.1s | Iter Loss 0.1679 | Iter Mean Loss 0.2113\n",
      "CF Training: Epoch 0002 Iter 0200 / 0490 | Time 0.1s | Iter Loss 0.1852 | Iter Mean Loss 0.2111\n",
      "CF Training: Epoch 0002 Iter 0201 / 0490 | Time 0.1s | Iter Loss 0.2054 | Iter Mean Loss 0.2111\n",
      "CF Training: Epoch 0002 Iter 0202 / 0490 | Time 0.1s | Iter Loss 0.1551 | Iter Mean Loss 0.2108\n",
      "CF Training: Epoch 0002 Iter 0203 / 0490 | Time 0.1s | Iter Loss 0.1628 | Iter Mean Loss 0.2106\n",
      "CF Training: Epoch 0002 Iter 0204 / 0490 | Time 0.1s | Iter Loss 0.1751 | Iter Mean Loss 0.2104\n",
      "CF Training: Epoch 0002 Iter 0205 / 0490 | Time 0.1s | Iter Loss 0.1680 | Iter Mean Loss 0.2102\n",
      "CF Training: Epoch 0002 Iter 0206 / 0490 | Time 0.1s | Iter Loss 0.2070 | Iter Mean Loss 0.2102\n",
      "CF Training: Epoch 0002 Iter 0207 / 0490 | Time 0.1s | Iter Loss 0.2026 | Iter Mean Loss 0.2101\n",
      "CF Training: Epoch 0002 Iter 0208 / 0490 | Time 0.1s | Iter Loss 0.2113 | Iter Mean Loss 0.2102\n",
      "CF Training: Epoch 0002 Iter 0209 / 0490 | Time 0.1s | Iter Loss 0.1923 | Iter Mean Loss 0.2101\n",
      "CF Training: Epoch 0002 Iter 0210 / 0490 | Time 0.1s | Iter Loss 0.2266 | Iter Mean Loss 0.2101\n",
      "CF Training: Epoch 0002 Iter 0211 / 0490 | Time 0.1s | Iter Loss 0.2151 | Iter Mean Loss 0.2102\n",
      "CF Training: Epoch 0002 Iter 0212 / 0490 | Time 0.1s | Iter Loss 0.2427 | Iter Mean Loss 0.2103\n",
      "CF Training: Epoch 0002 Iter 0213 / 0490 | Time 0.1s | Iter Loss 0.1763 | Iter Mean Loss 0.2102\n",
      "CF Training: Epoch 0002 Iter 0214 / 0490 | Time 0.1s | Iter Loss 0.1879 | Iter Mean Loss 0.2101\n",
      "CF Training: Epoch 0002 Iter 0215 / 0490 | Time 0.1s | Iter Loss 0.1965 | Iter Mean Loss 0.2100\n",
      "CF Training: Epoch 0002 Iter 0216 / 0490 | Time 0.1s | Iter Loss 0.2039 | Iter Mean Loss 0.2100\n",
      "CF Training: Epoch 0002 Iter 0217 / 0490 | Time 0.1s | Iter Loss 0.2105 | Iter Mean Loss 0.2100\n",
      "CF Training: Epoch 0002 Iter 0218 / 0490 | Time 0.1s | Iter Loss 0.2192 | Iter Mean Loss 0.2100\n",
      "CF Training: Epoch 0002 Iter 0219 / 0490 | Time 0.1s | Iter Loss 0.2167 | Iter Mean Loss 0.2100\n",
      "CF Training: Epoch 0002 Iter 0220 / 0490 | Time 0.1s | Iter Loss 0.2264 | Iter Mean Loss 0.2101\n",
      "CF Training: Epoch 0002 Iter 0221 / 0490 | Time 0.1s | Iter Loss 0.1865 | Iter Mean Loss 0.2100\n",
      "CF Training: Epoch 0002 Iter 0222 / 0490 | Time 0.1s | Iter Loss 0.2042 | Iter Mean Loss 0.2100\n",
      "CF Training: Epoch 0002 Iter 0223 / 0490 | Time 0.1s | Iter Loss 0.2385 | Iter Mean Loss 0.2101\n",
      "CF Training: Epoch 0002 Iter 0224 / 0490 | Time 0.1s | Iter Loss 0.1676 | Iter Mean Loss 0.2099\n",
      "CF Training: Epoch 0002 Iter 0225 / 0490 | Time 0.1s | Iter Loss 0.1822 | Iter Mean Loss 0.2098\n",
      "CF Training: Epoch 0002 Iter 0226 / 0490 | Time 0.1s | Iter Loss 0.1822 | Iter Mean Loss 0.2097\n",
      "CF Training: Epoch 0002 Iter 0227 / 0490 | Time 0.1s | Iter Loss 0.1648 | Iter Mean Loss 0.2095\n",
      "CF Training: Epoch 0002 Iter 0228 / 0490 | Time 0.1s | Iter Loss 0.1877 | Iter Mean Loss 0.2094\n",
      "CF Training: Epoch 0002 Iter 0229 / 0490 | Time 0.1s | Iter Loss 0.2337 | Iter Mean Loss 0.2095\n",
      "CF Training: Epoch 0002 Iter 0230 / 0490 | Time 0.1s | Iter Loss 0.1763 | Iter Mean Loss 0.2093\n",
      "CF Training: Epoch 0002 Iter 0231 / 0490 | Time 0.1s | Iter Loss 0.2133 | Iter Mean Loss 0.2094\n",
      "CF Training: Epoch 0002 Iter 0232 / 0490 | Time 0.1s | Iter Loss 0.2392 | Iter Mean Loss 0.2095\n",
      "CF Training: Epoch 0002 Iter 0233 / 0490 | Time 0.1s | Iter Loss 0.1627 | Iter Mean Loss 0.2093\n",
      "CF Training: Epoch 0002 Iter 0234 / 0490 | Time 0.1s | Iter Loss 0.2286 | Iter Mean Loss 0.2094\n",
      "CF Training: Epoch 0002 Iter 0235 / 0490 | Time 0.1s | Iter Loss 0.2020 | Iter Mean Loss 0.2093\n",
      "CF Training: Epoch 0002 Iter 0236 / 0490 | Time 0.1s | Iter Loss 0.1925 | Iter Mean Loss 0.2093\n",
      "CF Training: Epoch 0002 Iter 0237 / 0490 | Time 0.1s | Iter Loss 0.2116 | Iter Mean Loss 0.2093\n",
      "CF Training: Epoch 0002 Iter 0238 / 0490 | Time 0.1s | Iter Loss 0.1875 | Iter Mean Loss 0.2092\n",
      "CF Training: Epoch 0002 Iter 0239 / 0490 | Time 0.1s | Iter Loss 0.1789 | Iter Mean Loss 0.2091\n",
      "CF Training: Epoch 0002 Iter 0240 / 0490 | Time 0.1s | Iter Loss 0.1899 | Iter Mean Loss 0.2090\n",
      "CF Training: Epoch 0002 Iter 0241 / 0490 | Time 0.1s | Iter Loss 0.1698 | Iter Mean Loss 0.2088\n",
      "CF Training: Epoch 0002 Iter 0242 / 0490 | Time 0.1s | Iter Loss 0.2097 | Iter Mean Loss 0.2088\n",
      "CF Training: Epoch 0002 Iter 0243 / 0490 | Time 0.0s | Iter Loss 0.1884 | Iter Mean Loss 0.2087\n",
      "CF Training: Epoch 0002 Iter 0244 / 0490 | Time 0.1s | Iter Loss 0.1971 | Iter Mean Loss 0.2087\n",
      "CF Training: Epoch 0002 Iter 0245 / 0490 | Time 0.1s | Iter Loss 0.1955 | Iter Mean Loss 0.2086\n",
      "CF Training: Epoch 0002 Iter 0246 / 0490 | Time 0.1s | Iter Loss 0.1779 | Iter Mean Loss 0.2085\n",
      "CF Training: Epoch 0002 Iter 0247 / 0490 | Time 0.1s | Iter Loss 0.2072 | Iter Mean Loss 0.2085\n",
      "CF Training: Epoch 0002 Iter 0248 / 0490 | Time 0.1s | Iter Loss 0.2343 | Iter Mean Loss 0.2086\n",
      "CF Training: Epoch 0002 Iter 0249 / 0490 | Time 0.1s | Iter Loss 0.1587 | Iter Mean Loss 0.2084\n",
      "CF Training: Epoch 0002 Iter 0250 / 0490 | Time 0.1s | Iter Loss 0.1928 | Iter Mean Loss 0.2083\n",
      "CF Training: Epoch 0002 Iter 0251 / 0490 | Time 0.1s | Iter Loss 0.1730 | Iter Mean Loss 0.2082\n",
      "CF Training: Epoch 0002 Iter 0252 / 0490 | Time 0.1s | Iter Loss 0.2204 | Iter Mean Loss 0.2083\n",
      "CF Training: Epoch 0002 Iter 0253 / 0490 | Time 0.1s | Iter Loss 0.1341 | Iter Mean Loss 0.2080\n",
      "CF Training: Epoch 0002 Iter 0254 / 0490 | Time 0.1s | Iter Loss 0.1825 | Iter Mean Loss 0.2079\n",
      "CF Training: Epoch 0002 Iter 0255 / 0490 | Time 0.1s | Iter Loss 0.1812 | Iter Mean Loss 0.2078\n",
      "CF Training: Epoch 0002 Iter 0256 / 0490 | Time 0.1s | Iter Loss 0.1987 | Iter Mean Loss 0.2077\n",
      "CF Training: Epoch 0002 Iter 0257 / 0490 | Time 0.1s | Iter Loss 0.2291 | Iter Mean Loss 0.2078\n",
      "CF Training: Epoch 0002 Iter 0258 / 0490 | Time 0.1s | Iter Loss 0.2054 | Iter Mean Loss 0.2078\n",
      "CF Training: Epoch 0002 Iter 0259 / 0490 | Time 0.1s | Iter Loss 0.1528 | Iter Mean Loss 0.2076\n",
      "CF Training: Epoch 0002 Iter 0260 / 0490 | Time 0.1s | Iter Loss 0.2121 | Iter Mean Loss 0.2076\n",
      "CF Training: Epoch 0002 Iter 0261 / 0490 | Time 0.1s | Iter Loss 0.1884 | Iter Mean Loss 0.2075\n",
      "CF Training: Epoch 0002 Iter 0262 / 0490 | Time 0.1s | Iter Loss 0.1875 | Iter Mean Loss 0.2075\n",
      "CF Training: Epoch 0002 Iter 0263 / 0490 | Time 0.1s | Iter Loss 0.2058 | Iter Mean Loss 0.2074\n",
      "CF Training: Epoch 0002 Iter 0264 / 0490 | Time 0.1s | Iter Loss 0.1593 | Iter Mean Loss 0.2073\n",
      "CF Training: Epoch 0002 Iter 0265 / 0490 | Time 0.1s | Iter Loss 0.2032 | Iter Mean Loss 0.2072\n",
      "CF Training: Epoch 0002 Iter 0266 / 0490 | Time 0.1s | Iter Loss 0.1834 | Iter Mean Loss 0.2072\n",
      "CF Training: Epoch 0002 Iter 0267 / 0490 | Time 0.1s | Iter Loss 0.1771 | Iter Mean Loss 0.2070\n",
      "CF Training: Epoch 0002 Iter 0268 / 0490 | Time 0.1s | Iter Loss 0.2292 | Iter Mean Loss 0.2071\n",
      "CF Training: Epoch 0002 Iter 0269 / 0490 | Time 0.1s | Iter Loss 0.1870 | Iter Mean Loss 0.2071\n",
      "CF Training: Epoch 0002 Iter 0270 / 0490 | Time 0.1s | Iter Loss 0.1768 | Iter Mean Loss 0.2069\n",
      "CF Training: Epoch 0002 Iter 0271 / 0490 | Time 0.1s | Iter Loss 0.2302 | Iter Mean Loss 0.2070\n",
      "CF Training: Epoch 0002 Iter 0272 / 0490 | Time 0.1s | Iter Loss 0.2065 | Iter Mean Loss 0.2070\n",
      "CF Training: Epoch 0002 Iter 0273 / 0490 | Time 0.1s | Iter Loss 0.1917 | Iter Mean Loss 0.2070\n",
      "CF Training: Epoch 0002 Iter 0274 / 0490 | Time 0.1s | Iter Loss 0.2007 | Iter Mean Loss 0.2069\n",
      "CF Training: Epoch 0002 Iter 0275 / 0490 | Time 0.1s | Iter Loss 0.1983 | Iter Mean Loss 0.2069\n",
      "CF Training: Epoch 0002 Iter 0276 / 0490 | Time 0.1s | Iter Loss 0.1703 | Iter Mean Loss 0.2068\n",
      "CF Training: Epoch 0002 Iter 0277 / 0490 | Time 0.1s | Iter Loss 0.2323 | Iter Mean Loss 0.2069\n",
      "CF Training: Epoch 0002 Iter 0278 / 0490 | Time 0.1s | Iter Loss 0.1758 | Iter Mean Loss 0.2068\n",
      "CF Training: Epoch 0002 Iter 0279 / 0490 | Time 0.1s | Iter Loss 0.2007 | Iter Mean Loss 0.2067\n",
      "CF Training: Epoch 0002 Iter 0280 / 0490 | Time 0.1s | Iter Loss 0.1737 | Iter Mean Loss 0.2066\n",
      "CF Training: Epoch 0002 Iter 0281 / 0490 | Time 0.1s | Iter Loss 0.1938 | Iter Mean Loss 0.2066\n",
      "CF Training: Epoch 0002 Iter 0282 / 0490 | Time 0.1s | Iter Loss 0.2568 | Iter Mean Loss 0.2068\n",
      "CF Training: Epoch 0002 Iter 0283 / 0490 | Time 0.1s | Iter Loss 0.2021 | Iter Mean Loss 0.2067\n",
      "CF Training: Epoch 0002 Iter 0284 / 0490 | Time 0.1s | Iter Loss 0.1871 | Iter Mean Loss 0.2067\n",
      "CF Training: Epoch 0002 Iter 0285 / 0490 | Time 0.1s | Iter Loss 0.2035 | Iter Mean Loss 0.2067\n",
      "CF Training: Epoch 0002 Iter 0286 / 0490 | Time 0.1s | Iter Loss 0.1732 | Iter Mean Loss 0.2065\n",
      "CF Training: Epoch 0002 Iter 0287 / 0490 | Time 0.1s | Iter Loss 0.1758 | Iter Mean Loss 0.2064\n",
      "CF Training: Epoch 0002 Iter 0288 / 0490 | Time 0.1s | Iter Loss 0.1747 | Iter Mean Loss 0.2063\n",
      "CF Training: Epoch 0002 Iter 0289 / 0490 | Time 0.1s | Iter Loss 0.1624 | Iter Mean Loss 0.2062\n",
      "CF Training: Epoch 0002 Iter 0290 / 0490 | Time 0.1s | Iter Loss 0.1801 | Iter Mean Loss 0.2061\n",
      "CF Training: Epoch 0002 Iter 0291 / 0490 | Time 0.1s | Iter Loss 0.2005 | Iter Mean Loss 0.2061\n",
      "CF Training: Epoch 0002 Iter 0292 / 0490 | Time 0.1s | Iter Loss 0.1828 | Iter Mean Loss 0.2060\n",
      "CF Training: Epoch 0002 Iter 0293 / 0490 | Time 0.1s | Iter Loss 0.1795 | Iter Mean Loss 0.2059\n",
      "CF Training: Epoch 0002 Iter 0294 / 0490 | Time 0.1s | Iter Loss 0.1495 | Iter Mean Loss 0.2057\n",
      "CF Training: Epoch 0002 Iter 0295 / 0490 | Time 0.1s | Iter Loss 0.1783 | Iter Mean Loss 0.2056\n",
      "CF Training: Epoch 0002 Iter 0296 / 0490 | Time 0.1s | Iter Loss 0.1843 | Iter Mean Loss 0.2055\n",
      "CF Training: Epoch 0002 Iter 0297 / 0490 | Time 0.1s | Iter Loss 0.1905 | Iter Mean Loss 0.2055\n",
      "CF Training: Epoch 0002 Iter 0298 / 0490 | Time 0.1s | Iter Loss 0.2252 | Iter Mean Loss 0.2056\n",
      "CF Training: Epoch 0002 Iter 0299 / 0490 | Time 0.1s | Iter Loss 0.2193 | Iter Mean Loss 0.2056\n",
      "CF Training: Epoch 0002 Iter 0300 / 0490 | Time 0.1s | Iter Loss 0.2001 | Iter Mean Loss 0.2056\n",
      "CF Training: Epoch 0002 Iter 0301 / 0490 | Time 0.1s | Iter Loss 0.1775 | Iter Mean Loss 0.2055\n",
      "CF Training: Epoch 0002 Iter 0302 / 0490 | Time 0.1s | Iter Loss 0.2344 | Iter Mean Loss 0.2056\n",
      "CF Training: Epoch 0002 Iter 0303 / 0490 | Time 0.1s | Iter Loss 0.1992 | Iter Mean Loss 0.2056\n",
      "CF Training: Epoch 0002 Iter 0304 / 0490 | Time 0.1s | Iter Loss 0.2175 | Iter Mean Loss 0.2056\n",
      "CF Training: Epoch 0002 Iter 0305 / 0490 | Time 0.1s | Iter Loss 0.1978 | Iter Mean Loss 0.2056\n",
      "CF Training: Epoch 0002 Iter 0306 / 0490 | Time 0.1s | Iter Loss 0.1812 | Iter Mean Loss 0.2055\n",
      "CF Training: Epoch 0002 Iter 0307 / 0490 | Time 0.1s | Iter Loss 0.1861 | Iter Mean Loss 0.2054\n",
      "CF Training: Epoch 0002 Iter 0308 / 0490 | Time 0.1s | Iter Loss 0.1720 | Iter Mean Loss 0.2053\n",
      "CF Training: Epoch 0002 Iter 0309 / 0490 | Time 0.1s | Iter Loss 0.1677 | Iter Mean Loss 0.2052\n",
      "CF Training: Epoch 0002 Iter 0310 / 0490 | Time 0.1s | Iter Loss 0.1949 | Iter Mean Loss 0.2052\n",
      "CF Training: Epoch 0002 Iter 0311 / 0490 | Time 0.1s | Iter Loss 0.1785 | Iter Mean Loss 0.2051\n",
      "CF Training: Epoch 0002 Iter 0312 / 0490 | Time 0.1s | Iter Loss 0.2259 | Iter Mean Loss 0.2051\n",
      "CF Training: Epoch 0002 Iter 0313 / 0490 | Time 0.1s | Iter Loss 0.2157 | Iter Mean Loss 0.2052\n",
      "CF Training: Epoch 0002 Iter 0314 / 0490 | Time 0.1s | Iter Loss 0.1975 | Iter Mean Loss 0.2052\n",
      "CF Training: Epoch 0002 Iter 0315 / 0490 | Time 0.1s | Iter Loss 0.2463 | Iter Mean Loss 0.2053\n",
      "CF Training: Epoch 0002 Iter 0316 / 0490 | Time 0.1s | Iter Loss 0.1928 | Iter Mean Loss 0.2052\n",
      "CF Training: Epoch 0002 Iter 0317 / 0490 | Time 0.1s | Iter Loss 0.2249 | Iter Mean Loss 0.2053\n",
      "CF Training: Epoch 0002 Iter 0318 / 0490 | Time 0.1s | Iter Loss 0.2175 | Iter Mean Loss 0.2053\n",
      "CF Training: Epoch 0002 Iter 0319 / 0490 | Time 0.1s | Iter Loss 0.1749 | Iter Mean Loss 0.2053\n",
      "CF Training: Epoch 0002 Iter 0320 / 0490 | Time 0.1s | Iter Loss 0.2031 | Iter Mean Loss 0.2052\n",
      "CF Training: Epoch 0002 Iter 0321 / 0490 | Time 0.1s | Iter Loss 0.1724 | Iter Mean Loss 0.2051\n",
      "CF Training: Epoch 0002 Iter 0322 / 0490 | Time 0.1s | Iter Loss 0.2145 | Iter Mean Loss 0.2052\n",
      "CF Training: Epoch 0002 Iter 0323 / 0490 | Time 0.1s | Iter Loss 0.2139 | Iter Mean Loss 0.2052\n",
      "CF Training: Epoch 0002 Iter 0324 / 0490 | Time 0.1s | Iter Loss 0.2184 | Iter Mean Loss 0.2052\n",
      "CF Training: Epoch 0002 Iter 0325 / 0490 | Time 0.1s | Iter Loss 0.2278 | Iter Mean Loss 0.2053\n",
      "CF Training: Epoch 0002 Iter 0326 / 0490 | Time 0.1s | Iter Loss 0.1833 | Iter Mean Loss 0.2052\n",
      "CF Training: Epoch 0002 Iter 0327 / 0490 | Time 0.1s | Iter Loss 0.1783 | Iter Mean Loss 0.2052\n",
      "CF Training: Epoch 0002 Iter 0328 / 0490 | Time 0.1s | Iter Loss 0.1661 | Iter Mean Loss 0.2050\n",
      "CF Training: Epoch 0002 Iter 0329 / 0490 | Time 0.1s | Iter Loss 0.2021 | Iter Mean Loss 0.2050\n",
      "CF Training: Epoch 0002 Iter 0330 / 0490 | Time 0.1s | Iter Loss 0.1855 | Iter Mean Loss 0.2050\n",
      "CF Training: Epoch 0002 Iter 0331 / 0490 | Time 0.1s | Iter Loss 0.2213 | Iter Mean Loss 0.2050\n",
      "CF Training: Epoch 0002 Iter 0332 / 0490 | Time 0.1s | Iter Loss 0.1857 | Iter Mean Loss 0.2050\n",
      "CF Training: Epoch 0002 Iter 0333 / 0490 | Time 0.1s | Iter Loss 0.1569 | Iter Mean Loss 0.2048\n",
      "CF Training: Epoch 0002 Iter 0334 / 0490 | Time 0.1s | Iter Loss 0.1817 | Iter Mean Loss 0.2048\n",
      "CF Training: Epoch 0002 Iter 0335 / 0490 | Time 0.1s | Iter Loss 0.1727 | Iter Mean Loss 0.2047\n",
      "CF Training: Epoch 0002 Iter 0336 / 0490 | Time 0.1s | Iter Loss 0.1854 | Iter Mean Loss 0.2046\n",
      "CF Training: Epoch 0002 Iter 0337 / 0490 | Time 0.1s | Iter Loss 0.2035 | Iter Mean Loss 0.2046\n",
      "CF Training: Epoch 0002 Iter 0338 / 0490 | Time 0.1s | Iter Loss 0.2239 | Iter Mean Loss 0.2047\n",
      "CF Training: Epoch 0002 Iter 0339 / 0490 | Time 0.1s | Iter Loss 0.2103 | Iter Mean Loss 0.2047\n",
      "CF Training: Epoch 0002 Iter 0340 / 0490 | Time 0.1s | Iter Loss 0.1781 | Iter Mean Loss 0.2046\n",
      "CF Training: Epoch 0002 Iter 0341 / 0490 | Time 0.1s | Iter Loss 0.2142 | Iter Mean Loss 0.2046\n",
      "CF Training: Epoch 0002 Iter 0342 / 0490 | Time 0.1s | Iter Loss 0.2099 | Iter Mean Loss 0.2046\n",
      "CF Training: Epoch 0002 Iter 0343 / 0490 | Time 0.1s | Iter Loss 0.2134 | Iter Mean Loss 0.2047\n",
      "CF Training: Epoch 0002 Iter 0344 / 0490 | Time 0.1s | Iter Loss 0.2036 | Iter Mean Loss 0.2047\n",
      "CF Training: Epoch 0002 Iter 0345 / 0490 | Time 0.1s | Iter Loss 0.1864 | Iter Mean Loss 0.2046\n",
      "CF Training: Epoch 0002 Iter 0346 / 0490 | Time 0.1s | Iter Loss 0.1966 | Iter Mean Loss 0.2046\n",
      "CF Training: Epoch 0002 Iter 0347 / 0490 | Time 0.1s | Iter Loss 0.2150 | Iter Mean Loss 0.2046\n",
      "CF Training: Epoch 0002 Iter 0348 / 0490 | Time 0.1s | Iter Loss 0.1753 | Iter Mean Loss 0.2045\n",
      "CF Training: Epoch 0002 Iter 0349 / 0490 | Time 0.1s | Iter Loss 0.1760 | Iter Mean Loss 0.2044\n",
      "CF Training: Epoch 0002 Iter 0350 / 0490 | Time 0.1s | Iter Loss 0.1618 | Iter Mean Loss 0.2043\n",
      "CF Training: Epoch 0002 Iter 0351 / 0490 | Time 0.1s | Iter Loss 0.2732 | Iter Mean Loss 0.2045\n",
      "CF Training: Epoch 0002 Iter 0352 / 0490 | Time 0.1s | Iter Loss 0.1948 | Iter Mean Loss 0.2045\n",
      "CF Training: Epoch 0002 Iter 0353 / 0490 | Time 0.1s | Iter Loss 0.2202 | Iter Mean Loss 0.2045\n",
      "CF Training: Epoch 0002 Iter 0354 / 0490 | Time 0.1s | Iter Loss 0.1866 | Iter Mean Loss 0.2045\n",
      "CF Training: Epoch 0002 Iter 0355 / 0490 | Time 0.1s | Iter Loss 0.1995 | Iter Mean Loss 0.2045\n",
      "CF Training: Epoch 0002 Iter 0356 / 0490 | Time 0.1s | Iter Loss 0.2081 | Iter Mean Loss 0.2045\n",
      "CF Training: Epoch 0002 Iter 0357 / 0490 | Time 0.1s | Iter Loss 0.2273 | Iter Mean Loss 0.2045\n",
      "CF Training: Epoch 0002 Iter 0358 / 0490 | Time 0.1s | Iter Loss 0.1965 | Iter Mean Loss 0.2045\n",
      "CF Training: Epoch 0002 Iter 0359 / 0490 | Time 0.1s | Iter Loss 0.2034 | Iter Mean Loss 0.2045\n",
      "CF Training: Epoch 0002 Iter 0360 / 0490 | Time 0.1s | Iter Loss 0.1568 | Iter Mean Loss 0.2044\n",
      "CF Training: Epoch 0002 Iter 0361 / 0490 | Time 0.1s | Iter Loss 0.1960 | Iter Mean Loss 0.2044\n",
      "CF Training: Epoch 0002 Iter 0362 / 0490 | Time 0.1s | Iter Loss 0.1999 | Iter Mean Loss 0.2044\n",
      "CF Training: Epoch 0002 Iter 0363 / 0490 | Time 0.1s | Iter Loss 0.2142 | Iter Mean Loss 0.2044\n",
      "CF Training: Epoch 0002 Iter 0364 / 0490 | Time 0.1s | Iter Loss 0.1793 | Iter Mean Loss 0.2043\n",
      "CF Training: Epoch 0002 Iter 0365 / 0490 | Time 0.1s | Iter Loss 0.1592 | Iter Mean Loss 0.2042\n",
      "CF Training: Epoch 0002 Iter 0366 / 0490 | Time 0.1s | Iter Loss 0.2234 | Iter Mean Loss 0.2042\n",
      "CF Training: Epoch 0002 Iter 0367 / 0490 | Time 0.1s | Iter Loss 0.2035 | Iter Mean Loss 0.2042\n",
      "CF Training: Epoch 0002 Iter 0368 / 0490 | Time 0.1s | Iter Loss 0.1946 | Iter Mean Loss 0.2042\n",
      "CF Training: Epoch 0002 Iter 0369 / 0490 | Time 0.1s | Iter Loss 0.1631 | Iter Mean Loss 0.2041\n",
      "CF Training: Epoch 0002 Iter 0370 / 0490 | Time 0.1s | Iter Loss 0.2104 | Iter Mean Loss 0.2041\n",
      "CF Training: Epoch 0002 Iter 0371 / 0490 | Time 0.0s | Iter Loss 0.2078 | Iter Mean Loss 0.2041\n",
      "CF Training: Epoch 0002 Iter 0372 / 0490 | Time 0.1s | Iter Loss 0.1680 | Iter Mean Loss 0.2040\n",
      "CF Training: Epoch 0002 Iter 0373 / 0490 | Time 0.1s | Iter Loss 0.1679 | Iter Mean Loss 0.2039\n",
      "CF Training: Epoch 0002 Iter 0374 / 0490 | Time 0.1s | Iter Loss 0.1672 | Iter Mean Loss 0.2038\n",
      "CF Training: Epoch 0002 Iter 0375 / 0490 | Time 0.1s | Iter Loss 0.1831 | Iter Mean Loss 0.2038\n",
      "CF Training: Epoch 0002 Iter 0376 / 0490 | Time 0.1s | Iter Loss 0.2291 | Iter Mean Loss 0.2038\n",
      "CF Training: Epoch 0002 Iter 0377 / 0490 | Time 0.1s | Iter Loss 0.1902 | Iter Mean Loss 0.2038\n",
      "CF Training: Epoch 0002 Iter 0378 / 0490 | Time 0.1s | Iter Loss 0.1872 | Iter Mean Loss 0.2038\n",
      "CF Training: Epoch 0002 Iter 0379 / 0490 | Time 0.0s | Iter Loss 0.1430 | Iter Mean Loss 0.2036\n",
      "CF Training: Epoch 0002 Iter 0380 / 0490 | Time 0.1s | Iter Loss 0.1801 | Iter Mean Loss 0.2035\n",
      "CF Training: Epoch 0002 Iter 0381 / 0490 | Time 0.1s | Iter Loss 0.1625 | Iter Mean Loss 0.2034\n",
      "CF Training: Epoch 0002 Iter 0382 / 0490 | Time 0.1s | Iter Loss 0.1686 | Iter Mean Loss 0.2033\n",
      "CF Training: Epoch 0002 Iter 0383 / 0490 | Time 0.1s | Iter Loss 0.1576 | Iter Mean Loss 0.2032\n",
      "CF Training: Epoch 0002 Iter 0384 / 0490 | Time 0.1s | Iter Loss 0.1935 | Iter Mean Loss 0.2032\n",
      "CF Training: Epoch 0002 Iter 0385 / 0490 | Time 0.1s | Iter Loss 0.1887 | Iter Mean Loss 0.2032\n",
      "CF Training: Epoch 0002 Iter 0386 / 0490 | Time 0.1s | Iter Loss 0.2026 | Iter Mean Loss 0.2032\n",
      "CF Training: Epoch 0002 Iter 0387 / 0490 | Time 0.1s | Iter Loss 0.1621 | Iter Mean Loss 0.2031\n",
      "CF Training: Epoch 0002 Iter 0388 / 0490 | Time 0.1s | Iter Loss 0.1943 | Iter Mean Loss 0.2030\n",
      "CF Training: Epoch 0002 Iter 0389 / 0490 | Time 0.1s | Iter Loss 0.1806 | Iter Mean Loss 0.2030\n",
      "CF Training: Epoch 0002 Iter 0390 / 0490 | Time 0.1s | Iter Loss 0.1569 | Iter Mean Loss 0.2029\n",
      "CF Training: Epoch 0002 Iter 0391 / 0490 | Time 0.1s | Iter Loss 0.1802 | Iter Mean Loss 0.2028\n",
      "CF Training: Epoch 0002 Iter 0392 / 0490 | Time 0.1s | Iter Loss 0.1558 | Iter Mean Loss 0.2027\n",
      "CF Training: Epoch 0002 Iter 0393 / 0490 | Time 0.1s | Iter Loss 0.2160 | Iter Mean Loss 0.2027\n",
      "CF Training: Epoch 0002 Iter 0394 / 0490 | Time 0.1s | Iter Loss 0.1956 | Iter Mean Loss 0.2027\n",
      "CF Training: Epoch 0002 Iter 0395 / 0490 | Time 0.1s | Iter Loss 0.2333 | Iter Mean Loss 0.2028\n",
      "CF Training: Epoch 0002 Iter 0396 / 0490 | Time 0.1s | Iter Loss 0.1498 | Iter Mean Loss 0.2026\n",
      "CF Training: Epoch 0002 Iter 0397 / 0490 | Time 0.1s | Iter Loss 0.1792 | Iter Mean Loss 0.2026\n",
      "CF Training: Epoch 0002 Iter 0398 / 0490 | Time 0.1s | Iter Loss 0.1832 | Iter Mean Loss 0.2025\n",
      "CF Training: Epoch 0002 Iter 0399 / 0490 | Time 0.1s | Iter Loss 0.1791 | Iter Mean Loss 0.2025\n",
      "CF Training: Epoch 0002 Iter 0400 / 0490 | Time 0.1s | Iter Loss 0.1966 | Iter Mean Loss 0.2025\n",
      "CF Training: Epoch 0002 Iter 0401 / 0490 | Time 0.1s | Iter Loss 0.2249 | Iter Mean Loss 0.2025\n",
      "CF Training: Epoch 0002 Iter 0402 / 0490 | Time 0.1s | Iter Loss 0.1639 | Iter Mean Loss 0.2024\n",
      "CF Training: Epoch 0002 Iter 0403 / 0490 | Time 0.1s | Iter Loss 0.1629 | Iter Mean Loss 0.2023\n",
      "CF Training: Epoch 0002 Iter 0404 / 0490 | Time 0.1s | Iter Loss 0.1622 | Iter Mean Loss 0.2022\n",
      "CF Training: Epoch 0002 Iter 0405 / 0490 | Time 0.1s | Iter Loss 0.1696 | Iter Mean Loss 0.2021\n",
      "CF Training: Epoch 0002 Iter 0406 / 0490 | Time 0.1s | Iter Loss 0.1599 | Iter Mean Loss 0.2020\n",
      "CF Training: Epoch 0002 Iter 0407 / 0490 | Time 0.1s | Iter Loss 0.1831 | Iter Mean Loss 0.2020\n",
      "CF Training: Epoch 0002 Iter 0408 / 0490 | Time 0.1s | Iter Loss 0.2271 | Iter Mean Loss 0.2020\n",
      "CF Training: Epoch 0002 Iter 0409 / 0490 | Time 0.1s | Iter Loss 0.1655 | Iter Mean Loss 0.2020\n",
      "CF Training: Epoch 0002 Iter 0410 / 0490 | Time 0.1s | Iter Loss 0.1694 | Iter Mean Loss 0.2019\n",
      "CF Training: Epoch 0002 Iter 0411 / 0490 | Time 0.1s | Iter Loss 0.1804 | Iter Mean Loss 0.2018\n",
      "CF Training: Epoch 0002 Iter 0412 / 0490 | Time 0.1s | Iter Loss 0.2112 | Iter Mean Loss 0.2019\n",
      "CF Training: Epoch 0002 Iter 0413 / 0490 | Time 0.1s | Iter Loss 0.1538 | Iter Mean Loss 0.2017\n",
      "CF Training: Epoch 0002 Iter 0414 / 0490 | Time 0.1s | Iter Loss 0.1894 | Iter Mean Loss 0.2017\n",
      "CF Training: Epoch 0002 Iter 0415 / 0490 | Time 0.1s | Iter Loss 0.1794 | Iter Mean Loss 0.2017\n",
      "CF Training: Epoch 0002 Iter 0416 / 0490 | Time 0.1s | Iter Loss 0.2058 | Iter Mean Loss 0.2017\n",
      "CF Training: Epoch 0002 Iter 0417 / 0490 | Time 0.1s | Iter Loss 0.1859 | Iter Mean Loss 0.2016\n",
      "CF Training: Epoch 0002 Iter 0418 / 0490 | Time 0.1s | Iter Loss 0.1747 | Iter Mean Loss 0.2016\n",
      "CF Training: Epoch 0002 Iter 0419 / 0490 | Time 0.1s | Iter Loss 0.1724 | Iter Mean Loss 0.2015\n",
      "CF Training: Epoch 0002 Iter 0420 / 0490 | Time 0.1s | Iter Loss 0.1865 | Iter Mean Loss 0.2015\n",
      "CF Training: Epoch 0002 Iter 0421 / 0490 | Time 0.1s | Iter Loss 0.1648 | Iter Mean Loss 0.2014\n",
      "CF Training: Epoch 0002 Iter 0422 / 0490 | Time 0.1s | Iter Loss 0.1610 | Iter Mean Loss 0.2013\n",
      "CF Training: Epoch 0002 Iter 0423 / 0490 | Time 0.1s | Iter Loss 0.2041 | Iter Mean Loss 0.2013\n",
      "CF Training: Epoch 0002 Iter 0424 / 0490 | Time 0.1s | Iter Loss 0.1779 | Iter Mean Loss 0.2012\n",
      "CF Training: Epoch 0002 Iter 0425 / 0490 | Time 0.1s | Iter Loss 0.1780 | Iter Mean Loss 0.2012\n",
      "CF Training: Epoch 0002 Iter 0426 / 0490 | Time 0.1s | Iter Loss 0.2143 | Iter Mean Loss 0.2012\n",
      "CF Training: Epoch 0002 Iter 0427 / 0490 | Time 0.1s | Iter Loss 0.1895 | Iter Mean Loss 0.2012\n",
      "CF Training: Epoch 0002 Iter 0428 / 0490 | Time 0.1s | Iter Loss 0.1890 | Iter Mean Loss 0.2011\n",
      "CF Training: Epoch 0002 Iter 0429 / 0490 | Time 0.1s | Iter Loss 0.1666 | Iter Mean Loss 0.2011\n",
      "CF Training: Epoch 0002 Iter 0430 / 0490 | Time 0.1s | Iter Loss 0.1842 | Iter Mean Loss 0.2010\n",
      "CF Training: Epoch 0002 Iter 0431 / 0490 | Time 0.1s | Iter Loss 0.1713 | Iter Mean Loss 0.2010\n",
      "CF Training: Epoch 0002 Iter 0432 / 0490 | Time 0.1s | Iter Loss 0.1543 | Iter Mean Loss 0.2008\n",
      "CF Training: Epoch 0002 Iter 0433 / 0490 | Time 0.1s | Iter Loss 0.1426 | Iter Mean Loss 0.2007\n",
      "CF Training: Epoch 0002 Iter 0434 / 0490 | Time 0.1s | Iter Loss 0.1777 | Iter Mean Loss 0.2007\n",
      "CF Training: Epoch 0002 Iter 0435 / 0490 | Time 0.1s | Iter Loss 0.1878 | Iter Mean Loss 0.2006\n",
      "CF Training: Epoch 0002 Iter 0436 / 0490 | Time 0.1s | Iter Loss 0.1880 | Iter Mean Loss 0.2006\n",
      "CF Training: Epoch 0002 Iter 0437 / 0490 | Time 0.1s | Iter Loss 0.1528 | Iter Mean Loss 0.2005\n",
      "CF Training: Epoch 0002 Iter 0438 / 0490 | Time 0.1s | Iter Loss 0.1997 | Iter Mean Loss 0.2005\n",
      "CF Training: Epoch 0002 Iter 0439 / 0490 | Time 0.1s | Iter Loss 0.1541 | Iter Mean Loss 0.2004\n",
      "CF Training: Epoch 0002 Iter 0440 / 0490 | Time 0.1s | Iter Loss 0.1591 | Iter Mean Loss 0.2003\n",
      "CF Training: Epoch 0002 Iter 0441 / 0490 | Time 0.1s | Iter Loss 0.1836 | Iter Mean Loss 0.2003\n",
      "CF Training: Epoch 0002 Iter 0442 / 0490 | Time 0.1s | Iter Loss 0.1880 | Iter Mean Loss 0.2002\n",
      "CF Training: Epoch 0002 Iter 0443 / 0490 | Time 0.1s | Iter Loss 0.1542 | Iter Mean Loss 0.2001\n",
      "CF Training: Epoch 0002 Iter 0444 / 0490 | Time 0.1s | Iter Loss 0.1371 | Iter Mean Loss 0.2000\n",
      "CF Training: Epoch 0002 Iter 0445 / 0490 | Time 0.1s | Iter Loss 0.1468 | Iter Mean Loss 0.1999\n",
      "CF Training: Epoch 0002 Iter 0446 / 0490 | Time 0.1s | Iter Loss 0.1264 | Iter Mean Loss 0.1997\n",
      "CF Training: Epoch 0002 Iter 0447 / 0490 | Time 0.1s | Iter Loss 0.1609 | Iter Mean Loss 0.1996\n",
      "CF Training: Epoch 0002 Iter 0448 / 0490 | Time 0.1s | Iter Loss 0.2118 | Iter Mean Loss 0.1996\n",
      "CF Training: Epoch 0002 Iter 0449 / 0490 | Time 0.1s | Iter Loss 0.1992 | Iter Mean Loss 0.1996\n",
      "CF Training: Epoch 0002 Iter 0450 / 0490 | Time 0.1s | Iter Loss 0.2040 | Iter Mean Loss 0.1996\n",
      "CF Training: Epoch 0002 Iter 0451 / 0490 | Time 0.1s | Iter Loss 0.1821 | Iter Mean Loss 0.1996\n",
      "CF Training: Epoch 0002 Iter 0452 / 0490 | Time 0.1s | Iter Loss 0.1708 | Iter Mean Loss 0.1995\n",
      "CF Training: Epoch 0002 Iter 0453 / 0490 | Time 0.1s | Iter Loss 0.1492 | Iter Mean Loss 0.1994\n",
      "CF Training: Epoch 0002 Iter 0454 / 0490 | Time 0.1s | Iter Loss 0.1973 | Iter Mean Loss 0.1994\n",
      "CF Training: Epoch 0002 Iter 0455 / 0490 | Time 0.1s | Iter Loss 0.1791 | Iter Mean Loss 0.1994\n",
      "CF Training: Epoch 0002 Iter 0456 / 0490 | Time 0.1s | Iter Loss 0.1360 | Iter Mean Loss 0.1992\n",
      "CF Training: Epoch 0002 Iter 0457 / 0490 | Time 0.1s | Iter Loss 0.1853 | Iter Mean Loss 0.1992\n",
      "CF Training: Epoch 0002 Iter 0458 / 0490 | Time 0.1s | Iter Loss 0.1685 | Iter Mean Loss 0.1991\n",
      "CF Training: Epoch 0002 Iter 0459 / 0490 | Time 0.1s | Iter Loss 0.1781 | Iter Mean Loss 0.1991\n",
      "CF Training: Epoch 0002 Iter 0460 / 0490 | Time 0.1s | Iter Loss 0.2005 | Iter Mean Loss 0.1991\n",
      "CF Training: Epoch 0002 Iter 0461 / 0490 | Time 0.1s | Iter Loss 0.1715 | Iter Mean Loss 0.1990\n",
      "CF Training: Epoch 0002 Iter 0462 / 0490 | Time 0.1s | Iter Loss 0.1714 | Iter Mean Loss 0.1990\n",
      "CF Training: Epoch 0002 Iter 0463 / 0490 | Time 0.1s | Iter Loss 0.2174 | Iter Mean Loss 0.1990\n",
      "CF Training: Epoch 0002 Iter 0464 / 0490 | Time 0.1s | Iter Loss 0.2284 | Iter Mean Loss 0.1991\n",
      "CF Training: Epoch 0002 Iter 0465 / 0490 | Time 0.1s | Iter Loss 0.1759 | Iter Mean Loss 0.1990\n",
      "CF Training: Epoch 0002 Iter 0466 / 0490 | Time 0.1s | Iter Loss 0.1791 | Iter Mean Loss 0.1990\n",
      "CF Training: Epoch 0002 Iter 0467 / 0490 | Time 0.1s | Iter Loss 0.2079 | Iter Mean Loss 0.1990\n",
      "CF Training: Epoch 0002 Iter 0468 / 0490 | Time 0.1s | Iter Loss 0.1607 | Iter Mean Loss 0.1989\n",
      "CF Training: Epoch 0002 Iter 0469 / 0490 | Time 0.1s | Iter Loss 0.2012 | Iter Mean Loss 0.1989\n",
      "CF Training: Epoch 0002 Iter 0470 / 0490 | Time 0.1s | Iter Loss 0.1502 | Iter Mean Loss 0.1988\n",
      "CF Training: Epoch 0002 Iter 0471 / 0490 | Time 0.1s | Iter Loss 0.1986 | Iter Mean Loss 0.1988\n",
      "CF Training: Epoch 0002 Iter 0472 / 0490 | Time 0.1s | Iter Loss 0.1955 | Iter Mean Loss 0.1988\n",
      "CF Training: Epoch 0002 Iter 0473 / 0490 | Time 0.1s | Iter Loss 0.1846 | Iter Mean Loss 0.1988\n",
      "CF Training: Epoch 0002 Iter 0474 / 0490 | Time 0.1s | Iter Loss 0.1900 | Iter Mean Loss 0.1988\n",
      "CF Training: Epoch 0002 Iter 0475 / 0490 | Time 0.1s | Iter Loss 0.1835 | Iter Mean Loss 0.1987\n",
      "CF Training: Epoch 0002 Iter 0476 / 0490 | Time 0.1s | Iter Loss 0.1469 | Iter Mean Loss 0.1986\n",
      "CF Training: Epoch 0002 Iter 0477 / 0490 | Time 0.1s | Iter Loss 0.1545 | Iter Mean Loss 0.1985\n",
      "CF Training: Epoch 0002 Iter 0478 / 0490 | Time 0.1s | Iter Loss 0.1819 | Iter Mean Loss 0.1985\n",
      "CF Training: Epoch 0002 Iter 0479 / 0490 | Time 0.0s | Iter Loss 0.1868 | Iter Mean Loss 0.1985\n",
      "CF Training: Epoch 0002 Iter 0480 / 0490 | Time 0.1s | Iter Loss 0.1765 | Iter Mean Loss 0.1984\n",
      "CF Training: Epoch 0002 Iter 0481 / 0490 | Time 0.1s | Iter Loss 0.1519 | Iter Mean Loss 0.1983\n",
      "CF Training: Epoch 0002 Iter 0482 / 0490 | Time 0.1s | Iter Loss 0.2042 | Iter Mean Loss 0.1984\n",
      "CF Training: Epoch 0002 Iter 0483 / 0490 | Time 0.1s | Iter Loss 0.2030 | Iter Mean Loss 0.1984\n",
      "CF Training: Epoch 0002 Iter 0484 / 0490 | Time 0.1s | Iter Loss 0.1623 | Iter Mean Loss 0.1983\n",
      "CF Training: Epoch 0002 Iter 0485 / 0490 | Time 0.1s | Iter Loss 0.1415 | Iter Mean Loss 0.1982\n",
      "CF Training: Epoch 0002 Iter 0486 / 0490 | Time 0.1s | Iter Loss 0.1413 | Iter Mean Loss 0.1981\n",
      "CF Training: Epoch 0002 Iter 0487 / 0490 | Time 0.1s | Iter Loss 0.1736 | Iter Mean Loss 0.1980\n",
      "CF Training: Epoch 0002 Iter 0488 / 0490 | Time 0.1s | Iter Loss 0.1584 | Iter Mean Loss 0.1979\n",
      "CF Training: Epoch 0002 Iter 0489 / 0490 | Time 0.1s | Iter Loss 0.1820 | Iter Mean Loss 0.1979\n",
      "CF Training: Epoch 0002 Iter 0490 / 0490 | Time 0.1s | Iter Loss 0.1582 | Iter Mean Loss 0.1978\n",
      "CF Training: Epoch 0002 Total Iter 0490 | Total Time 48.9s | Iter Mean Loss 0.1978\n",
      "CF Training: Epoch 0003 Iter 0001 / 0490 | Time 0.1s | Iter Loss 0.1778 | Iter Mean Loss 0.1778\n",
      "CF Training: Epoch 0003 Iter 0002 / 0490 | Time 0.1s | Iter Loss 0.1788 | Iter Mean Loss 0.1783\n",
      "CF Training: Epoch 0003 Iter 0003 / 0490 | Time 0.1s | Iter Loss 0.1450 | Iter Mean Loss 0.1672\n",
      "CF Training: Epoch 0003 Iter 0004 / 0490 | Time 0.1s | Iter Loss 0.1795 | Iter Mean Loss 0.1703\n",
      "CF Training: Epoch 0003 Iter 0005 / 0490 | Time 0.1s | Iter Loss 0.1303 | Iter Mean Loss 0.1623\n",
      "CF Training: Epoch 0003 Iter 0006 / 0490 | Time 0.1s | Iter Loss 0.1787 | Iter Mean Loss 0.1650\n",
      "CF Training: Epoch 0003 Iter 0007 / 0490 | Time 0.1s | Iter Loss 0.1711 | Iter Mean Loss 0.1659\n",
      "CF Training: Epoch 0003 Iter 0008 / 0490 | Time 0.1s | Iter Loss 0.1830 | Iter Mean Loss 0.1680\n",
      "CF Training: Epoch 0003 Iter 0009 / 0490 | Time 0.1s | Iter Loss 0.1599 | Iter Mean Loss 0.1671\n",
      "CF Training: Epoch 0003 Iter 0010 / 0490 | Time 0.1s | Iter Loss 0.1250 | Iter Mean Loss 0.1629\n",
      "CF Training: Epoch 0003 Iter 0011 / 0490 | Time 0.1s | Iter Loss 0.1557 | Iter Mean Loss 0.1622\n",
      "CF Training: Epoch 0003 Iter 0012 / 0490 | Time 0.1s | Iter Loss 0.1749 | Iter Mean Loss 0.1633\n",
      "CF Training: Epoch 0003 Iter 0013 / 0490 | Time 0.0s | Iter Loss 0.1504 | Iter Mean Loss 0.1623\n",
      "CF Training: Epoch 0003 Iter 0014 / 0490 | Time 0.1s | Iter Loss 0.1771 | Iter Mean Loss 0.1634\n",
      "CF Training: Epoch 0003 Iter 0015 / 0490 | Time 0.1s | Iter Loss 0.1764 | Iter Mean Loss 0.1642\n",
      "CF Training: Epoch 0003 Iter 0016 / 0490 | Time 0.1s | Iter Loss 0.2373 | Iter Mean Loss 0.1688\n",
      "CF Training: Epoch 0003 Iter 0017 / 0490 | Time 0.1s | Iter Loss 0.1996 | Iter Mean Loss 0.1706\n",
      "CF Training: Epoch 0003 Iter 0018 / 0490 | Time 0.1s | Iter Loss 0.1610 | Iter Mean Loss 0.1701\n",
      "CF Training: Epoch 0003 Iter 0019 / 0490 | Time 0.1s | Iter Loss 0.1750 | Iter Mean Loss 0.1703\n",
      "CF Training: Epoch 0003 Iter 0020 / 0490 | Time 0.1s | Iter Loss 0.1701 | Iter Mean Loss 0.1703\n",
      "CF Training: Epoch 0003 Iter 0021 / 0490 | Time 0.0s | Iter Loss 0.1597 | Iter Mean Loss 0.1698\n",
      "CF Training: Epoch 0003 Iter 0022 / 0490 | Time 0.1s | Iter Loss 0.1890 | Iter Mean Loss 0.1707\n",
      "CF Training: Epoch 0003 Iter 0023 / 0490 | Time 0.1s | Iter Loss 0.1469 | Iter Mean Loss 0.1697\n",
      "CF Training: Epoch 0003 Iter 0024 / 0490 | Time 0.1s | Iter Loss 0.1789 | Iter Mean Loss 0.1700\n",
      "CF Training: Epoch 0003 Iter 0025 / 0490 | Time 0.1s | Iter Loss 0.1642 | Iter Mean Loss 0.1698\n",
      "CF Training: Epoch 0003 Iter 0026 / 0490 | Time 0.1s | Iter Loss 0.1531 | Iter Mean Loss 0.1692\n",
      "CF Training: Epoch 0003 Iter 0027 / 0490 | Time 0.1s | Iter Loss 0.1686 | Iter Mean Loss 0.1691\n",
      "CF Training: Epoch 0003 Iter 0028 / 0490 | Time 0.1s | Iter Loss 0.1698 | Iter Mean Loss 0.1692\n",
      "CF Training: Epoch 0003 Iter 0029 / 0490 | Time 0.1s | Iter Loss 0.1967 | Iter Mean Loss 0.1701\n",
      "CF Training: Epoch 0003 Iter 0030 / 0490 | Time 0.1s | Iter Loss 0.1725 | Iter Mean Loss 0.1702\n",
      "CF Training: Epoch 0003 Iter 0031 / 0490 | Time 0.1s | Iter Loss 0.1806 | Iter Mean Loss 0.1705\n",
      "CF Training: Epoch 0003 Iter 0032 / 0490 | Time 0.1s | Iter Loss 0.1912 | Iter Mean Loss 0.1712\n",
      "CF Training: Epoch 0003 Iter 0033 / 0490 | Time 0.1s | Iter Loss 0.1489 | Iter Mean Loss 0.1705\n",
      "CF Training: Epoch 0003 Iter 0034 / 0490 | Time 0.1s | Iter Loss 0.1823 | Iter Mean Loss 0.1708\n",
      "CF Training: Epoch 0003 Iter 0035 / 0490 | Time 0.1s | Iter Loss 0.1963 | Iter Mean Loss 0.1716\n",
      "CF Training: Epoch 0003 Iter 0036 / 0490 | Time 0.1s | Iter Loss 0.1712 | Iter Mean Loss 0.1716\n",
      "CF Training: Epoch 0003 Iter 0037 / 0490 | Time 0.1s | Iter Loss 0.1427 | Iter Mean Loss 0.1708\n",
      "CF Training: Epoch 0003 Iter 0038 / 0490 | Time 0.1s | Iter Loss 0.2101 | Iter Mean Loss 0.1718\n",
      "CF Training: Epoch 0003 Iter 0039 / 0490 | Time 0.1s | Iter Loss 0.1292 | Iter Mean Loss 0.1707\n",
      "CF Training: Epoch 0003 Iter 0040 / 0490 | Time 0.1s | Iter Loss 0.1913 | Iter Mean Loss 0.1712\n",
      "CF Training: Epoch 0003 Iter 0041 / 0490 | Time 0.1s | Iter Loss 0.1885 | Iter Mean Loss 0.1717\n",
      "CF Training: Epoch 0003 Iter 0042 / 0490 | Time 0.1s | Iter Loss 0.1669 | Iter Mean Loss 0.1715\n",
      "CF Training: Epoch 0003 Iter 0043 / 0490 | Time 0.1s | Iter Loss 0.1275 | Iter Mean Loss 0.1705\n",
      "CF Training: Epoch 0003 Iter 0044 / 0490 | Time 0.1s | Iter Loss 0.1344 | Iter Mean Loss 0.1697\n",
      "CF Training: Epoch 0003 Iter 0045 / 0490 | Time 0.1s | Iter Loss 0.1847 | Iter Mean Loss 0.1700\n",
      "CF Training: Epoch 0003 Iter 0046 / 0490 | Time 0.1s | Iter Loss 0.1717 | Iter Mean Loss 0.1701\n",
      "CF Training: Epoch 0003 Iter 0047 / 0490 | Time 0.1s | Iter Loss 0.2414 | Iter Mean Loss 0.1716\n",
      "CF Training: Epoch 0003 Iter 0048 / 0490 | Time 0.1s | Iter Loss 0.1922 | Iter Mean Loss 0.1720\n",
      "CF Training: Epoch 0003 Iter 0049 / 0490 | Time 0.1s | Iter Loss 0.1984 | Iter Mean Loss 0.1726\n",
      "CF Training: Epoch 0003 Iter 0050 / 0490 | Time 0.1s | Iter Loss 0.1636 | Iter Mean Loss 0.1724\n",
      "CF Training: Epoch 0003 Iter 0051 / 0490 | Time 0.1s | Iter Loss 0.1738 | Iter Mean Loss 0.1724\n",
      "CF Training: Epoch 0003 Iter 0052 / 0490 | Time 0.1s | Iter Loss 0.1459 | Iter Mean Loss 0.1719\n",
      "CF Training: Epoch 0003 Iter 0053 / 0490 | Time 0.1s | Iter Loss 0.1928 | Iter Mean Loss 0.1723\n",
      "CF Training: Epoch 0003 Iter 0054 / 0490 | Time 0.1s | Iter Loss 0.1356 | Iter Mean Loss 0.1716\n",
      "CF Training: Epoch 0003 Iter 0055 / 0490 | Time 0.1s | Iter Loss 0.1624 | Iter Mean Loss 0.1714\n",
      "CF Training: Epoch 0003 Iter 0056 / 0490 | Time 0.1s | Iter Loss 0.1635 | Iter Mean Loss 0.1713\n",
      "CF Training: Epoch 0003 Iter 0057 / 0490 | Time 0.1s | Iter Loss 0.1468 | Iter Mean Loss 0.1709\n",
      "CF Training: Epoch 0003 Iter 0058 / 0490 | Time 0.1s | Iter Loss 0.2047 | Iter Mean Loss 0.1715\n",
      "CF Training: Epoch 0003 Iter 0059 / 0490 | Time 0.0s | Iter Loss 0.2244 | Iter Mean Loss 0.1724\n",
      "CF Training: Epoch 0003 Iter 0060 / 0490 | Time 0.1s | Iter Loss 0.1558 | Iter Mean Loss 0.1721\n",
      "CF Training: Epoch 0003 Iter 0061 / 0490 | Time 0.1s | Iter Loss 0.1744 | Iter Mean Loss 0.1721\n",
      "CF Training: Epoch 0003 Iter 0062 / 0490 | Time 0.1s | Iter Loss 0.1947 | Iter Mean Loss 0.1725\n",
      "CF Training: Epoch 0003 Iter 0063 / 0490 | Time 0.1s | Iter Loss 0.1401 | Iter Mean Loss 0.1720\n",
      "CF Training: Epoch 0003 Iter 0064 / 0490 | Time 0.1s | Iter Loss 0.1432 | Iter Mean Loss 0.1715\n",
      "CF Training: Epoch 0003 Iter 0065 / 0490 | Time 0.1s | Iter Loss 0.1571 | Iter Mean Loss 0.1713\n",
      "CF Training: Epoch 0003 Iter 0066 / 0490 | Time 0.1s | Iter Loss 0.1506 | Iter Mean Loss 0.1710\n",
      "CF Training: Epoch 0003 Iter 0067 / 0490 | Time 0.1s | Iter Loss 0.2250 | Iter Mean Loss 0.1718\n",
      "CF Training: Epoch 0003 Iter 0068 / 0490 | Time 0.1s | Iter Loss 0.1447 | Iter Mean Loss 0.1714\n",
      "CF Training: Epoch 0003 Iter 0069 / 0490 | Time 0.1s | Iter Loss 0.1645 | Iter Mean Loss 0.1713\n",
      "CF Training: Epoch 0003 Iter 0070 / 0490 | Time 0.1s | Iter Loss 0.1790 | Iter Mean Loss 0.1714\n",
      "CF Training: Epoch 0003 Iter 0071 / 0490 | Time 0.1s | Iter Loss 0.1534 | Iter Mean Loss 0.1711\n",
      "CF Training: Epoch 0003 Iter 0072 / 0490 | Time 0.1s | Iter Loss 0.1724 | Iter Mean Loss 0.1712\n",
      "CF Training: Epoch 0003 Iter 0073 / 0490 | Time 0.1s | Iter Loss 0.1762 | Iter Mean Loss 0.1712\n",
      "CF Training: Epoch 0003 Iter 0074 / 0490 | Time 0.1s | Iter Loss 0.1597 | Iter Mean Loss 0.1711\n",
      "CF Training: Epoch 0003 Iter 0075 / 0490 | Time 0.1s | Iter Loss 0.1794 | Iter Mean Loss 0.1712\n",
      "CF Training: Epoch 0003 Iter 0076 / 0490 | Time 0.1s | Iter Loss 0.1642 | Iter Mean Loss 0.1711\n",
      "CF Training: Epoch 0003 Iter 0077 / 0490 | Time 0.1s | Iter Loss 0.1604 | Iter Mean Loss 0.1710\n",
      "CF Training: Epoch 0003 Iter 0078 / 0490 | Time 0.1s | Iter Loss 0.1794 | Iter Mean Loss 0.1711\n",
      "CF Training: Epoch 0003 Iter 0079 / 0490 | Time 0.1s | Iter Loss 0.1314 | Iter Mean Loss 0.1706\n",
      "CF Training: Epoch 0003 Iter 0080 / 0490 | Time 0.1s | Iter Loss 0.2245 | Iter Mean Loss 0.1712\n",
      "CF Training: Epoch 0003 Iter 0081 / 0490 | Time 0.1s | Iter Loss 0.1665 | Iter Mean Loss 0.1712\n",
      "CF Training: Epoch 0003 Iter 0082 / 0490 | Time 0.1s | Iter Loss 0.1355 | Iter Mean Loss 0.1707\n",
      "CF Training: Epoch 0003 Iter 0083 / 0490 | Time 0.1s | Iter Loss 0.1428 | Iter Mean Loss 0.1704\n",
      "CF Training: Epoch 0003 Iter 0084 / 0490 | Time 0.1s | Iter Loss 0.1734 | Iter Mean Loss 0.1704\n",
      "CF Training: Epoch 0003 Iter 0085 / 0490 | Time 0.1s | Iter Loss 0.1972 | Iter Mean Loss 0.1708\n",
      "CF Training: Epoch 0003 Iter 0086 / 0490 | Time 0.1s | Iter Loss 0.1808 | Iter Mean Loss 0.1709\n",
      "CF Training: Epoch 0003 Iter 0087 / 0490 | Time 0.1s | Iter Loss 0.1556 | Iter Mean Loss 0.1707\n",
      "CF Training: Epoch 0003 Iter 0088 / 0490 | Time 0.1s | Iter Loss 0.1607 | Iter Mean Loss 0.1706\n",
      "CF Training: Epoch 0003 Iter 0089 / 0490 | Time 0.1s | Iter Loss 0.1374 | Iter Mean Loss 0.1702\n",
      "CF Training: Epoch 0003 Iter 0090 / 0490 | Time 0.1s | Iter Loss 0.1966 | Iter Mean Loss 0.1705\n",
      "CF Training: Epoch 0003 Iter 0091 / 0490 | Time 0.1s | Iter Loss 0.1658 | Iter Mean Loss 0.1705\n",
      "CF Training: Epoch 0003 Iter 0092 / 0490 | Time 0.1s | Iter Loss 0.1882 | Iter Mean Loss 0.1706\n",
      "CF Training: Epoch 0003 Iter 0093 / 0490 | Time 0.1s | Iter Loss 0.1485 | Iter Mean Loss 0.1704\n",
      "CF Training: Epoch 0003 Iter 0094 / 0490 | Time 0.1s | Iter Loss 0.1243 | Iter Mean Loss 0.1699\n",
      "CF Training: Epoch 0003 Iter 0095 / 0490 | Time 0.1s | Iter Loss 0.1897 | Iter Mean Loss 0.1701\n",
      "CF Training: Epoch 0003 Iter 0096 / 0490 | Time 0.1s | Iter Loss 0.1729 | Iter Mean Loss 0.1702\n",
      "CF Training: Epoch 0003 Iter 0097 / 0490 | Time 0.1s | Iter Loss 0.1580 | Iter Mean Loss 0.1700\n",
      "CF Training: Epoch 0003 Iter 0098 / 0490 | Time 0.1s | Iter Loss 0.1752 | Iter Mean Loss 0.1701\n",
      "CF Training: Epoch 0003 Iter 0099 / 0490 | Time 0.1s | Iter Loss 0.1533 | Iter Mean Loss 0.1699\n",
      "CF Training: Epoch 0003 Iter 0100 / 0490 | Time 0.1s | Iter Loss 0.1835 | Iter Mean Loss 0.1700\n",
      "CF Training: Epoch 0003 Iter 0101 / 0490 | Time 0.1s | Iter Loss 0.1521 | Iter Mean Loss 0.1699\n",
      "CF Training: Epoch 0003 Iter 0102 / 0490 | Time 0.1s | Iter Loss 0.1907 | Iter Mean Loss 0.1701\n",
      "CF Training: Epoch 0003 Iter 0103 / 0490 | Time 0.1s | Iter Loss 0.1847 | Iter Mean Loss 0.1702\n",
      "CF Training: Epoch 0003 Iter 0104 / 0490 | Time 0.1s | Iter Loss 0.1641 | Iter Mean Loss 0.1702\n",
      "CF Training: Epoch 0003 Iter 0105 / 0490 | Time 0.1s | Iter Loss 0.1572 | Iter Mean Loss 0.1700\n",
      "CF Training: Epoch 0003 Iter 0106 / 0490 | Time 0.1s | Iter Loss 0.1608 | Iter Mean Loss 0.1699\n",
      "CF Training: Epoch 0003 Iter 0107 / 0490 | Time 0.1s | Iter Loss 0.1608 | Iter Mean Loss 0.1699\n",
      "CF Training: Epoch 0003 Iter 0108 / 0490 | Time 0.1s | Iter Loss 0.1660 | Iter Mean Loss 0.1698\n",
      "CF Training: Epoch 0003 Iter 0109 / 0490 | Time 0.1s | Iter Loss 0.1688 | Iter Mean Loss 0.1698\n",
      "CF Training: Epoch 0003 Iter 0110 / 0490 | Time 0.0s | Iter Loss 0.1209 | Iter Mean Loss 0.1694\n",
      "CF Training: Epoch 0003 Iter 0111 / 0490 | Time 0.1s | Iter Loss 0.1442 | Iter Mean Loss 0.1691\n",
      "CF Training: Epoch 0003 Iter 0112 / 0490 | Time 0.1s | Iter Loss 0.1638 | Iter Mean Loss 0.1691\n",
      "CF Training: Epoch 0003 Iter 0113 / 0490 | Time 0.1s | Iter Loss 0.1314 | Iter Mean Loss 0.1688\n",
      "CF Training: Epoch 0003 Iter 0114 / 0490 | Time 0.1s | Iter Loss 0.1436 | Iter Mean Loss 0.1685\n",
      "CF Training: Epoch 0003 Iter 0115 / 0490 | Time 0.1s | Iter Loss 0.2057 | Iter Mean Loss 0.1689\n",
      "CF Training: Epoch 0003 Iter 0116 / 0490 | Time 0.1s | Iter Loss 0.1497 | Iter Mean Loss 0.1687\n",
      "CF Training: Epoch 0003 Iter 0117 / 0490 | Time 0.1s | Iter Loss 0.1852 | Iter Mean Loss 0.1688\n",
      "CF Training: Epoch 0003 Iter 0118 / 0490 | Time 0.1s | Iter Loss 0.1260 | Iter Mean Loss 0.1685\n",
      "CF Training: Epoch 0003 Iter 0119 / 0490 | Time 0.1s | Iter Loss 0.1260 | Iter Mean Loss 0.1681\n",
      "CF Training: Epoch 0003 Iter 0120 / 0490 | Time 0.1s | Iter Loss 0.1407 | Iter Mean Loss 0.1679\n",
      "CF Training: Epoch 0003 Iter 0121 / 0490 | Time 0.1s | Iter Loss 0.1951 | Iter Mean Loss 0.1681\n",
      "CF Training: Epoch 0003 Iter 0122 / 0490 | Time 0.1s | Iter Loss 0.1742 | Iter Mean Loss 0.1682\n",
      "CF Training: Epoch 0003 Iter 0123 / 0490 | Time 0.1s | Iter Loss 0.2093 | Iter Mean Loss 0.1685\n",
      "CF Training: Epoch 0003 Iter 0124 / 0490 | Time 0.1s | Iter Loss 0.1666 | Iter Mean Loss 0.1685\n",
      "CF Training: Epoch 0003 Iter 0125 / 0490 | Time 0.1s | Iter Loss 0.1766 | Iter Mean Loss 0.1686\n",
      "CF Training: Epoch 0003 Iter 0126 / 0490 | Time 0.1s | Iter Loss 0.1538 | Iter Mean Loss 0.1684\n",
      "CF Training: Epoch 0003 Iter 0127 / 0490 | Time 0.1s | Iter Loss 0.1704 | Iter Mean Loss 0.1685\n",
      "CF Training: Epoch 0003 Iter 0128 / 0490 | Time 0.1s | Iter Loss 0.1680 | Iter Mean Loss 0.1684\n",
      "CF Training: Epoch 0003 Iter 0129 / 0490 | Time 0.1s | Iter Loss 0.1618 | Iter Mean Loss 0.1684\n",
      "CF Training: Epoch 0003 Iter 0130 / 0490 | Time 0.1s | Iter Loss 0.1760 | Iter Mean Loss 0.1685\n",
      "CF Training: Epoch 0003 Iter 0131 / 0490 | Time 0.1s | Iter Loss 0.1792 | Iter Mean Loss 0.1685\n",
      "CF Training: Epoch 0003 Iter 0132 / 0490 | Time 0.1s | Iter Loss 0.1839 | Iter Mean Loss 0.1687\n",
      "CF Training: Epoch 0003 Iter 0133 / 0490 | Time 0.1s | Iter Loss 0.1591 | Iter Mean Loss 0.1686\n",
      "CF Training: Epoch 0003 Iter 0134 / 0490 | Time 0.1s | Iter Loss 0.1843 | Iter Mean Loss 0.1687\n",
      "CF Training: Epoch 0003 Iter 0135 / 0490 | Time 0.1s | Iter Loss 0.1760 | Iter Mean Loss 0.1688\n",
      "CF Training: Epoch 0003 Iter 0136 / 0490 | Time 0.1s | Iter Loss 0.1269 | Iter Mean Loss 0.1684\n",
      "CF Training: Epoch 0003 Iter 0137 / 0490 | Time 0.1s | Iter Loss 0.1288 | Iter Mean Loss 0.1682\n",
      "CF Training: Epoch 0003 Iter 0138 / 0490 | Time 0.1s | Iter Loss 0.1760 | Iter Mean Loss 0.1682\n",
      "CF Training: Epoch 0003 Iter 0139 / 0490 | Time 0.1s | Iter Loss 0.1537 | Iter Mean Loss 0.1681\n",
      "CF Training: Epoch 0003 Iter 0140 / 0490 | Time 0.1s | Iter Loss 0.1640 | Iter Mean Loss 0.1681\n",
      "CF Training: Epoch 0003 Iter 0141 / 0490 | Time 0.1s | Iter Loss 0.1747 | Iter Mean Loss 0.1681\n",
      "CF Training: Epoch 0003 Iter 0142 / 0490 | Time 0.1s | Iter Loss 0.1306 | Iter Mean Loss 0.1679\n",
      "CF Training: Epoch 0003 Iter 0143 / 0490 | Time 0.1s | Iter Loss 0.1797 | Iter Mean Loss 0.1679\n",
      "CF Training: Epoch 0003 Iter 0144 / 0490 | Time 0.1s | Iter Loss 0.1618 | Iter Mean Loss 0.1679\n",
      "CF Training: Epoch 0003 Iter 0145 / 0490 | Time 0.1s | Iter Loss 0.1123 | Iter Mean Loss 0.1675\n",
      "CF Training: Epoch 0003 Iter 0146 / 0490 | Time 0.1s | Iter Loss 0.1730 | Iter Mean Loss 0.1676\n",
      "CF Training: Epoch 0003 Iter 0147 / 0490 | Time 0.1s | Iter Loss 0.1590 | Iter Mean Loss 0.1675\n",
      "CF Training: Epoch 0003 Iter 0148 / 0490 | Time 0.1s | Iter Loss 0.1464 | Iter Mean Loss 0.1674\n",
      "CF Training: Epoch 0003 Iter 0149 / 0490 | Time 0.1s | Iter Loss 0.1498 | Iter Mean Loss 0.1672\n",
      "CF Training: Epoch 0003 Iter 0150 / 0490 | Time 0.1s | Iter Loss 0.1440 | Iter Mean Loss 0.1671\n",
      "CF Training: Epoch 0003 Iter 0151 / 0490 | Time 0.1s | Iter Loss 0.1547 | Iter Mean Loss 0.1670\n",
      "CF Training: Epoch 0003 Iter 0152 / 0490 | Time 0.1s | Iter Loss 0.1570 | Iter Mean Loss 0.1669\n",
      "CF Training: Epoch 0003 Iter 0153 / 0490 | Time 0.1s | Iter Loss 0.1355 | Iter Mean Loss 0.1667\n",
      "CF Training: Epoch 0003 Iter 0154 / 0490 | Time 0.1s | Iter Loss 0.1846 | Iter Mean Loss 0.1668\n",
      "CF Training: Epoch 0003 Iter 0155 / 0490 | Time 0.1s | Iter Loss 0.1418 | Iter Mean Loss 0.1667\n",
      "CF Training: Epoch 0003 Iter 0156 / 0490 | Time 0.1s | Iter Loss 0.1561 | Iter Mean Loss 0.1666\n",
      "CF Training: Epoch 0003 Iter 0157 / 0490 | Time 0.1s | Iter Loss 0.1672 | Iter Mean Loss 0.1666\n",
      "CF Training: Epoch 0003 Iter 0158 / 0490 | Time 0.1s | Iter Loss 0.2043 | Iter Mean Loss 0.1669\n",
      "CF Training: Epoch 0003 Iter 0159 / 0490 | Time 0.1s | Iter Loss 0.1517 | Iter Mean Loss 0.1668\n",
      "CF Training: Epoch 0003 Iter 0160 / 0490 | Time 0.1s | Iter Loss 0.1479 | Iter Mean Loss 0.1666\n",
      "CF Training: Epoch 0003 Iter 0161 / 0490 | Time 0.1s | Iter Loss 0.1749 | Iter Mean Loss 0.1667\n",
      "CF Training: Epoch 0003 Iter 0162 / 0490 | Time 0.1s | Iter Loss 0.1869 | Iter Mean Loss 0.1668\n",
      "CF Training: Epoch 0003 Iter 0163 / 0490 | Time 0.1s | Iter Loss 0.1377 | Iter Mean Loss 0.1666\n",
      "CF Training: Epoch 0003 Iter 0164 / 0490 | Time 0.1s | Iter Loss 0.1893 | Iter Mean Loss 0.1668\n",
      "CF Training: Epoch 0003 Iter 0165 / 0490 | Time 0.1s | Iter Loss 0.1408 | Iter Mean Loss 0.1666\n",
      "CF Training: Epoch 0003 Iter 0166 / 0490 | Time 0.1s | Iter Loss 0.1042 | Iter Mean Loss 0.1662\n",
      "CF Training: Epoch 0003 Iter 0167 / 0490 | Time 0.1s | Iter Loss 0.1396 | Iter Mean Loss 0.1661\n",
      "CF Training: Epoch 0003 Iter 0168 / 0490 | Time 0.1s | Iter Loss 0.1955 | Iter Mean Loss 0.1663\n",
      "CF Training: Epoch 0003 Iter 0169 / 0490 | Time 0.1s | Iter Loss 0.1766 | Iter Mean Loss 0.1663\n",
      "CF Training: Epoch 0003 Iter 0170 / 0490 | Time 0.0s | Iter Loss 0.0930 | Iter Mean Loss 0.1659\n",
      "CF Training: Epoch 0003 Iter 0171 / 0490 | Time 0.1s | Iter Loss 0.1982 | Iter Mean Loss 0.1661\n",
      "CF Training: Epoch 0003 Iter 0172 / 0490 | Time 0.1s | Iter Loss 0.1934 | Iter Mean Loss 0.1662\n",
      "CF Training: Epoch 0003 Iter 0173 / 0490 | Time 0.1s | Iter Loss 0.1720 | Iter Mean Loss 0.1663\n",
      "CF Training: Epoch 0003 Iter 0174 / 0490 | Time 0.1s | Iter Loss 0.1433 | Iter Mean Loss 0.1661\n",
      "CF Training: Epoch 0003 Iter 0175 / 0490 | Time 0.1s | Iter Loss 0.1530 | Iter Mean Loss 0.1661\n",
      "CF Training: Epoch 0003 Iter 0176 / 0490 | Time 0.1s | Iter Loss 0.1509 | Iter Mean Loss 0.1660\n",
      "CF Training: Epoch 0003 Iter 0177 / 0490 | Time 0.1s | Iter Loss 0.1244 | Iter Mean Loss 0.1657\n",
      "CF Training: Epoch 0003 Iter 0178 / 0490 | Time 0.1s | Iter Loss 0.1916 | Iter Mean Loss 0.1659\n",
      "CF Training: Epoch 0003 Iter 0179 / 0490 | Time 0.1s | Iter Loss 0.1486 | Iter Mean Loss 0.1658\n",
      "CF Training: Epoch 0003 Iter 0180 / 0490 | Time 0.1s | Iter Loss 0.1626 | Iter Mean Loss 0.1658\n",
      "CF Training: Epoch 0003 Iter 0181 / 0490 | Time 0.1s | Iter Loss 0.1990 | Iter Mean Loss 0.1660\n",
      "CF Training: Epoch 0003 Iter 0182 / 0490 | Time 0.1s | Iter Loss 0.1917 | Iter Mean Loss 0.1661\n",
      "CF Training: Epoch 0003 Iter 0183 / 0490 | Time 0.1s | Iter Loss 0.1350 | Iter Mean Loss 0.1659\n",
      "CF Training: Epoch 0003 Iter 0184 / 0490 | Time 0.1s | Iter Loss 0.1584 | Iter Mean Loss 0.1659\n",
      "CF Training: Epoch 0003 Iter 0185 / 0490 | Time 0.1s | Iter Loss 0.1509 | Iter Mean Loss 0.1658\n",
      "CF Training: Epoch 0003 Iter 0186 / 0490 | Time 0.1s | Iter Loss 0.1442 | Iter Mean Loss 0.1657\n",
      "CF Training: Epoch 0003 Iter 0187 / 0490 | Time 0.1s | Iter Loss 0.1682 | Iter Mean Loss 0.1657\n",
      "CF Training: Epoch 0003 Iter 0188 / 0490 | Time 0.1s | Iter Loss 0.1792 | Iter Mean Loss 0.1658\n",
      "CF Training: Epoch 0003 Iter 0189 / 0490 | Time 0.1s | Iter Loss 0.1574 | Iter Mean Loss 0.1657\n",
      "CF Training: Epoch 0003 Iter 0190 / 0490 | Time 0.1s | Iter Loss 0.1455 | Iter Mean Loss 0.1656\n",
      "CF Training: Epoch 0003 Iter 0191 / 0490 | Time 0.1s | Iter Loss 0.1748 | Iter Mean Loss 0.1657\n",
      "CF Training: Epoch 0003 Iter 0192 / 0490 | Time 0.1s | Iter Loss 0.1764 | Iter Mean Loss 0.1657\n",
      "CF Training: Epoch 0003 Iter 0193 / 0490 | Time 0.1s | Iter Loss 0.1467 | Iter Mean Loss 0.1656\n",
      "CF Training: Epoch 0003 Iter 0194 / 0490 | Time 0.1s | Iter Loss 0.1644 | Iter Mean Loss 0.1656\n",
      "CF Training: Epoch 0003 Iter 0195 / 0490 | Time 0.1s | Iter Loss 0.1302 | Iter Mean Loss 0.1654\n",
      "CF Training: Epoch 0003 Iter 0196 / 0490 | Time 0.1s | Iter Loss 0.1587 | Iter Mean Loss 0.1654\n",
      "CF Training: Epoch 0003 Iter 0197 / 0490 | Time 0.1s | Iter Loss 0.1852 | Iter Mean Loss 0.1655\n",
      "CF Training: Epoch 0003 Iter 0198 / 0490 | Time 0.1s | Iter Loss 0.1360 | Iter Mean Loss 0.1654\n",
      "CF Training: Epoch 0003 Iter 0199 / 0490 | Time 0.1s | Iter Loss 0.1324 | Iter Mean Loss 0.1652\n",
      "CF Training: Epoch 0003 Iter 0200 / 0490 | Time 0.1s | Iter Loss 0.1572 | Iter Mean Loss 0.1652\n",
      "CF Training: Epoch 0003 Iter 0201 / 0490 | Time 0.1s | Iter Loss 0.1721 | Iter Mean Loss 0.1652\n",
      "CF Training: Epoch 0003 Iter 0202 / 0490 | Time 0.1s | Iter Loss 0.1675 | Iter Mean Loss 0.1652\n",
      "CF Training: Epoch 0003 Iter 0203 / 0490 | Time 0.1s | Iter Loss 0.1551 | Iter Mean Loss 0.1652\n",
      "CF Training: Epoch 0003 Iter 0204 / 0490 | Time 0.1s | Iter Loss 0.1784 | Iter Mean Loss 0.1652\n",
      "CF Training: Epoch 0003 Iter 0205 / 0490 | Time 0.1s | Iter Loss 0.1769 | Iter Mean Loss 0.1653\n",
      "CF Training: Epoch 0003 Iter 0206 / 0490 | Time 0.1s | Iter Loss 0.1649 | Iter Mean Loss 0.1653\n",
      "CF Training: Epoch 0003 Iter 0207 / 0490 | Time 0.1s | Iter Loss 0.1705 | Iter Mean Loss 0.1653\n",
      "CF Training: Epoch 0003 Iter 0208 / 0490 | Time 0.1s | Iter Loss 0.1244 | Iter Mean Loss 0.1651\n",
      "CF Training: Epoch 0003 Iter 0209 / 0490 | Time 0.1s | Iter Loss 0.1488 | Iter Mean Loss 0.1650\n",
      "CF Training: Epoch 0003 Iter 0210 / 0490 | Time 0.1s | Iter Loss 0.2097 | Iter Mean Loss 0.1652\n",
      "CF Training: Epoch 0003 Iter 0211 / 0490 | Time 0.1s | Iter Loss 0.1977 | Iter Mean Loss 0.1654\n",
      "CF Training: Epoch 0003 Iter 0212 / 0490 | Time 0.1s | Iter Loss 0.1740 | Iter Mean Loss 0.1654\n",
      "CF Training: Epoch 0003 Iter 0213 / 0490 | Time 0.1s | Iter Loss 0.1347 | Iter Mean Loss 0.1653\n",
      "CF Training: Epoch 0003 Iter 0214 / 0490 | Time 0.1s | Iter Loss 0.1796 | Iter Mean Loss 0.1654\n",
      "CF Training: Epoch 0003 Iter 0215 / 0490 | Time 0.1s | Iter Loss 0.1229 | Iter Mean Loss 0.1652\n",
      "CF Training: Epoch 0003 Iter 0216 / 0490 | Time 0.1s | Iter Loss 0.1595 | Iter Mean Loss 0.1651\n",
      "CF Training: Epoch 0003 Iter 0217 / 0490 | Time 0.1s | Iter Loss 0.1347 | Iter Mean Loss 0.1650\n",
      "CF Training: Epoch 0003 Iter 0218 / 0490 | Time 0.1s | Iter Loss 0.1441 | Iter Mean Loss 0.1649\n",
      "CF Training: Epoch 0003 Iter 0219 / 0490 | Time 0.1s | Iter Loss 0.1652 | Iter Mean Loss 0.1649\n",
      "CF Training: Epoch 0003 Iter 0220 / 0490 | Time 0.1s | Iter Loss 0.1594 | Iter Mean Loss 0.1649\n",
      "CF Training: Epoch 0003 Iter 0221 / 0490 | Time 0.1s | Iter Loss 0.1455 | Iter Mean Loss 0.1648\n",
      "CF Training: Epoch 0003 Iter 0222 / 0490 | Time 0.1s | Iter Loss 0.1848 | Iter Mean Loss 0.1649\n",
      "CF Training: Epoch 0003 Iter 0223 / 0490 | Time 0.1s | Iter Loss 0.1532 | Iter Mean Loss 0.1648\n",
      "CF Training: Epoch 0003 Iter 0224 / 0490 | Time 0.1s | Iter Loss 0.1540 | Iter Mean Loss 0.1648\n",
      "CF Training: Epoch 0003 Iter 0225 / 0490 | Time 0.1s | Iter Loss 0.1635 | Iter Mean Loss 0.1648\n",
      "CF Training: Epoch 0003 Iter 0226 / 0490 | Time 0.1s | Iter Loss 0.1212 | Iter Mean Loss 0.1646\n",
      "CF Training: Epoch 0003 Iter 0227 / 0490 | Time 0.1s | Iter Loss 0.1499 | Iter Mean Loss 0.1645\n",
      "CF Training: Epoch 0003 Iter 0228 / 0490 | Time 0.1s | Iter Loss 0.1530 | Iter Mean Loss 0.1645\n",
      "CF Training: Epoch 0003 Iter 0229 / 0490 | Time 0.1s | Iter Loss 0.1271 | Iter Mean Loss 0.1643\n",
      "CF Training: Epoch 0003 Iter 0230 / 0490 | Time 0.1s | Iter Loss 0.1601 | Iter Mean Loss 0.1643\n",
      "CF Training: Epoch 0003 Iter 0231 / 0490 | Time 0.1s | Iter Loss 0.1343 | Iter Mean Loss 0.1641\n",
      "CF Training: Epoch 0003 Iter 0232 / 0490 | Time 0.1s | Iter Loss 0.1471 | Iter Mean Loss 0.1641\n",
      "CF Training: Epoch 0003 Iter 0233 / 0490 | Time 0.1s | Iter Loss 0.1404 | Iter Mean Loss 0.1640\n",
      "CF Training: Epoch 0003 Iter 0234 / 0490 | Time 0.1s | Iter Loss 0.1572 | Iter Mean Loss 0.1639\n",
      "CF Training: Epoch 0003 Iter 0235 / 0490 | Time 0.1s | Iter Loss 0.1254 | Iter Mean Loss 0.1638\n",
      "CF Training: Epoch 0003 Iter 0236 / 0490 | Time 0.1s | Iter Loss 0.1622 | Iter Mean Loss 0.1638\n",
      "CF Training: Epoch 0003 Iter 0237 / 0490 | Time 0.1s | Iter Loss 0.1626 | Iter Mean Loss 0.1638\n",
      "CF Training: Epoch 0003 Iter 0238 / 0490 | Time 0.1s | Iter Loss 0.1727 | Iter Mean Loss 0.1638\n",
      "CF Training: Epoch 0003 Iter 0239 / 0490 | Time 0.1s | Iter Loss 0.1755 | Iter Mean Loss 0.1639\n",
      "CF Training: Epoch 0003 Iter 0240 / 0490 | Time 0.1s | Iter Loss 0.1480 | Iter Mean Loss 0.1638\n",
      "CF Training: Epoch 0003 Iter 0241 / 0490 | Time 0.1s | Iter Loss 0.1692 | Iter Mean Loss 0.1638\n",
      "CF Training: Epoch 0003 Iter 0242 / 0490 | Time 0.1s | Iter Loss 0.1461 | Iter Mean Loss 0.1637\n",
      "CF Training: Epoch 0003 Iter 0243 / 0490 | Time 0.1s | Iter Loss 0.1822 | Iter Mean Loss 0.1638\n",
      "CF Training: Epoch 0003 Iter 0244 / 0490 | Time 0.0s | Iter Loss 0.2026 | Iter Mean Loss 0.1640\n",
      "CF Training: Epoch 0003 Iter 0245 / 0490 | Time 0.1s | Iter Loss 0.1588 | Iter Mean Loss 0.1640\n",
      "CF Training: Epoch 0003 Iter 0246 / 0490 | Time 0.1s | Iter Loss 0.1514 | Iter Mean Loss 0.1639\n",
      "CF Training: Epoch 0003 Iter 0247 / 0490 | Time 0.1s | Iter Loss 0.1396 | Iter Mean Loss 0.1638\n",
      "CF Training: Epoch 0003 Iter 0248 / 0490 | Time 0.1s | Iter Loss 0.1536 | Iter Mean Loss 0.1638\n",
      "CF Training: Epoch 0003 Iter 0249 / 0490 | Time 0.1s | Iter Loss 0.1857 | Iter Mean Loss 0.1638\n",
      "CF Training: Epoch 0003 Iter 0250 / 0490 | Time 0.1s | Iter Loss 0.1534 | Iter Mean Loss 0.1638\n",
      "CF Training: Epoch 0003 Iter 0251 / 0490 | Time 0.1s | Iter Loss 0.1245 | Iter Mean Loss 0.1637\n",
      "CF Training: Epoch 0003 Iter 0252 / 0490 | Time 0.1s | Iter Loss 0.1344 | Iter Mean Loss 0.1635\n",
      "CF Training: Epoch 0003 Iter 0253 / 0490 | Time 0.1s | Iter Loss 0.1375 | Iter Mean Loss 0.1634\n",
      "CF Training: Epoch 0003 Iter 0254 / 0490 | Time 0.1s | Iter Loss 0.1699 | Iter Mean Loss 0.1635\n",
      "CF Training: Epoch 0003 Iter 0255 / 0490 | Time 0.1s | Iter Loss 0.1480 | Iter Mean Loss 0.1634\n",
      "CF Training: Epoch 0003 Iter 0256 / 0490 | Time 0.1s | Iter Loss 0.1385 | Iter Mean Loss 0.1633\n",
      "CF Training: Epoch 0003 Iter 0257 / 0490 | Time 0.1s | Iter Loss 0.1500 | Iter Mean Loss 0.1632\n",
      "CF Training: Epoch 0003 Iter 0258 / 0490 | Time 0.1s | Iter Loss 0.1561 | Iter Mean Loss 0.1632\n",
      "CF Training: Epoch 0003 Iter 0259 / 0490 | Time 0.1s | Iter Loss 0.1321 | Iter Mean Loss 0.1631\n",
      "CF Training: Epoch 0003 Iter 0260 / 0490 | Time 0.1s | Iter Loss 0.1547 | Iter Mean Loss 0.1631\n",
      "CF Training: Epoch 0003 Iter 0261 / 0490 | Time 0.1s | Iter Loss 0.1416 | Iter Mean Loss 0.1630\n",
      "CF Training: Epoch 0003 Iter 0262 / 0490 | Time 0.1s | Iter Loss 0.1584 | Iter Mean Loss 0.1630\n",
      "CF Training: Epoch 0003 Iter 0263 / 0490 | Time 0.1s | Iter Loss 0.1336 | Iter Mean Loss 0.1629\n",
      "CF Training: Epoch 0003 Iter 0264 / 0490 | Time 0.1s | Iter Loss 0.1481 | Iter Mean Loss 0.1628\n",
      "CF Training: Epoch 0003 Iter 0265 / 0490 | Time 0.1s | Iter Loss 0.1232 | Iter Mean Loss 0.1627\n",
      "CF Training: Epoch 0003 Iter 0266 / 0490 | Time 0.1s | Iter Loss 0.1695 | Iter Mean Loss 0.1627\n",
      "CF Training: Epoch 0003 Iter 0267 / 0490 | Time 0.1s | Iter Loss 0.1520 | Iter Mean Loss 0.1626\n",
      "CF Training: Epoch 0003 Iter 0268 / 0490 | Time 0.1s | Iter Loss 0.1435 | Iter Mean Loss 0.1626\n",
      "CF Training: Epoch 0003 Iter 0269 / 0490 | Time 0.1s | Iter Loss 0.1388 | Iter Mean Loss 0.1625\n",
      "CF Training: Epoch 0003 Iter 0270 / 0490 | Time 0.0s | Iter Loss 0.1707 | Iter Mean Loss 0.1625\n",
      "CF Training: Epoch 0003 Iter 0271 / 0490 | Time 0.1s | Iter Loss 0.1601 | Iter Mean Loss 0.1625\n",
      "CF Training: Epoch 0003 Iter 0272 / 0490 | Time 0.1s | Iter Loss 0.1578 | Iter Mean Loss 0.1625\n",
      "CF Training: Epoch 0003 Iter 0273 / 0490 | Time 0.1s | Iter Loss 0.1629 | Iter Mean Loss 0.1625\n",
      "CF Training: Epoch 0003 Iter 0274 / 0490 | Time 0.1s | Iter Loss 0.1585 | Iter Mean Loss 0.1625\n",
      "CF Training: Epoch 0003 Iter 0275 / 0490 | Time 0.1s | Iter Loss 0.1414 | Iter Mean Loss 0.1624\n",
      "CF Training: Epoch 0003 Iter 0276 / 0490 | Time 0.1s | Iter Loss 0.1566 | Iter Mean Loss 0.1624\n",
      "CF Training: Epoch 0003 Iter 0277 / 0490 | Time 0.1s | Iter Loss 0.1090 | Iter Mean Loss 0.1622\n",
      "CF Training: Epoch 0003 Iter 0278 / 0490 | Time 0.1s | Iter Loss 0.1101 | Iter Mean Loss 0.1620\n",
      "CF Training: Epoch 0003 Iter 0279 / 0490 | Time 0.1s | Iter Loss 0.1742 | Iter Mean Loss 0.1620\n",
      "CF Training: Epoch 0003 Iter 0280 / 0490 | Time 0.1s | Iter Loss 0.1510 | Iter Mean Loss 0.1620\n",
      "CF Training: Epoch 0003 Iter 0281 / 0490 | Time 0.1s | Iter Loss 0.1346 | Iter Mean Loss 0.1619\n",
      "CF Training: Epoch 0003 Iter 0282 / 0490 | Time 0.1s | Iter Loss 0.1619 | Iter Mean Loss 0.1619\n",
      "CF Training: Epoch 0003 Iter 0283 / 0490 | Time 0.1s | Iter Loss 0.1448 | Iter Mean Loss 0.1618\n",
      "CF Training: Epoch 0003 Iter 0284 / 0490 | Time 0.1s | Iter Loss 0.1506 | Iter Mean Loss 0.1618\n",
      "CF Training: Epoch 0003 Iter 0285 / 0490 | Time 0.1s | Iter Loss 0.1529 | Iter Mean Loss 0.1618\n",
      "CF Training: Epoch 0003 Iter 0286 / 0490 | Time 0.1s | Iter Loss 0.1990 | Iter Mean Loss 0.1619\n",
      "CF Training: Epoch 0003 Iter 0287 / 0490 | Time 0.1s | Iter Loss 0.1483 | Iter Mean Loss 0.1618\n",
      "CF Training: Epoch 0003 Iter 0288 / 0490 | Time 0.1s | Iter Loss 0.1162 | Iter Mean Loss 0.1617\n",
      "CF Training: Epoch 0003 Iter 0289 / 0490 | Time 0.1s | Iter Loss 0.1605 | Iter Mean Loss 0.1617\n",
      "CF Training: Epoch 0003 Iter 0290 / 0490 | Time 0.1s | Iter Loss 0.1162 | Iter Mean Loss 0.1615\n",
      "CF Training: Epoch 0003 Iter 0291 / 0490 | Time 0.1s | Iter Loss 0.1235 | Iter Mean Loss 0.1614\n",
      "CF Training: Epoch 0003 Iter 0292 / 0490 | Time 0.1s | Iter Loss 0.1500 | Iter Mean Loss 0.1614\n",
      "CF Training: Epoch 0003 Iter 0293 / 0490 | Time 0.1s | Iter Loss 0.1359 | Iter Mean Loss 0.1613\n",
      "CF Training: Epoch 0003 Iter 0294 / 0490 | Time 0.1s | Iter Loss 0.1369 | Iter Mean Loss 0.1612\n",
      "CF Training: Epoch 0003 Iter 0295 / 0490 | Time 0.1s | Iter Loss 0.1582 | Iter Mean Loss 0.1612\n",
      "CF Training: Epoch 0003 Iter 0296 / 0490 | Time 0.1s | Iter Loss 0.1250 | Iter Mean Loss 0.1611\n",
      "CF Training: Epoch 0003 Iter 0297 / 0490 | Time 0.1s | Iter Loss 0.1586 | Iter Mean Loss 0.1611\n",
      "CF Training: Epoch 0003 Iter 0298 / 0490 | Time 0.1s | Iter Loss 0.1622 | Iter Mean Loss 0.1611\n",
      "CF Training: Epoch 0003 Iter 0299 / 0490 | Time 0.1s | Iter Loss 0.1535 | Iter Mean Loss 0.1610\n",
      "CF Training: Epoch 0003 Iter 0300 / 0490 | Time 0.1s | Iter Loss 0.1578 | Iter Mean Loss 0.1610\n",
      "CF Training: Epoch 0003 Iter 0301 / 0490 | Time 0.1s | Iter Loss 0.1734 | Iter Mean Loss 0.1611\n",
      "CF Training: Epoch 0003 Iter 0302 / 0490 | Time 0.1s | Iter Loss 0.1471 | Iter Mean Loss 0.1610\n",
      "CF Training: Epoch 0003 Iter 0303 / 0490 | Time 0.1s | Iter Loss 0.1665 | Iter Mean Loss 0.1610\n",
      "CF Training: Epoch 0003 Iter 0304 / 0490 | Time 0.1s | Iter Loss 0.1576 | Iter Mean Loss 0.1610\n",
      "CF Training: Epoch 0003 Iter 0305 / 0490 | Time 0.1s | Iter Loss 0.1716 | Iter Mean Loss 0.1611\n",
      "CF Training: Epoch 0003 Iter 0306 / 0490 | Time 0.1s | Iter Loss 0.2071 | Iter Mean Loss 0.1612\n",
      "CF Training: Epoch 0003 Iter 0307 / 0490 | Time 0.1s | Iter Loss 0.1321 | Iter Mean Loss 0.1611\n",
      "CF Training: Epoch 0003 Iter 0308 / 0490 | Time 0.1s | Iter Loss 0.1513 | Iter Mean Loss 0.1611\n",
      "CF Training: Epoch 0003 Iter 0309 / 0490 | Time 0.1s | Iter Loss 0.1516 | Iter Mean Loss 0.1610\n",
      "CF Training: Epoch 0003 Iter 0310 / 0490 | Time 0.1s | Iter Loss 0.1269 | Iter Mean Loss 0.1609\n",
      "CF Training: Epoch 0003 Iter 0311 / 0490 | Time 0.0s | Iter Loss 0.1594 | Iter Mean Loss 0.1609\n",
      "CF Training: Epoch 0003 Iter 0312 / 0490 | Time 0.1s | Iter Loss 0.1425 | Iter Mean Loss 0.1609\n",
      "CF Training: Epoch 0003 Iter 0313 / 0490 | Time 0.1s | Iter Loss 0.2270 | Iter Mean Loss 0.1611\n",
      "CF Training: Epoch 0003 Iter 0314 / 0490 | Time 0.1s | Iter Loss 0.1995 | Iter Mean Loss 0.1612\n",
      "CF Training: Epoch 0003 Iter 0315 / 0490 | Time 0.1s | Iter Loss 0.1226 | Iter Mean Loss 0.1611\n",
      "CF Training: Epoch 0003 Iter 0316 / 0490 | Time 0.1s | Iter Loss 0.1392 | Iter Mean Loss 0.1610\n",
      "CF Training: Epoch 0003 Iter 0317 / 0490 | Time 0.1s | Iter Loss 0.1627 | Iter Mean Loss 0.1610\n",
      "CF Training: Epoch 0003 Iter 0318 / 0490 | Time 0.1s | Iter Loss 0.1477 | Iter Mean Loss 0.1610\n",
      "CF Training: Epoch 0003 Iter 0319 / 0490 | Time 0.1s | Iter Loss 0.1495 | Iter Mean Loss 0.1609\n",
      "CF Training: Epoch 0003 Iter 0320 / 0490 | Time 0.1s | Iter Loss 0.1368 | Iter Mean Loss 0.1609\n",
      "CF Training: Epoch 0003 Iter 0321 / 0490 | Time 0.1s | Iter Loss 0.1330 | Iter Mean Loss 0.1608\n",
      "CF Training: Epoch 0003 Iter 0322 / 0490 | Time 0.1s | Iter Loss 0.2198 | Iter Mean Loss 0.1610\n",
      "CF Training: Epoch 0003 Iter 0323 / 0490 | Time 0.1s | Iter Loss 0.1403 | Iter Mean Loss 0.1609\n",
      "CF Training: Epoch 0003 Iter 0324 / 0490 | Time 0.1s | Iter Loss 0.1736 | Iter Mean Loss 0.1609\n",
      "CF Training: Epoch 0003 Iter 0325 / 0490 | Time 0.1s | Iter Loss 0.1356 | Iter Mean Loss 0.1609\n",
      "CF Training: Epoch 0003 Iter 0326 / 0490 | Time 0.1s | Iter Loss 0.1399 | Iter Mean Loss 0.1608\n",
      "CF Training: Epoch 0003 Iter 0327 / 0490 | Time 0.0s | Iter Loss 0.1803 | Iter Mean Loss 0.1609\n",
      "CF Training: Epoch 0003 Iter 0328 / 0490 | Time 0.1s | Iter Loss 0.1718 | Iter Mean Loss 0.1609\n",
      "CF Training: Epoch 0003 Iter 0329 / 0490 | Time 0.1s | Iter Loss 0.1465 | Iter Mean Loss 0.1608\n",
      "CF Training: Epoch 0003 Iter 0330 / 0490 | Time 0.1s | Iter Loss 0.1408 | Iter Mean Loss 0.1608\n",
      "CF Training: Epoch 0003 Iter 0331 / 0490 | Time 0.1s | Iter Loss 0.1264 | Iter Mean Loss 0.1607\n",
      "CF Training: Epoch 0003 Iter 0332 / 0490 | Time 0.1s | Iter Loss 0.1196 | Iter Mean Loss 0.1606\n",
      "CF Training: Epoch 0003 Iter 0333 / 0490 | Time 0.1s | Iter Loss 0.1249 | Iter Mean Loss 0.1605\n",
      "CF Training: Epoch 0003 Iter 0334 / 0490 | Time 0.1s | Iter Loss 0.1646 | Iter Mean Loss 0.1605\n",
      "CF Training: Epoch 0003 Iter 0335 / 0490 | Time 0.1s | Iter Loss 0.1663 | Iter Mean Loss 0.1605\n",
      "CF Training: Epoch 0003 Iter 0336 / 0490 | Time 0.1s | Iter Loss 0.1553 | Iter Mean Loss 0.1605\n",
      "CF Training: Epoch 0003 Iter 0337 / 0490 | Time 0.1s | Iter Loss 0.1385 | Iter Mean Loss 0.1604\n",
      "CF Training: Epoch 0003 Iter 0338 / 0490 | Time 0.1s | Iter Loss 0.1586 | Iter Mean Loss 0.1604\n",
      "CF Training: Epoch 0003 Iter 0339 / 0490 | Time 0.1s | Iter Loss 0.1793 | Iter Mean Loss 0.1605\n",
      "CF Training: Epoch 0003 Iter 0340 / 0490 | Time 0.1s | Iter Loss 0.1545 | Iter Mean Loss 0.1604\n",
      "CF Training: Epoch 0003 Iter 0341 / 0490 | Time 0.1s | Iter Loss 0.1232 | Iter Mean Loss 0.1603\n",
      "CF Training: Epoch 0003 Iter 0342 / 0490 | Time 0.1s | Iter Loss 0.1442 | Iter Mean Loss 0.1603\n",
      "CF Training: Epoch 0003 Iter 0343 / 0490 | Time 0.1s | Iter Loss 0.1580 | Iter Mean Loss 0.1603\n",
      "CF Training: Epoch 0003 Iter 0344 / 0490 | Time 0.1s | Iter Loss 0.1493 | Iter Mean Loss 0.1602\n",
      "CF Training: Epoch 0003 Iter 0345 / 0490 | Time 0.1s | Iter Loss 0.1564 | Iter Mean Loss 0.1602\n",
      "CF Training: Epoch 0003 Iter 0346 / 0490 | Time 0.1s | Iter Loss 0.1281 | Iter Mean Loss 0.1601\n",
      "CF Training: Epoch 0003 Iter 0347 / 0490 | Time 0.1s | Iter Loss 0.1423 | Iter Mean Loss 0.1601\n",
      "CF Training: Epoch 0003 Iter 0348 / 0490 | Time 0.1s | Iter Loss 0.1559 | Iter Mean Loss 0.1601\n",
      "CF Training: Epoch 0003 Iter 0349 / 0490 | Time 0.1s | Iter Loss 0.1674 | Iter Mean Loss 0.1601\n",
      "CF Training: Epoch 0003 Iter 0350 / 0490 | Time 0.1s | Iter Loss 0.1632 | Iter Mean Loss 0.1601\n",
      "CF Training: Epoch 0003 Iter 0351 / 0490 | Time 0.1s | Iter Loss 0.1612 | Iter Mean Loss 0.1601\n",
      "CF Training: Epoch 0003 Iter 0352 / 0490 | Time 0.1s | Iter Loss 0.1244 | Iter Mean Loss 0.1600\n",
      "CF Training: Epoch 0003 Iter 0353 / 0490 | Time 0.1s | Iter Loss 0.1444 | Iter Mean Loss 0.1600\n",
      "CF Training: Epoch 0003 Iter 0354 / 0490 | Time 0.1s | Iter Loss 0.0934 | Iter Mean Loss 0.1598\n",
      "CF Training: Epoch 0003 Iter 0355 / 0490 | Time 0.1s | Iter Loss 0.1545 | Iter Mean Loss 0.1598\n",
      "CF Training: Epoch 0003 Iter 0356 / 0490 | Time 0.1s | Iter Loss 0.1566 | Iter Mean Loss 0.1597\n",
      "CF Training: Epoch 0003 Iter 0357 / 0490 | Time 0.1s | Iter Loss 0.1283 | Iter Mean Loss 0.1597\n",
      "CF Training: Epoch 0003 Iter 0358 / 0490 | Time 0.1s | Iter Loss 0.1222 | Iter Mean Loss 0.1596\n",
      "CF Training: Epoch 0003 Iter 0359 / 0490 | Time 0.1s | Iter Loss 0.1263 | Iter Mean Loss 0.1595\n",
      "CF Training: Epoch 0003 Iter 0360 / 0490 | Time 0.0s | Iter Loss 0.1762 | Iter Mean Loss 0.1595\n",
      "CF Training: Epoch 0003 Iter 0361 / 0490 | Time 0.1s | Iter Loss 0.1896 | Iter Mean Loss 0.1596\n",
      "CF Training: Epoch 0003 Iter 0362 / 0490 | Time 0.1s | Iter Loss 0.1231 | Iter Mean Loss 0.1595\n",
      "CF Training: Epoch 0003 Iter 0363 / 0490 | Time 0.1s | Iter Loss 0.1170 | Iter Mean Loss 0.1594\n",
      "CF Training: Epoch 0003 Iter 0364 / 0490 | Time 0.1s | Iter Loss 0.1459 | Iter Mean Loss 0.1593\n",
      "CF Training: Epoch 0003 Iter 0365 / 0490 | Time 0.1s | Iter Loss 0.1542 | Iter Mean Loss 0.1593\n",
      "CF Training: Epoch 0003 Iter 0366 / 0490 | Time 0.1s | Iter Loss 0.1806 | Iter Mean Loss 0.1594\n",
      "CF Training: Epoch 0003 Iter 0367 / 0490 | Time 0.1s | Iter Loss 0.1682 | Iter Mean Loss 0.1594\n",
      "CF Training: Epoch 0003 Iter 0368 / 0490 | Time 0.1s | Iter Loss 0.1198 | Iter Mean Loss 0.1593\n",
      "CF Training: Epoch 0003 Iter 0369 / 0490 | Time 0.1s | Iter Loss 0.1807 | Iter Mean Loss 0.1594\n",
      "CF Training: Epoch 0003 Iter 0370 / 0490 | Time 0.1s | Iter Loss 0.1472 | Iter Mean Loss 0.1593\n",
      "CF Training: Epoch 0003 Iter 0371 / 0490 | Time 0.1s | Iter Loss 0.1217 | Iter Mean Loss 0.1592\n",
      "CF Training: Epoch 0003 Iter 0372 / 0490 | Time 0.1s | Iter Loss 0.1740 | Iter Mean Loss 0.1593\n",
      "CF Training: Epoch 0003 Iter 0373 / 0490 | Time 0.1s | Iter Loss 0.1451 | Iter Mean Loss 0.1592\n",
      "CF Training: Epoch 0003 Iter 0374 / 0490 | Time 0.1s | Iter Loss 0.1734 | Iter Mean Loss 0.1593\n",
      "CF Training: Epoch 0003 Iter 0375 / 0490 | Time 0.1s | Iter Loss 0.1409 | Iter Mean Loss 0.1592\n",
      "CF Training: Epoch 0003 Iter 0376 / 0490 | Time 0.1s | Iter Loss 0.1212 | Iter Mean Loss 0.1591\n",
      "CF Training: Epoch 0003 Iter 0377 / 0490 | Time 0.1s | Iter Loss 0.1280 | Iter Mean Loss 0.1590\n",
      "CF Training: Epoch 0003 Iter 0378 / 0490 | Time 0.1s | Iter Loss 0.1400 | Iter Mean Loss 0.1590\n",
      "CF Training: Epoch 0003 Iter 0379 / 0490 | Time 0.1s | Iter Loss 0.1413 | Iter Mean Loss 0.1589\n",
      "CF Training: Epoch 0003 Iter 0380 / 0490 | Time 0.1s | Iter Loss 0.1510 | Iter Mean Loss 0.1589\n",
      "CF Training: Epoch 0003 Iter 0381 / 0490 | Time 0.1s | Iter Loss 0.1388 | Iter Mean Loss 0.1589\n",
      "CF Training: Epoch 0003 Iter 0382 / 0490 | Time 0.1s | Iter Loss 0.1355 | Iter Mean Loss 0.1588\n",
      "CF Training: Epoch 0003 Iter 0383 / 0490 | Time 0.1s | Iter Loss 0.1296 | Iter Mean Loss 0.1587\n",
      "CF Training: Epoch 0003 Iter 0384 / 0490 | Time 0.1s | Iter Loss 0.1553 | Iter Mean Loss 0.1587\n",
      "CF Training: Epoch 0003 Iter 0385 / 0490 | Time 0.0s | Iter Loss 0.1661 | Iter Mean Loss 0.1587\n",
      "CF Training: Epoch 0003 Iter 0386 / 0490 | Time 0.1s | Iter Loss 0.1212 | Iter Mean Loss 0.1586\n",
      "CF Training: Epoch 0003 Iter 0387 / 0490 | Time 0.1s | Iter Loss 0.1974 | Iter Mean Loss 0.1587\n",
      "CF Training: Epoch 0003 Iter 0388 / 0490 | Time 0.1s | Iter Loss 0.1146 | Iter Mean Loss 0.1586\n",
      "CF Training: Epoch 0003 Iter 0389 / 0490 | Time 0.1s | Iter Loss 0.1512 | Iter Mean Loss 0.1586\n",
      "CF Training: Epoch 0003 Iter 0390 / 0490 | Time 0.1s | Iter Loss 0.1293 | Iter Mean Loss 0.1585\n",
      "CF Training: Epoch 0003 Iter 0391 / 0490 | Time 0.1s | Iter Loss 0.1478 | Iter Mean Loss 0.1585\n",
      "CF Training: Epoch 0003 Iter 0392 / 0490 | Time 0.1s | Iter Loss 0.1318 | Iter Mean Loss 0.1584\n",
      "CF Training: Epoch 0003 Iter 0393 / 0490 | Time 0.1s | Iter Loss 0.1438 | Iter Mean Loss 0.1584\n",
      "CF Training: Epoch 0003 Iter 0394 / 0490 | Time 0.1s | Iter Loss 0.1458 | Iter Mean Loss 0.1584\n",
      "CF Training: Epoch 0003 Iter 0395 / 0490 | Time 0.1s | Iter Loss 0.1016 | Iter Mean Loss 0.1582\n",
      "CF Training: Epoch 0003 Iter 0396 / 0490 | Time 0.1s | Iter Loss 0.1415 | Iter Mean Loss 0.1582\n",
      "CF Training: Epoch 0003 Iter 0397 / 0490 | Time 0.1s | Iter Loss 0.1410 | Iter Mean Loss 0.1581\n",
      "CF Training: Epoch 0003 Iter 0398 / 0490 | Time 0.1s | Iter Loss 0.1305 | Iter Mean Loss 0.1581\n",
      "CF Training: Epoch 0003 Iter 0399 / 0490 | Time 0.1s | Iter Loss 0.1554 | Iter Mean Loss 0.1581\n",
      "CF Training: Epoch 0003 Iter 0400 / 0490 | Time 0.1s | Iter Loss 0.1372 | Iter Mean Loss 0.1580\n",
      "CF Training: Epoch 0003 Iter 0401 / 0490 | Time 0.1s | Iter Loss 0.1554 | Iter Mean Loss 0.1580\n",
      "CF Training: Epoch 0003 Iter 0402 / 0490 | Time 0.1s | Iter Loss 0.1499 | Iter Mean Loss 0.1580\n",
      "CF Training: Epoch 0003 Iter 0403 / 0490 | Time 0.1s | Iter Loss 0.1533 | Iter Mean Loss 0.1580\n",
      "CF Training: Epoch 0003 Iter 0404 / 0490 | Time 0.1s | Iter Loss 0.1386 | Iter Mean Loss 0.1579\n",
      "CF Training: Epoch 0003 Iter 0405 / 0490 | Time 0.1s | Iter Loss 0.1469 | Iter Mean Loss 0.1579\n",
      "CF Training: Epoch 0003 Iter 0406 / 0490 | Time 0.1s | Iter Loss 0.1743 | Iter Mean Loss 0.1579\n",
      "CF Training: Epoch 0003 Iter 0407 / 0490 | Time 0.1s | Iter Loss 0.1271 | Iter Mean Loss 0.1579\n",
      "CF Training: Epoch 0003 Iter 0408 / 0490 | Time 0.1s | Iter Loss 0.1250 | Iter Mean Loss 0.1578\n",
      "CF Training: Epoch 0003 Iter 0409 / 0490 | Time 0.1s | Iter Loss 0.1348 | Iter Mean Loss 0.1577\n",
      "CF Training: Epoch 0003 Iter 0410 / 0490 | Time 0.1s | Iter Loss 0.1679 | Iter Mean Loss 0.1577\n",
      "CF Training: Epoch 0003 Iter 0411 / 0490 | Time 0.1s | Iter Loss 0.1390 | Iter Mean Loss 0.1577\n",
      "CF Training: Epoch 0003 Iter 0412 / 0490 | Time 0.1s | Iter Loss 0.1351 | Iter Mean Loss 0.1576\n",
      "CF Training: Epoch 0003 Iter 0413 / 0490 | Time 0.1s | Iter Loss 0.1541 | Iter Mean Loss 0.1576\n",
      "CF Training: Epoch 0003 Iter 0414 / 0490 | Time 0.1s | Iter Loss 0.1268 | Iter Mean Loss 0.1576\n",
      "CF Training: Epoch 0003 Iter 0415 / 0490 | Time 0.1s | Iter Loss 0.1948 | Iter Mean Loss 0.1576\n",
      "CF Training: Epoch 0003 Iter 0416 / 0490 | Time 0.1s | Iter Loss 0.1455 | Iter Mean Loss 0.1576\n",
      "CF Training: Epoch 0003 Iter 0417 / 0490 | Time 0.1s | Iter Loss 0.1690 | Iter Mean Loss 0.1576\n",
      "CF Training: Epoch 0003 Iter 0418 / 0490 | Time 0.1s | Iter Loss 0.1468 | Iter Mean Loss 0.1576\n",
      "CF Training: Epoch 0003 Iter 0419 / 0490 | Time 0.1s | Iter Loss 0.1314 | Iter Mean Loss 0.1576\n",
      "CF Training: Epoch 0003 Iter 0420 / 0490 | Time 0.1s | Iter Loss 0.1760 | Iter Mean Loss 0.1576\n",
      "CF Training: Epoch 0003 Iter 0421 / 0490 | Time 0.1s | Iter Loss 0.1629 | Iter Mean Loss 0.1576\n",
      "CF Training: Epoch 0003 Iter 0422 / 0490 | Time 0.1s | Iter Loss 0.1231 | Iter Mean Loss 0.1575\n",
      "CF Training: Epoch 0003 Iter 0423 / 0490 | Time 0.1s | Iter Loss 0.1347 | Iter Mean Loss 0.1575\n",
      "CF Training: Epoch 0003 Iter 0424 / 0490 | Time 0.1s | Iter Loss 0.1498 | Iter Mean Loss 0.1575\n",
      "CF Training: Epoch 0003 Iter 0425 / 0490 | Time 0.1s | Iter Loss 0.1723 | Iter Mean Loss 0.1575\n",
      "CF Training: Epoch 0003 Iter 0426 / 0490 | Time 0.1s | Iter Loss 0.1408 | Iter Mean Loss 0.1575\n",
      "CF Training: Epoch 0003 Iter 0427 / 0490 | Time 0.1s | Iter Loss 0.1219 | Iter Mean Loss 0.1574\n",
      "CF Training: Epoch 0003 Iter 0428 / 0490 | Time 0.1s | Iter Loss 0.1344 | Iter Mean Loss 0.1573\n",
      "CF Training: Epoch 0003 Iter 0429 / 0490 | Time 0.1s | Iter Loss 0.1482 | Iter Mean Loss 0.1573\n",
      "CF Training: Epoch 0003 Iter 0430 / 0490 | Time 0.1s | Iter Loss 0.1348 | Iter Mean Loss 0.1572\n",
      "CF Training: Epoch 0003 Iter 0431 / 0490 | Time 0.1s | Iter Loss 0.1357 | Iter Mean Loss 0.1572\n",
      "CF Training: Epoch 0003 Iter 0432 / 0490 | Time 0.1s | Iter Loss 0.1454 | Iter Mean Loss 0.1572\n",
      "CF Training: Epoch 0003 Iter 0433 / 0490 | Time 0.1s | Iter Loss 0.1479 | Iter Mean Loss 0.1571\n",
      "CF Training: Epoch 0003 Iter 0434 / 0490 | Time 0.1s | Iter Loss 0.1571 | Iter Mean Loss 0.1571\n",
      "CF Training: Epoch 0003 Iter 0435 / 0490 | Time 0.1s | Iter Loss 0.1425 | Iter Mean Loss 0.1571\n",
      "CF Training: Epoch 0003 Iter 0436 / 0490 | Time 0.1s | Iter Loss 0.1361 | Iter Mean Loss 0.1571\n",
      "CF Training: Epoch 0003 Iter 0437 / 0490 | Time 0.1s | Iter Loss 0.1511 | Iter Mean Loss 0.1571\n",
      "CF Training: Epoch 0003 Iter 0438 / 0490 | Time 0.1s | Iter Loss 0.1119 | Iter Mean Loss 0.1569\n",
      "CF Training: Epoch 0003 Iter 0439 / 0490 | Time 0.1s | Iter Loss 0.1409 | Iter Mean Loss 0.1569\n",
      "CF Training: Epoch 0003 Iter 0440 / 0490 | Time 0.1s | Iter Loss 0.1190 | Iter Mean Loss 0.1568\n",
      "CF Training: Epoch 0003 Iter 0441 / 0490 | Time 0.1s | Iter Loss 0.1203 | Iter Mean Loss 0.1567\n",
      "CF Training: Epoch 0003 Iter 0442 / 0490 | Time 0.1s | Iter Loss 0.1700 | Iter Mean Loss 0.1568\n",
      "CF Training: Epoch 0003 Iter 0443 / 0490 | Time 0.1s | Iter Loss 0.1410 | Iter Mean Loss 0.1567\n",
      "CF Training: Epoch 0003 Iter 0444 / 0490 | Time 0.1s | Iter Loss 0.1127 | Iter Mean Loss 0.1566\n",
      "CF Training: Epoch 0003 Iter 0445 / 0490 | Time 0.1s | Iter Loss 0.1242 | Iter Mean Loss 0.1566\n",
      "CF Training: Epoch 0003 Iter 0446 / 0490 | Time 0.1s | Iter Loss 0.1286 | Iter Mean Loss 0.1565\n",
      "CF Training: Epoch 0003 Iter 0447 / 0490 | Time 0.1s | Iter Loss 0.1659 | Iter Mean Loss 0.1565\n",
      "CF Training: Epoch 0003 Iter 0448 / 0490 | Time 0.1s | Iter Loss 0.1623 | Iter Mean Loss 0.1565\n",
      "CF Training: Epoch 0003 Iter 0449 / 0490 | Time 0.1s | Iter Loss 0.1276 | Iter Mean Loss 0.1565\n",
      "CF Training: Epoch 0003 Iter 0450 / 0490 | Time 0.1s | Iter Loss 0.1269 | Iter Mean Loss 0.1564\n",
      "CF Training: Epoch 0003 Iter 0451 / 0490 | Time 0.1s | Iter Loss 0.1332 | Iter Mean Loss 0.1564\n",
      "CF Training: Epoch 0003 Iter 0452 / 0490 | Time 0.1s | Iter Loss 0.1345 | Iter Mean Loss 0.1563\n",
      "CF Training: Epoch 0003 Iter 0453 / 0490 | Time 0.1s | Iter Loss 0.1718 | Iter Mean Loss 0.1563\n",
      "CF Training: Epoch 0003 Iter 0454 / 0490 | Time 0.1s | Iter Loss 0.1197 | Iter Mean Loss 0.1563\n",
      "CF Training: Epoch 0003 Iter 0455 / 0490 | Time 0.1s | Iter Loss 0.1485 | Iter Mean Loss 0.1562\n",
      "CF Training: Epoch 0003 Iter 0456 / 0490 | Time 0.1s | Iter Loss 0.1863 | Iter Mean Loss 0.1563\n",
      "CF Training: Epoch 0003 Iter 0457 / 0490 | Time 0.1s | Iter Loss 0.1358 | Iter Mean Loss 0.1563\n",
      "CF Training: Epoch 0003 Iter 0458 / 0490 | Time 0.1s | Iter Loss 0.1600 | Iter Mean Loss 0.1563\n",
      "CF Training: Epoch 0003 Iter 0459 / 0490 | Time 0.1s | Iter Loss 0.1213 | Iter Mean Loss 0.1562\n",
      "CF Training: Epoch 0003 Iter 0460 / 0490 | Time 0.1s | Iter Loss 0.1261 | Iter Mean Loss 0.1561\n",
      "CF Training: Epoch 0003 Iter 0461 / 0490 | Time 0.1s | Iter Loss 0.1412 | Iter Mean Loss 0.1561\n",
      "CF Training: Epoch 0003 Iter 0462 / 0490 | Time 0.1s | Iter Loss 0.1397 | Iter Mean Loss 0.1561\n",
      "CF Training: Epoch 0003 Iter 0463 / 0490 | Time 0.1s | Iter Loss 0.1252 | Iter Mean Loss 0.1560\n",
      "CF Training: Epoch 0003 Iter 0464 / 0490 | Time 0.1s | Iter Loss 0.1841 | Iter Mean Loss 0.1561\n",
      "CF Training: Epoch 0003 Iter 0465 / 0490 | Time 0.1s | Iter Loss 0.1257 | Iter Mean Loss 0.1560\n",
      "CF Training: Epoch 0003 Iter 0466 / 0490 | Time 0.1s | Iter Loss 0.1652 | Iter Mean Loss 0.1560\n",
      "CF Training: Epoch 0003 Iter 0467 / 0490 | Time 0.1s | Iter Loss 0.1358 | Iter Mean Loss 0.1560\n",
      "CF Training: Epoch 0003 Iter 0468 / 0490 | Time 0.1s | Iter Loss 0.1431 | Iter Mean Loss 0.1559\n",
      "CF Training: Epoch 0003 Iter 0469 / 0490 | Time 0.1s | Iter Loss 0.1198 | Iter Mean Loss 0.1559\n",
      "CF Training: Epoch 0003 Iter 0470 / 0490 | Time 0.1s | Iter Loss 0.1370 | Iter Mean Loss 0.1558\n",
      "CF Training: Epoch 0003 Iter 0471 / 0490 | Time 0.1s | Iter Loss 0.1157 | Iter Mean Loss 0.1557\n",
      "CF Training: Epoch 0003 Iter 0472 / 0490 | Time 0.1s | Iter Loss 0.1253 | Iter Mean Loss 0.1557\n",
      "CF Training: Epoch 0003 Iter 0473 / 0490 | Time 0.1s | Iter Loss 0.1555 | Iter Mean Loss 0.1557\n",
      "CF Training: Epoch 0003 Iter 0474 / 0490 | Time 0.1s | Iter Loss 0.1443 | Iter Mean Loss 0.1556\n",
      "CF Training: Epoch 0003 Iter 0475 / 0490 | Time 0.1s | Iter Loss 0.1298 | Iter Mean Loss 0.1556\n",
      "CF Training: Epoch 0003 Iter 0476 / 0490 | Time 0.1s | Iter Loss 0.1291 | Iter Mean Loss 0.1555\n",
      "CF Training: Epoch 0003 Iter 0477 / 0490 | Time 0.1s | Iter Loss 0.1145 | Iter Mean Loss 0.1555\n",
      "CF Training: Epoch 0003 Iter 0478 / 0490 | Time 0.1s | Iter Loss 0.1159 | Iter Mean Loss 0.1554\n",
      "CF Training: Epoch 0003 Iter 0479 / 0490 | Time 0.1s | Iter Loss 0.1337 | Iter Mean Loss 0.1553\n",
      "CF Training: Epoch 0003 Iter 0480 / 0490 | Time 0.1s | Iter Loss 0.1585 | Iter Mean Loss 0.1553\n",
      "CF Training: Epoch 0003 Iter 0481 / 0490 | Time 0.1s | Iter Loss 0.1587 | Iter Mean Loss 0.1553\n",
      "CF Training: Epoch 0003 Iter 0482 / 0490 | Time 0.1s | Iter Loss 0.1576 | Iter Mean Loss 0.1553\n",
      "CF Training: Epoch 0003 Iter 0483 / 0490 | Time 0.1s | Iter Loss 0.1451 | Iter Mean Loss 0.1553\n",
      "CF Training: Epoch 0003 Iter 0484 / 0490 | Time 0.1s | Iter Loss 0.1340 | Iter Mean Loss 0.1553\n",
      "CF Training: Epoch 0003 Iter 0485 / 0490 | Time 0.1s | Iter Loss 0.1141 | Iter Mean Loss 0.1552\n",
      "CF Training: Epoch 0003 Iter 0486 / 0490 | Time 0.1s | Iter Loss 0.1632 | Iter Mean Loss 0.1552\n",
      "CF Training: Epoch 0003 Iter 0487 / 0490 | Time 0.1s | Iter Loss 0.1271 | Iter Mean Loss 0.1552\n",
      "CF Training: Epoch 0003 Iter 0488 / 0490 | Time 0.1s | Iter Loss 0.1184 | Iter Mean Loss 0.1551\n",
      "CF Training: Epoch 0003 Iter 0489 / 0490 | Time 0.1s | Iter Loss 0.1479 | Iter Mean Loss 0.1551\n",
      "CF Training: Epoch 0003 Iter 0490 / 0490 | Time 0.1s | Iter Loss 0.1263 | Iter Mean Loss 0.1550\n",
      "CF Training: Epoch 0003 Total Iter 0490 | Total Time 48.5s | Iter Mean Loss 0.1550\n",
      "CF Training: Epoch 0004 Iter 0001 / 0490 | Time 0.1s | Iter Loss 0.1092 | Iter Mean Loss 0.1092\n",
      "CF Training: Epoch 0004 Iter 0002 / 0490 | Time 0.1s | Iter Loss 0.1336 | Iter Mean Loss 0.1214\n",
      "CF Training: Epoch 0004 Iter 0003 / 0490 | Time 0.1s | Iter Loss 0.1088 | Iter Mean Loss 0.1172\n",
      "CF Training: Epoch 0004 Iter 0004 / 0490 | Time 0.1s | Iter Loss 0.1132 | Iter Mean Loss 0.1162\n",
      "CF Training: Epoch 0004 Iter 0005 / 0490 | Time 0.1s | Iter Loss 0.1377 | Iter Mean Loss 0.1205\n",
      "CF Training: Epoch 0004 Iter 0006 / 0490 | Time 0.1s | Iter Loss 0.1087 | Iter Mean Loss 0.1186\n",
      "CF Training: Epoch 0004 Iter 0007 / 0490 | Time 0.1s | Iter Loss 0.1295 | Iter Mean Loss 0.1201\n",
      "CF Training: Epoch 0004 Iter 0008 / 0490 | Time 0.1s | Iter Loss 0.1639 | Iter Mean Loss 0.1256\n",
      "CF Training: Epoch 0004 Iter 0009 / 0490 | Time 0.1s | Iter Loss 0.2139 | Iter Mean Loss 0.1354\n",
      "CF Training: Epoch 0004 Iter 0010 / 0490 | Time 0.1s | Iter Loss 0.1831 | Iter Mean Loss 0.1402\n",
      "CF Training: Epoch 0004 Iter 0011 / 0490 | Time 0.1s | Iter Loss 0.1911 | Iter Mean Loss 0.1448\n",
      "CF Training: Epoch 0004 Iter 0012 / 0490 | Time 0.1s | Iter Loss 0.1503 | Iter Mean Loss 0.1453\n",
      "CF Training: Epoch 0004 Iter 0013 / 0490 | Time 0.1s | Iter Loss 0.1289 | Iter Mean Loss 0.1440\n",
      "CF Training: Epoch 0004 Iter 0014 / 0490 | Time 0.1s | Iter Loss 0.1350 | Iter Mean Loss 0.1434\n",
      "CF Training: Epoch 0004 Iter 0015 / 0490 | Time 0.1s | Iter Loss 0.1433 | Iter Mean Loss 0.1434\n",
      "CF Training: Epoch 0004 Iter 0016 / 0490 | Time 0.1s | Iter Loss 0.1422 | Iter Mean Loss 0.1433\n",
      "CF Training: Epoch 0004 Iter 0017 / 0490 | Time 0.1s | Iter Loss 0.1452 | Iter Mean Loss 0.1434\n",
      "CF Training: Epoch 0004 Iter 0018 / 0490 | Time 0.1s | Iter Loss 0.1405 | Iter Mean Loss 0.1432\n",
      "CF Training: Epoch 0004 Iter 0019 / 0490 | Time 0.1s | Iter Loss 0.1173 | Iter Mean Loss 0.1419\n",
      "CF Training: Epoch 0004 Iter 0020 / 0490 | Time 0.1s | Iter Loss 0.1326 | Iter Mean Loss 0.1414\n",
      "CF Training: Epoch 0004 Iter 0021 / 0490 | Time 0.1s | Iter Loss 0.1203 | Iter Mean Loss 0.1404\n",
      "CF Training: Epoch 0004 Iter 0022 / 0490 | Time 0.1s | Iter Loss 0.0999 | Iter Mean Loss 0.1386\n",
      "CF Training: Epoch 0004 Iter 0023 / 0490 | Time 0.1s | Iter Loss 0.1609 | Iter Mean Loss 0.1395\n",
      "CF Training: Epoch 0004 Iter 0024 / 0490 | Time 0.1s | Iter Loss 0.1601 | Iter Mean Loss 0.1404\n",
      "CF Training: Epoch 0004 Iter 0025 / 0490 | Time 0.1s | Iter Loss 0.1405 | Iter Mean Loss 0.1404\n",
      "CF Training: Epoch 0004 Iter 0026 / 0490 | Time 0.1s | Iter Loss 0.1240 | Iter Mean Loss 0.1398\n",
      "CF Training: Epoch 0004 Iter 0027 / 0490 | Time 0.1s | Iter Loss 0.1175 | Iter Mean Loss 0.1389\n",
      "CF Training: Epoch 0004 Iter 0028 / 0490 | Time 0.1s | Iter Loss 0.1411 | Iter Mean Loss 0.1390\n",
      "CF Training: Epoch 0004 Iter 0029 / 0490 | Time 0.1s | Iter Loss 0.1380 | Iter Mean Loss 0.1390\n",
      "CF Training: Epoch 0004 Iter 0030 / 0490 | Time 0.1s | Iter Loss 0.1182 | Iter Mean Loss 0.1383\n",
      "CF Training: Epoch 0004 Iter 0031 / 0490 | Time 0.1s | Iter Loss 0.1166 | Iter Mean Loss 0.1376\n",
      "CF Training: Epoch 0004 Iter 0032 / 0490 | Time 0.1s | Iter Loss 0.1466 | Iter Mean Loss 0.1379\n",
      "CF Training: Epoch 0004 Iter 0033 / 0490 | Time 0.1s | Iter Loss 0.1493 | Iter Mean Loss 0.1382\n",
      "CF Training: Epoch 0004 Iter 0034 / 0490 | Time 0.1s | Iter Loss 0.2037 | Iter Mean Loss 0.1401\n",
      "CF Training: Epoch 0004 Iter 0035 / 0490 | Time 0.1s | Iter Loss 0.1272 | Iter Mean Loss 0.1398\n",
      "CF Training: Epoch 0004 Iter 0036 / 0490 | Time 0.1s | Iter Loss 0.0952 | Iter Mean Loss 0.1385\n",
      "CF Training: Epoch 0004 Iter 0037 / 0490 | Time 0.1s | Iter Loss 0.1161 | Iter Mean Loss 0.1379\n",
      "CF Training: Epoch 0004 Iter 0038 / 0490 | Time 0.1s | Iter Loss 0.1115 | Iter Mean Loss 0.1372\n",
      "CF Training: Epoch 0004 Iter 0039 / 0490 | Time 0.1s | Iter Loss 0.1370 | Iter Mean Loss 0.1372\n",
      "CF Training: Epoch 0004 Iter 0040 / 0490 | Time 0.1s | Iter Loss 0.1544 | Iter Mean Loss 0.1377\n",
      "CF Training: Epoch 0004 Iter 0041 / 0490 | Time 0.1s | Iter Loss 0.1318 | Iter Mean Loss 0.1375\n",
      "CF Training: Epoch 0004 Iter 0042 / 0490 | Time 0.1s | Iter Loss 0.1010 | Iter Mean Loss 0.1366\n",
      "CF Training: Epoch 0004 Iter 0043 / 0490 | Time 0.1s | Iter Loss 0.1443 | Iter Mean Loss 0.1368\n",
      "CF Training: Epoch 0004 Iter 0044 / 0490 | Time 0.1s | Iter Loss 0.1381 | Iter Mean Loss 0.1369\n",
      "CF Training: Epoch 0004 Iter 0045 / 0490 | Time 0.1s | Iter Loss 0.1287 | Iter Mean Loss 0.1367\n",
      "CF Training: Epoch 0004 Iter 0046 / 0490 | Time 0.1s | Iter Loss 0.1339 | Iter Mean Loss 0.1366\n",
      "CF Training: Epoch 0004 Iter 0047 / 0490 | Time 0.1s | Iter Loss 0.1708 | Iter Mean Loss 0.1373\n",
      "CF Training: Epoch 0004 Iter 0048 / 0490 | Time 0.1s | Iter Loss 0.1819 | Iter Mean Loss 0.1383\n",
      "CF Training: Epoch 0004 Iter 0049 / 0490 | Time 0.0s | Iter Loss 0.1349 | Iter Mean Loss 0.1382\n",
      "CF Training: Epoch 0004 Iter 0050 / 0490 | Time 0.1s | Iter Loss 0.1388 | Iter Mean Loss 0.1382\n",
      "CF Training: Epoch 0004 Iter 0051 / 0490 | Time 0.1s | Iter Loss 0.1076 | Iter Mean Loss 0.1376\n",
      "CF Training: Epoch 0004 Iter 0052 / 0490 | Time 0.1s | Iter Loss 0.1322 | Iter Mean Loss 0.1375\n",
      "CF Training: Epoch 0004 Iter 0053 / 0490 | Time 0.1s | Iter Loss 0.1767 | Iter Mean Loss 0.1382\n",
      "CF Training: Epoch 0004 Iter 0054 / 0490 | Time 0.1s | Iter Loss 0.1405 | Iter Mean Loss 0.1383\n",
      "CF Training: Epoch 0004 Iter 0055 / 0490 | Time 0.1s | Iter Loss 0.1493 | Iter Mean Loss 0.1385\n",
      "CF Training: Epoch 0004 Iter 0056 / 0490 | Time 0.1s | Iter Loss 0.1309 | Iter Mean Loss 0.1384\n",
      "CF Training: Epoch 0004 Iter 0057 / 0490 | Time 0.0s | Iter Loss 0.1429 | Iter Mean Loss 0.1384\n",
      "CF Training: Epoch 0004 Iter 0058 / 0490 | Time 0.1s | Iter Loss 0.1330 | Iter Mean Loss 0.1383\n",
      "CF Training: Epoch 0004 Iter 0059 / 0490 | Time 0.1s | Iter Loss 0.0879 | Iter Mean Loss 0.1375\n",
      "CF Training: Epoch 0004 Iter 0060 / 0490 | Time 0.1s | Iter Loss 0.1505 | Iter Mean Loss 0.1377\n",
      "CF Training: Epoch 0004 Iter 0061 / 0490 | Time 0.1s | Iter Loss 0.1621 | Iter Mean Loss 0.1381\n",
      "CF Training: Epoch 0004 Iter 0062 / 0490 | Time 0.1s | Iter Loss 0.1480 | Iter Mean Loss 0.1383\n",
      "CF Training: Epoch 0004 Iter 0063 / 0490 | Time 0.1s | Iter Loss 0.1154 | Iter Mean Loss 0.1379\n",
      "CF Training: Epoch 0004 Iter 0064 / 0490 | Time 0.1s | Iter Loss 0.1624 | Iter Mean Loss 0.1383\n",
      "CF Training: Epoch 0004 Iter 0065 / 0490 | Time 0.1s | Iter Loss 0.1229 | Iter Mean Loss 0.1380\n",
      "CF Training: Epoch 0004 Iter 0066 / 0490 | Time 0.1s | Iter Loss 0.1185 | Iter Mean Loss 0.1377\n",
      "CF Training: Epoch 0004 Iter 0067 / 0490 | Time 0.1s | Iter Loss 0.1831 | Iter Mean Loss 0.1384\n",
      "CF Training: Epoch 0004 Iter 0068 / 0490 | Time 0.1s | Iter Loss 0.1189 | Iter Mean Loss 0.1381\n",
      "CF Training: Epoch 0004 Iter 0069 / 0490 | Time 0.1s | Iter Loss 0.1297 | Iter Mean Loss 0.1380\n",
      "CF Training: Epoch 0004 Iter 0070 / 0490 | Time 0.1s | Iter Loss 0.1411 | Iter Mean Loss 0.1381\n",
      "CF Training: Epoch 0004 Iter 0071 / 0490 | Time 0.1s | Iter Loss 0.1337 | Iter Mean Loss 0.1380\n",
      "CF Training: Epoch 0004 Iter 0072 / 0490 | Time 0.1s | Iter Loss 0.1319 | Iter Mean Loss 0.1379\n",
      "CF Training: Epoch 0004 Iter 0073 / 0490 | Time 0.1s | Iter Loss 0.1325 | Iter Mean Loss 0.1378\n",
      "CF Training: Epoch 0004 Iter 0074 / 0490 | Time 0.1s | Iter Loss 0.1436 | Iter Mean Loss 0.1379\n",
      "CF Training: Epoch 0004 Iter 0075 / 0490 | Time 0.1s | Iter Loss 0.1431 | Iter Mean Loss 0.1380\n",
      "CF Training: Epoch 0004 Iter 0076 / 0490 | Time 0.1s | Iter Loss 0.1021 | Iter Mean Loss 0.1375\n",
      "CF Training: Epoch 0004 Iter 0077 / 0490 | Time 0.1s | Iter Loss 0.1533 | Iter Mean Loss 0.1377\n",
      "CF Training: Epoch 0004 Iter 0078 / 0490 | Time 0.1s | Iter Loss 0.1230 | Iter Mean Loss 0.1375\n",
      "CF Training: Epoch 0004 Iter 0079 / 0490 | Time 0.1s | Iter Loss 0.1863 | Iter Mean Loss 0.1381\n",
      "CF Training: Epoch 0004 Iter 0080 / 0490 | Time 0.1s | Iter Loss 0.1103 | Iter Mean Loss 0.1378\n",
      "CF Training: Epoch 0004 Iter 0081 / 0490 | Time 0.1s | Iter Loss 0.1635 | Iter Mean Loss 0.1381\n",
      "CF Training: Epoch 0004 Iter 0082 / 0490 | Time 0.1s | Iter Loss 0.1466 | Iter Mean Loss 0.1382\n",
      "CF Training: Epoch 0004 Iter 0083 / 0490 | Time 0.1s | Iter Loss 0.1433 | Iter Mean Loss 0.1383\n",
      "CF Training: Epoch 0004 Iter 0084 / 0490 | Time 0.1s | Iter Loss 0.1111 | Iter Mean Loss 0.1380\n",
      "CF Training: Epoch 0004 Iter 0085 / 0490 | Time 0.0s | Iter Loss 0.1441 | Iter Mean Loss 0.1380\n",
      "CF Training: Epoch 0004 Iter 0086 / 0490 | Time 0.1s | Iter Loss 0.1321 | Iter Mean Loss 0.1380\n",
      "CF Training: Epoch 0004 Iter 0087 / 0490 | Time 0.1s | Iter Loss 0.1166 | Iter Mean Loss 0.1377\n",
      "CF Training: Epoch 0004 Iter 0088 / 0490 | Time 0.1s | Iter Loss 0.1388 | Iter Mean Loss 0.1377\n",
      "CF Training: Epoch 0004 Iter 0089 / 0490 | Time 0.1s | Iter Loss 0.1195 | Iter Mean Loss 0.1375\n",
      "CF Training: Epoch 0004 Iter 0090 / 0490 | Time 0.1s | Iter Loss 0.1607 | Iter Mean Loss 0.1378\n",
      "CF Training: Epoch 0004 Iter 0091 / 0490 | Time 0.1s | Iter Loss 0.1266 | Iter Mean Loss 0.1377\n",
      "CF Training: Epoch 0004 Iter 0092 / 0490 | Time 0.1s | Iter Loss 0.1230 | Iter Mean Loss 0.1375\n",
      "CF Training: Epoch 0004 Iter 0093 / 0490 | Time 0.1s | Iter Loss 0.1294 | Iter Mean Loss 0.1374\n",
      "CF Training: Epoch 0004 Iter 0094 / 0490 | Time 0.1s | Iter Loss 0.1287 | Iter Mean Loss 0.1373\n",
      "CF Training: Epoch 0004 Iter 0095 / 0490 | Time 0.1s | Iter Loss 0.1171 | Iter Mean Loss 0.1371\n",
      "CF Training: Epoch 0004 Iter 0096 / 0490 | Time 0.1s | Iter Loss 0.1264 | Iter Mean Loss 0.1370\n",
      "CF Training: Epoch 0004 Iter 0097 / 0490 | Time 0.1s | Iter Loss 0.1492 | Iter Mean Loss 0.1371\n",
      "CF Training: Epoch 0004 Iter 0098 / 0490 | Time 0.1s | Iter Loss 0.1467 | Iter Mean Loss 0.1372\n",
      "CF Training: Epoch 0004 Iter 0099 / 0490 | Time 0.1s | Iter Loss 0.1399 | Iter Mean Loss 0.1372\n",
      "CF Training: Epoch 0004 Iter 0100 / 0490 | Time 0.1s | Iter Loss 0.1179 | Iter Mean Loss 0.1370\n",
      "CF Training: Epoch 0004 Iter 0101 / 0490 | Time 0.1s | Iter Loss 0.1440 | Iter Mean Loss 0.1371\n",
      "CF Training: Epoch 0004 Iter 0102 / 0490 | Time 0.0s | Iter Loss 0.1239 | Iter Mean Loss 0.1370\n",
      "CF Training: Epoch 0004 Iter 0103 / 0490 | Time 0.1s | Iter Loss 0.1019 | Iter Mean Loss 0.1366\n",
      "CF Training: Epoch 0004 Iter 0104 / 0490 | Time 0.1s | Iter Loss 0.1578 | Iter Mean Loss 0.1369\n",
      "CF Training: Epoch 0004 Iter 0105 / 0490 | Time 0.1s | Iter Loss 0.1249 | Iter Mean Loss 0.1367\n",
      "CF Training: Epoch 0004 Iter 0106 / 0490 | Time 0.1s | Iter Loss 0.1536 | Iter Mean Loss 0.1369\n",
      "CF Training: Epoch 0004 Iter 0107 / 0490 | Time 0.1s | Iter Loss 0.1031 | Iter Mean Loss 0.1366\n",
      "CF Training: Epoch 0004 Iter 0108 / 0490 | Time 0.1s | Iter Loss 0.1337 | Iter Mean Loss 0.1366\n",
      "CF Training: Epoch 0004 Iter 0109 / 0490 | Time 0.1s | Iter Loss 0.1494 | Iter Mean Loss 0.1367\n",
      "CF Training: Epoch 0004 Iter 0110 / 0490 | Time 0.1s | Iter Loss 0.1459 | Iter Mean Loss 0.1368\n",
      "CF Training: Epoch 0004 Iter 0111 / 0490 | Time 0.1s | Iter Loss 0.1348 | Iter Mean Loss 0.1367\n",
      "CF Training: Epoch 0004 Iter 0112 / 0490 | Time 0.1s | Iter Loss 0.1195 | Iter Mean Loss 0.1366\n",
      "CF Training: Epoch 0004 Iter 0113 / 0490 | Time 0.1s | Iter Loss 0.1692 | Iter Mean Loss 0.1369\n",
      "CF Training: Epoch 0004 Iter 0114 / 0490 | Time 0.1s | Iter Loss 0.1298 | Iter Mean Loss 0.1368\n",
      "CF Training: Epoch 0004 Iter 0115 / 0490 | Time 0.1s | Iter Loss 0.1692 | Iter Mean Loss 0.1371\n",
      "CF Training: Epoch 0004 Iter 0116 / 0490 | Time 0.1s | Iter Loss 0.1272 | Iter Mean Loss 0.1370\n",
      "CF Training: Epoch 0004 Iter 0117 / 0490 | Time 0.1s | Iter Loss 0.1435 | Iter Mean Loss 0.1371\n",
      "CF Training: Epoch 0004 Iter 0118 / 0490 | Time 0.1s | Iter Loss 0.1383 | Iter Mean Loss 0.1371\n",
      "CF Training: Epoch 0004 Iter 0119 / 0490 | Time 0.1s | Iter Loss 0.1047 | Iter Mean Loss 0.1368\n",
      "CF Training: Epoch 0004 Iter 0120 / 0490 | Time 0.1s | Iter Loss 0.1246 | Iter Mean Loss 0.1367\n",
      "CF Training: Epoch 0004 Iter 0121 / 0490 | Time 0.1s | Iter Loss 0.1666 | Iter Mean Loss 0.1369\n",
      "CF Training: Epoch 0004 Iter 0122 / 0490 | Time 0.1s | Iter Loss 0.2086 | Iter Mean Loss 0.1375\n",
      "CF Training: Epoch 0004 Iter 0123 / 0490 | Time 0.1s | Iter Loss 0.1016 | Iter Mean Loss 0.1372\n",
      "CF Training: Epoch 0004 Iter 0124 / 0490 | Time 0.1s | Iter Loss 0.1118 | Iter Mean Loss 0.1370\n",
      "CF Training: Epoch 0004 Iter 0125 / 0490 | Time 0.1s | Iter Loss 0.1269 | Iter Mean Loss 0.1370\n",
      "CF Training: Epoch 0004 Iter 0126 / 0490 | Time 0.1s | Iter Loss 0.1108 | Iter Mean Loss 0.1367\n",
      "CF Training: Epoch 0004 Iter 0127 / 0490 | Time 0.1s | Iter Loss 0.1366 | Iter Mean Loss 0.1367\n",
      "CF Training: Epoch 0004 Iter 0128 / 0490 | Time 0.1s | Iter Loss 0.1676 | Iter Mean Loss 0.1370\n",
      "CF Training: Epoch 0004 Iter 0129 / 0490 | Time 0.1s | Iter Loss 0.1224 | Iter Mean Loss 0.1369\n",
      "CF Training: Epoch 0004 Iter 0130 / 0490 | Time 0.1s | Iter Loss 0.1186 | Iter Mean Loss 0.1367\n",
      "CF Training: Epoch 0004 Iter 0131 / 0490 | Time 0.1s | Iter Loss 0.1348 | Iter Mean Loss 0.1367\n",
      "CF Training: Epoch 0004 Iter 0132 / 0490 | Time 0.1s | Iter Loss 0.1523 | Iter Mean Loss 0.1368\n",
      "CF Training: Epoch 0004 Iter 0133 / 0490 | Time 0.1s | Iter Loss 0.1248 | Iter Mean Loss 0.1367\n",
      "CF Training: Epoch 0004 Iter 0134 / 0490 | Time 0.1s | Iter Loss 0.1225 | Iter Mean Loss 0.1366\n",
      "CF Training: Epoch 0004 Iter 0135 / 0490 | Time 0.1s | Iter Loss 0.1284 | Iter Mean Loss 0.1366\n",
      "CF Training: Epoch 0004 Iter 0136 / 0490 | Time 0.1s | Iter Loss 0.1485 | Iter Mean Loss 0.1367\n",
      "CF Training: Epoch 0004 Iter 0137 / 0490 | Time 0.1s | Iter Loss 0.1285 | Iter Mean Loss 0.1366\n",
      "CF Training: Epoch 0004 Iter 0138 / 0490 | Time 0.1s | Iter Loss 0.1260 | Iter Mean Loss 0.1365\n",
      "CF Training: Epoch 0004 Iter 0139 / 0490 | Time 0.1s | Iter Loss 0.1183 | Iter Mean Loss 0.1364\n",
      "CF Training: Epoch 0004 Iter 0140 / 0490 | Time 0.1s | Iter Loss 0.1466 | Iter Mean Loss 0.1365\n",
      "CF Training: Epoch 0004 Iter 0141 / 0490 | Time 0.1s | Iter Loss 0.1643 | Iter Mean Loss 0.1367\n",
      "CF Training: Epoch 0004 Iter 0142 / 0490 | Time 0.1s | Iter Loss 0.1443 | Iter Mean Loss 0.1367\n",
      "CF Training: Epoch 0004 Iter 0143 / 0490 | Time 0.1s | Iter Loss 0.1521 | Iter Mean Loss 0.1368\n",
      "CF Training: Epoch 0004 Iter 0144 / 0490 | Time 0.1s | Iter Loss 0.1075 | Iter Mean Loss 0.1366\n",
      "CF Training: Epoch 0004 Iter 0145 / 0490 | Time 0.1s | Iter Loss 0.1140 | Iter Mean Loss 0.1365\n",
      "CF Training: Epoch 0004 Iter 0146 / 0490 | Time 0.1s | Iter Loss 0.1581 | Iter Mean Loss 0.1366\n",
      "CF Training: Epoch 0004 Iter 0147 / 0490 | Time 0.1s | Iter Loss 0.1034 | Iter Mean Loss 0.1364\n",
      "CF Training: Epoch 0004 Iter 0148 / 0490 | Time 0.1s | Iter Loss 0.1397 | Iter Mean Loss 0.1364\n",
      "CF Training: Epoch 0004 Iter 0149 / 0490 | Time 0.1s | Iter Loss 0.1379 | Iter Mean Loss 0.1364\n",
      "CF Training: Epoch 0004 Iter 0150 / 0490 | Time 0.1s | Iter Loss 0.1136 | Iter Mean Loss 0.1363\n",
      "CF Training: Epoch 0004 Iter 0151 / 0490 | Time 0.1s | Iter Loss 0.1290 | Iter Mean Loss 0.1362\n",
      "CF Training: Epoch 0004 Iter 0152 / 0490 | Time 0.1s | Iter Loss 0.1249 | Iter Mean Loss 0.1361\n",
      "CF Training: Epoch 0004 Iter 0153 / 0490 | Time 0.1s | Iter Loss 0.1319 | Iter Mean Loss 0.1361\n",
      "CF Training: Epoch 0004 Iter 0154 / 0490 | Time 0.1s | Iter Loss 0.1719 | Iter Mean Loss 0.1364\n",
      "CF Training: Epoch 0004 Iter 0155 / 0490 | Time 0.1s | Iter Loss 0.1510 | Iter Mean Loss 0.1364\n",
      "CF Training: Epoch 0004 Iter 0156 / 0490 | Time 0.1s | Iter Loss 0.1645 | Iter Mean Loss 0.1366\n",
      "CF Training: Epoch 0004 Iter 0157 / 0490 | Time 0.1s | Iter Loss 0.1279 | Iter Mean Loss 0.1366\n",
      "CF Training: Epoch 0004 Iter 0158 / 0490 | Time 0.1s | Iter Loss 0.1617 | Iter Mean Loss 0.1367\n",
      "CF Training: Epoch 0004 Iter 0159 / 0490 | Time 0.1s | Iter Loss 0.1258 | Iter Mean Loss 0.1367\n",
      "CF Training: Epoch 0004 Iter 0160 / 0490 | Time 0.1s | Iter Loss 0.1286 | Iter Mean Loss 0.1366\n",
      "CF Training: Epoch 0004 Iter 0161 / 0490 | Time 0.1s | Iter Loss 0.1550 | Iter Mean Loss 0.1367\n",
      "CF Training: Epoch 0004 Iter 0162 / 0490 | Time 0.1s | Iter Loss 0.1208 | Iter Mean Loss 0.1366\n",
      "CF Training: Epoch 0004 Iter 0163 / 0490 | Time 0.1s | Iter Loss 0.1408 | Iter Mean Loss 0.1367\n",
      "CF Training: Epoch 0004 Iter 0164 / 0490 | Time 0.1s | Iter Loss 0.1586 | Iter Mean Loss 0.1368\n",
      "CF Training: Epoch 0004 Iter 0165 / 0490 | Time 0.1s | Iter Loss 0.1428 | Iter Mean Loss 0.1368\n",
      "CF Training: Epoch 0004 Iter 0166 / 0490 | Time 0.1s | Iter Loss 0.1798 | Iter Mean Loss 0.1371\n",
      "CF Training: Epoch 0004 Iter 0167 / 0490 | Time 0.1s | Iter Loss 0.1390 | Iter Mean Loss 0.1371\n",
      "CF Training: Epoch 0004 Iter 0168 / 0490 | Time 0.1s | Iter Loss 0.1446 | Iter Mean Loss 0.1371\n",
      "CF Training: Epoch 0004 Iter 0169 / 0490 | Time 0.1s | Iter Loss 0.1680 | Iter Mean Loss 0.1373\n",
      "CF Training: Epoch 0004 Iter 0170 / 0490 | Time 0.1s | Iter Loss 0.1399 | Iter Mean Loss 0.1373\n",
      "CF Training: Epoch 0004 Iter 0171 / 0490 | Time 0.1s | Iter Loss 0.1481 | Iter Mean Loss 0.1374\n",
      "CF Training: Epoch 0004 Iter 0172 / 0490 | Time 0.1s | Iter Loss 0.1181 | Iter Mean Loss 0.1373\n",
      "CF Training: Epoch 0004 Iter 0173 / 0490 | Time 0.1s | Iter Loss 0.1437 | Iter Mean Loss 0.1373\n",
      "CF Training: Epoch 0004 Iter 0174 / 0490 | Time 0.1s | Iter Loss 0.1412 | Iter Mean Loss 0.1373\n",
      "CF Training: Epoch 0004 Iter 0175 / 0490 | Time 0.1s | Iter Loss 0.1448 | Iter Mean Loss 0.1374\n",
      "CF Training: Epoch 0004 Iter 0176 / 0490 | Time 0.1s | Iter Loss 0.1521 | Iter Mean Loss 0.1375\n",
      "CF Training: Epoch 0004 Iter 0177 / 0490 | Time 0.1s | Iter Loss 0.1135 | Iter Mean Loss 0.1373\n",
      "CF Training: Epoch 0004 Iter 0178 / 0490 | Time 0.1s | Iter Loss 0.1421 | Iter Mean Loss 0.1374\n",
      "CF Training: Epoch 0004 Iter 0179 / 0490 | Time 0.1s | Iter Loss 0.1046 | Iter Mean Loss 0.1372\n",
      "CF Training: Epoch 0004 Iter 0180 / 0490 | Time 0.1s | Iter Loss 0.1036 | Iter Mean Loss 0.1370\n",
      "CF Training: Epoch 0004 Iter 0181 / 0490 | Time 0.1s | Iter Loss 0.0978 | Iter Mean Loss 0.1368\n",
      "CF Training: Epoch 0004 Iter 0182 / 0490 | Time 0.1s | Iter Loss 0.1498 | Iter Mean Loss 0.1368\n",
      "CF Training: Epoch 0004 Iter 0183 / 0490 | Time 0.1s | Iter Loss 0.1283 | Iter Mean Loss 0.1368\n",
      "CF Training: Epoch 0004 Iter 0184 / 0490 | Time 0.1s | Iter Loss 0.1378 | Iter Mean Loss 0.1368\n",
      "CF Training: Epoch 0004 Iter 0185 / 0490 | Time 0.1s | Iter Loss 0.1194 | Iter Mean Loss 0.1367\n",
      "CF Training: Epoch 0004 Iter 0186 / 0490 | Time 0.1s | Iter Loss 0.1244 | Iter Mean Loss 0.1366\n",
      "CF Training: Epoch 0004 Iter 0187 / 0490 | Time 0.1s | Iter Loss 0.1753 | Iter Mean Loss 0.1369\n",
      "CF Training: Epoch 0004 Iter 0188 / 0490 | Time 0.1s | Iter Loss 0.1147 | Iter Mean Loss 0.1367\n",
      "CF Training: Epoch 0004 Iter 0189 / 0490 | Time 0.1s | Iter Loss 0.1369 | Iter Mean Loss 0.1367\n",
      "CF Training: Epoch 0004 Iter 0190 / 0490 | Time 0.1s | Iter Loss 0.1124 | Iter Mean Loss 0.1366\n",
      "CF Training: Epoch 0004 Iter 0191 / 0490 | Time 0.1s | Iter Loss 0.1355 | Iter Mean Loss 0.1366\n",
      "CF Training: Epoch 0004 Iter 0192 / 0490 | Time 0.1s | Iter Loss 0.1593 | Iter Mean Loss 0.1367\n",
      "CF Training: Epoch 0004 Iter 0193 / 0490 | Time 0.1s | Iter Loss 0.1337 | Iter Mean Loss 0.1367\n",
      "CF Training: Epoch 0004 Iter 0194 / 0490 | Time 0.0s | Iter Loss 0.1003 | Iter Mean Loss 0.1365\n",
      "CF Training: Epoch 0004 Iter 0195 / 0490 | Time 0.1s | Iter Loss 0.1405 | Iter Mean Loss 0.1365\n",
      "CF Training: Epoch 0004 Iter 0196 / 0490 | Time 0.1s | Iter Loss 0.1058 | Iter Mean Loss 0.1364\n",
      "CF Training: Epoch 0004 Iter 0197 / 0490 | Time 0.1s | Iter Loss 0.1422 | Iter Mean Loss 0.1364\n",
      "CF Training: Epoch 0004 Iter 0198 / 0490 | Time 0.1s | Iter Loss 0.1365 | Iter Mean Loss 0.1364\n",
      "CF Training: Epoch 0004 Iter 0199 / 0490 | Time 0.1s | Iter Loss 0.1168 | Iter Mean Loss 0.1363\n",
      "CF Training: Epoch 0004 Iter 0200 / 0490 | Time 0.1s | Iter Loss 0.1654 | Iter Mean Loss 0.1365\n",
      "CF Training: Epoch 0004 Iter 0201 / 0490 | Time 0.1s | Iter Loss 0.1441 | Iter Mean Loss 0.1365\n",
      "CF Training: Epoch 0004 Iter 0202 / 0490 | Time 0.1s | Iter Loss 0.1060 | Iter Mean Loss 0.1363\n",
      "CF Training: Epoch 0004 Iter 0203 / 0490 | Time 0.1s | Iter Loss 0.1889 | Iter Mean Loss 0.1366\n",
      "CF Training: Epoch 0004 Iter 0204 / 0490 | Time 0.1s | Iter Loss 0.1283 | Iter Mean Loss 0.1366\n",
      "CF Training: Epoch 0004 Iter 0205 / 0490 | Time 0.1s | Iter Loss 0.1046 | Iter Mean Loss 0.1364\n",
      "CF Training: Epoch 0004 Iter 0206 / 0490 | Time 0.1s | Iter Loss 0.1385 | Iter Mean Loss 0.1364\n",
      "CF Training: Epoch 0004 Iter 0207 / 0490 | Time 0.1s | Iter Loss 0.1286 | Iter Mean Loss 0.1364\n",
      "CF Training: Epoch 0004 Iter 0208 / 0490 | Time 0.1s | Iter Loss 0.1276 | Iter Mean Loss 0.1363\n",
      "CF Training: Epoch 0004 Iter 0209 / 0490 | Time 0.1s | Iter Loss 0.1467 | Iter Mean Loss 0.1364\n",
      "CF Training: Epoch 0004 Iter 0210 / 0490 | Time 0.1s | Iter Loss 0.1190 | Iter Mean Loss 0.1363\n",
      "CF Training: Epoch 0004 Iter 0211 / 0490 | Time 0.1s | Iter Loss 0.1440 | Iter Mean Loss 0.1363\n",
      "CF Training: Epoch 0004 Iter 0212 / 0490 | Time 0.1s | Iter Loss 0.1320 | Iter Mean Loss 0.1363\n",
      "CF Training: Epoch 0004 Iter 0213 / 0490 | Time 0.1s | Iter Loss 0.1323 | Iter Mean Loss 0.1363\n",
      "CF Training: Epoch 0004 Iter 0214 / 0490 | Time 0.1s | Iter Loss 0.1131 | Iter Mean Loss 0.1362\n",
      "CF Training: Epoch 0004 Iter 0215 / 0490 | Time 0.1s | Iter Loss 0.1925 | Iter Mean Loss 0.1365\n",
      "CF Training: Epoch 0004 Iter 0216 / 0490 | Time 0.1s | Iter Loss 0.1322 | Iter Mean Loss 0.1364\n",
      "CF Training: Epoch 0004 Iter 0217 / 0490 | Time 0.1s | Iter Loss 0.1403 | Iter Mean Loss 0.1365\n",
      "CF Training: Epoch 0004 Iter 0218 / 0490 | Time 0.1s | Iter Loss 0.1257 | Iter Mean Loss 0.1364\n",
      "CF Training: Epoch 0004 Iter 0219 / 0490 | Time 0.1s | Iter Loss 0.1363 | Iter Mean Loss 0.1364\n",
      "CF Training: Epoch 0004 Iter 0220 / 0490 | Time 0.1s | Iter Loss 0.1081 | Iter Mean Loss 0.1363\n",
      "CF Training: Epoch 0004 Iter 0221 / 0490 | Time 0.1s | Iter Loss 0.1158 | Iter Mean Loss 0.1362\n",
      "CF Training: Epoch 0004 Iter 0222 / 0490 | Time 0.1s | Iter Loss 0.1073 | Iter Mean Loss 0.1361\n",
      "CF Training: Epoch 0004 Iter 0223 / 0490 | Time 0.1s | Iter Loss 0.1022 | Iter Mean Loss 0.1359\n",
      "CF Training: Epoch 0004 Iter 0224 / 0490 | Time 0.1s | Iter Loss 0.1808 | Iter Mean Loss 0.1361\n",
      "CF Training: Epoch 0004 Iter 0225 / 0490 | Time 0.1s | Iter Loss 0.1664 | Iter Mean Loss 0.1362\n",
      "CF Training: Epoch 0004 Iter 0226 / 0490 | Time 0.1s | Iter Loss 0.1810 | Iter Mean Loss 0.1364\n",
      "CF Training: Epoch 0004 Iter 0227 / 0490 | Time 0.1s | Iter Loss 0.0877 | Iter Mean Loss 0.1362\n",
      "CF Training: Epoch 0004 Iter 0228 / 0490 | Time 0.1s | Iter Loss 0.1306 | Iter Mean Loss 0.1362\n",
      "CF Training: Epoch 0004 Iter 0229 / 0490 | Time 0.1s | Iter Loss 0.1238 | Iter Mean Loss 0.1361\n",
      "CF Training: Epoch 0004 Iter 0230 / 0490 | Time 0.1s | Iter Loss 0.1257 | Iter Mean Loss 0.1361\n",
      "CF Training: Epoch 0004 Iter 0231 / 0490 | Time 0.1s | Iter Loss 0.1184 | Iter Mean Loss 0.1360\n",
      "CF Training: Epoch 0004 Iter 0232 / 0490 | Time 0.1s | Iter Loss 0.1187 | Iter Mean Loss 0.1359\n",
      "CF Training: Epoch 0004 Iter 0233 / 0490 | Time 0.1s | Iter Loss 0.1051 | Iter Mean Loss 0.1358\n",
      "CF Training: Epoch 0004 Iter 0234 / 0490 | Time 0.1s | Iter Loss 0.1029 | Iter Mean Loss 0.1357\n",
      "CF Training: Epoch 0004 Iter 0235 / 0490 | Time 0.1s | Iter Loss 0.1094 | Iter Mean Loss 0.1356\n",
      "CF Training: Epoch 0004 Iter 0236 / 0490 | Time 0.1s | Iter Loss 0.1641 | Iter Mean Loss 0.1357\n",
      "CF Training: Epoch 0004 Iter 0237 / 0490 | Time 0.1s | Iter Loss 0.1496 | Iter Mean Loss 0.1357\n",
      "CF Training: Epoch 0004 Iter 0238 / 0490 | Time 0.1s | Iter Loss 0.1200 | Iter Mean Loss 0.1357\n",
      "CF Training: Epoch 0004 Iter 0239 / 0490 | Time 0.1s | Iter Loss 0.1640 | Iter Mean Loss 0.1358\n",
      "CF Training: Epoch 0004 Iter 0240 / 0490 | Time 0.1s | Iter Loss 0.1250 | Iter Mean Loss 0.1357\n",
      "CF Training: Epoch 0004 Iter 0241 / 0490 | Time 0.1s | Iter Loss 0.1100 | Iter Mean Loss 0.1356\n",
      "CF Training: Epoch 0004 Iter 0242 / 0490 | Time 0.1s | Iter Loss 0.1214 | Iter Mean Loss 0.1356\n",
      "CF Training: Epoch 0004 Iter 0243 / 0490 | Time 0.1s | Iter Loss 0.1344 | Iter Mean Loss 0.1356\n",
      "CF Training: Epoch 0004 Iter 0244 / 0490 | Time 0.1s | Iter Loss 0.2297 | Iter Mean Loss 0.1360\n",
      "CF Training: Epoch 0004 Iter 0245 / 0490 | Time 0.1s | Iter Loss 0.0979 | Iter Mean Loss 0.1358\n",
      "CF Training: Epoch 0004 Iter 0246 / 0490 | Time 0.1s | Iter Loss 0.1447 | Iter Mean Loss 0.1358\n",
      "CF Training: Epoch 0004 Iter 0247 / 0490 | Time 0.1s | Iter Loss 0.1286 | Iter Mean Loss 0.1358\n",
      "CF Training: Epoch 0004 Iter 0248 / 0490 | Time 0.1s | Iter Loss 0.1201 | Iter Mean Loss 0.1357\n",
      "CF Training: Epoch 0004 Iter 0249 / 0490 | Time 0.1s | Iter Loss 0.1054 | Iter Mean Loss 0.1356\n",
      "CF Training: Epoch 0004 Iter 0250 / 0490 | Time 0.1s | Iter Loss 0.1219 | Iter Mean Loss 0.1356\n",
      "CF Training: Epoch 0004 Iter 0251 / 0490 | Time 0.1s | Iter Loss 0.1121 | Iter Mean Loss 0.1355\n",
      "CF Training: Epoch 0004 Iter 0252 / 0490 | Time 0.1s | Iter Loss 0.1310 | Iter Mean Loss 0.1355\n",
      "CF Training: Epoch 0004 Iter 0253 / 0490 | Time 0.1s | Iter Loss 0.1269 | Iter Mean Loss 0.1354\n",
      "CF Training: Epoch 0004 Iter 0254 / 0490 | Time 0.0s | Iter Loss 0.1015 | Iter Mean Loss 0.1353\n",
      "CF Training: Epoch 0004 Iter 0255 / 0490 | Time 0.1s | Iter Loss 0.1274 | Iter Mean Loss 0.1353\n",
      "CF Training: Epoch 0004 Iter 0256 / 0490 | Time 0.1s | Iter Loss 0.1219 | Iter Mean Loss 0.1352\n",
      "CF Training: Epoch 0004 Iter 0257 / 0490 | Time 0.1s | Iter Loss 0.1529 | Iter Mean Loss 0.1353\n",
      "CF Training: Epoch 0004 Iter 0258 / 0490 | Time 0.1s | Iter Loss 0.1344 | Iter Mean Loss 0.1353\n",
      "CF Training: Epoch 0004 Iter 0259 / 0490 | Time 0.1s | Iter Loss 0.0914 | Iter Mean Loss 0.1351\n",
      "CF Training: Epoch 0004 Iter 0260 / 0490 | Time 0.1s | Iter Loss 0.1189 | Iter Mean Loss 0.1350\n",
      "CF Training: Epoch 0004 Iter 0261 / 0490 | Time 0.1s | Iter Loss 0.1209 | Iter Mean Loss 0.1350\n",
      "CF Training: Epoch 0004 Iter 0262 / 0490 | Time 0.1s | Iter Loss 0.1220 | Iter Mean Loss 0.1349\n",
      "CF Training: Epoch 0004 Iter 0263 / 0490 | Time 0.1s | Iter Loss 0.1364 | Iter Mean Loss 0.1349\n",
      "CF Training: Epoch 0004 Iter 0264 / 0490 | Time 0.1s | Iter Loss 0.1257 | Iter Mean Loss 0.1349\n",
      "CF Training: Epoch 0004 Iter 0265 / 0490 | Time 0.1s | Iter Loss 0.1379 | Iter Mean Loss 0.1349\n",
      "CF Training: Epoch 0004 Iter 0266 / 0490 | Time 0.1s | Iter Loss 0.1681 | Iter Mean Loss 0.1350\n",
      "CF Training: Epoch 0004 Iter 0267 / 0490 | Time 0.1s | Iter Loss 0.1270 | Iter Mean Loss 0.1350\n",
      "CF Training: Epoch 0004 Iter 0268 / 0490 | Time 0.1s | Iter Loss 0.1345 | Iter Mean Loss 0.1350\n",
      "CF Training: Epoch 0004 Iter 0269 / 0490 | Time 0.1s | Iter Loss 0.1283 | Iter Mean Loss 0.1350\n",
      "CF Training: Epoch 0004 Iter 0270 / 0490 | Time 0.1s | Iter Loss 0.1338 | Iter Mean Loss 0.1350\n",
      "CF Training: Epoch 0004 Iter 0271 / 0490 | Time 0.1s | Iter Loss 0.1737 | Iter Mean Loss 0.1351\n",
      "CF Training: Epoch 0004 Iter 0272 / 0490 | Time 0.1s | Iter Loss 0.0954 | Iter Mean Loss 0.1350\n",
      "CF Training: Epoch 0004 Iter 0273 / 0490 | Time 0.1s | Iter Loss 0.1635 | Iter Mean Loss 0.1351\n",
      "CF Training: Epoch 0004 Iter 0274 / 0490 | Time 0.1s | Iter Loss 0.1217 | Iter Mean Loss 0.1350\n",
      "CF Training: Epoch 0004 Iter 0275 / 0490 | Time 0.1s | Iter Loss 0.1386 | Iter Mean Loss 0.1351\n",
      "CF Training: Epoch 0004 Iter 0276 / 0490 | Time 0.1s | Iter Loss 0.1199 | Iter Mean Loss 0.1350\n",
      "CF Training: Epoch 0004 Iter 0277 / 0490 | Time 0.1s | Iter Loss 0.0991 | Iter Mean Loss 0.1349\n",
      "CF Training: Epoch 0004 Iter 0278 / 0490 | Time 0.0s | Iter Loss 0.1419 | Iter Mean Loss 0.1349\n",
      "CF Training: Epoch 0004 Iter 0279 / 0490 | Time 0.1s | Iter Loss 0.1526 | Iter Mean Loss 0.1350\n",
      "CF Training: Epoch 0004 Iter 0280 / 0490 | Time 0.1s | Iter Loss 0.1327 | Iter Mean Loss 0.1349\n",
      "CF Training: Epoch 0004 Iter 0281 / 0490 | Time 0.1s | Iter Loss 0.1096 | Iter Mean Loss 0.1349\n",
      "CF Training: Epoch 0004 Iter 0282 / 0490 | Time 0.1s | Iter Loss 0.1352 | Iter Mean Loss 0.1349\n",
      "CF Training: Epoch 0004 Iter 0283 / 0490 | Time 0.1s | Iter Loss 0.1526 | Iter Mean Loss 0.1349\n",
      "CF Training: Epoch 0004 Iter 0284 / 0490 | Time 0.1s | Iter Loss 0.1249 | Iter Mean Loss 0.1349\n",
      "CF Training: Epoch 0004 Iter 0285 / 0490 | Time 0.1s | Iter Loss 0.1010 | Iter Mean Loss 0.1348\n",
      "CF Training: Epoch 0004 Iter 0286 / 0490 | Time 0.1s | Iter Loss 0.1251 | Iter Mean Loss 0.1347\n",
      "CF Training: Epoch 0004 Iter 0287 / 0490 | Time 0.1s | Iter Loss 0.0982 | Iter Mean Loss 0.1346\n",
      "CF Training: Epoch 0004 Iter 0288 / 0490 | Time 0.1s | Iter Loss 0.1200 | Iter Mean Loss 0.1346\n",
      "CF Training: Epoch 0004 Iter 0289 / 0490 | Time 0.1s | Iter Loss 0.0988 | Iter Mean Loss 0.1344\n",
      "CF Training: Epoch 0004 Iter 0290 / 0490 | Time 0.1s | Iter Loss 0.1177 | Iter Mean Loss 0.1344\n",
      "CF Training: Epoch 0004 Iter 0291 / 0490 | Time 0.1s | Iter Loss 0.1325 | Iter Mean Loss 0.1344\n",
      "CF Training: Epoch 0004 Iter 0292 / 0490 | Time 0.1s | Iter Loss 0.1010 | Iter Mean Loss 0.1343\n",
      "CF Training: Epoch 0004 Iter 0293 / 0490 | Time 0.1s | Iter Loss 0.1091 | Iter Mean Loss 0.1342\n",
      "CF Training: Epoch 0004 Iter 0294 / 0490 | Time 0.1s | Iter Loss 0.1345 | Iter Mean Loss 0.1342\n",
      "CF Training: Epoch 0004 Iter 0295 / 0490 | Time 0.1s | Iter Loss 0.1008 | Iter Mean Loss 0.1341\n",
      "CF Training: Epoch 0004 Iter 0296 / 0490 | Time 0.1s | Iter Loss 0.1115 | Iter Mean Loss 0.1340\n",
      "CF Training: Epoch 0004 Iter 0297 / 0490 | Time 0.1s | Iter Loss 0.1768 | Iter Mean Loss 0.1341\n",
      "CF Training: Epoch 0004 Iter 0298 / 0490 | Time 0.1s | Iter Loss 0.1293 | Iter Mean Loss 0.1341\n",
      "CF Training: Epoch 0004 Iter 0299 / 0490 | Time 0.1s | Iter Loss 0.1091 | Iter Mean Loss 0.1340\n",
      "CF Training: Epoch 0004 Iter 0300 / 0490 | Time 0.1s | Iter Loss 0.1275 | Iter Mean Loss 0.1340\n",
      "CF Training: Epoch 0004 Iter 0301 / 0490 | Time 0.1s | Iter Loss 0.1549 | Iter Mean Loss 0.1341\n",
      "CF Training: Epoch 0004 Iter 0302 / 0490 | Time 0.1s | Iter Loss 0.1161 | Iter Mean Loss 0.1340\n",
      "CF Training: Epoch 0004 Iter 0303 / 0490 | Time 0.1s | Iter Loss 0.1334 | Iter Mean Loss 0.1340\n",
      "CF Training: Epoch 0004 Iter 0304 / 0490 | Time 0.1s | Iter Loss 0.1236 | Iter Mean Loss 0.1340\n",
      "CF Training: Epoch 0004 Iter 0305 / 0490 | Time 0.1s | Iter Loss 0.1118 | Iter Mean Loss 0.1339\n",
      "CF Training: Epoch 0004 Iter 0306 / 0490 | Time 0.1s | Iter Loss 0.1145 | Iter Mean Loss 0.1338\n",
      "CF Training: Epoch 0004 Iter 0307 / 0490 | Time 0.1s | Iter Loss 0.1308 | Iter Mean Loss 0.1338\n",
      "CF Training: Epoch 0004 Iter 0308 / 0490 | Time 0.1s | Iter Loss 0.1053 | Iter Mean Loss 0.1337\n",
      "CF Training: Epoch 0004 Iter 0309 / 0490 | Time 0.1s | Iter Loss 0.1010 | Iter Mean Loss 0.1336\n",
      "CF Training: Epoch 0004 Iter 0310 / 0490 | Time 0.1s | Iter Loss 0.1316 | Iter Mean Loss 0.1336\n",
      "CF Training: Epoch 0004 Iter 0311 / 0490 | Time 0.1s | Iter Loss 0.0972 | Iter Mean Loss 0.1335\n",
      "CF Training: Epoch 0004 Iter 0312 / 0490 | Time 0.1s | Iter Loss 0.1615 | Iter Mean Loss 0.1336\n",
      "CF Training: Epoch 0004 Iter 0313 / 0490 | Time 0.1s | Iter Loss 0.1749 | Iter Mean Loss 0.1337\n",
      "CF Training: Epoch 0004 Iter 0314 / 0490 | Time 0.1s | Iter Loss 0.1250 | Iter Mean Loss 0.1337\n",
      "CF Training: Epoch 0004 Iter 0315 / 0490 | Time 0.1s | Iter Loss 0.1303 | Iter Mean Loss 0.1337\n",
      "CF Training: Epoch 0004 Iter 0316 / 0490 | Time 0.1s | Iter Loss 0.1244 | Iter Mean Loss 0.1337\n",
      "CF Training: Epoch 0004 Iter 0317 / 0490 | Time 0.1s | Iter Loss 0.1618 | Iter Mean Loss 0.1337\n",
      "CF Training: Epoch 0004 Iter 0318 / 0490 | Time 0.1s | Iter Loss 0.1260 | Iter Mean Loss 0.1337\n",
      "CF Training: Epoch 0004 Iter 0319 / 0490 | Time 0.0s | Iter Loss 0.1417 | Iter Mean Loss 0.1337\n",
      "CF Training: Epoch 0004 Iter 0320 / 0490 | Time 0.1s | Iter Loss 0.1208 | Iter Mean Loss 0.1337\n",
      "CF Training: Epoch 0004 Iter 0321 / 0490 | Time 0.1s | Iter Loss 0.0987 | Iter Mean Loss 0.1336\n",
      "CF Training: Epoch 0004 Iter 0322 / 0490 | Time 0.1s | Iter Loss 0.1726 | Iter Mean Loss 0.1337\n",
      "CF Training: Epoch 0004 Iter 0323 / 0490 | Time 0.1s | Iter Loss 0.1165 | Iter Mean Loss 0.1337\n",
      "CF Training: Epoch 0004 Iter 0324 / 0490 | Time 0.1s | Iter Loss 0.1178 | Iter Mean Loss 0.1336\n",
      "CF Training: Epoch 0004 Iter 0325 / 0490 | Time 0.1s | Iter Loss 0.0863 | Iter Mean Loss 0.1335\n",
      "CF Training: Epoch 0004 Iter 0326 / 0490 | Time 0.1s | Iter Loss 0.0728 | Iter Mean Loss 0.1333\n",
      "CF Training: Epoch 0004 Iter 0327 / 0490 | Time 0.1s | Iter Loss 0.1320 | Iter Mean Loss 0.1333\n",
      "CF Training: Epoch 0004 Iter 0328 / 0490 | Time 0.1s | Iter Loss 0.1247 | Iter Mean Loss 0.1333\n",
      "CF Training: Epoch 0004 Iter 0329 / 0490 | Time 0.1s | Iter Loss 0.1102 | Iter Mean Loss 0.1332\n",
      "CF Training: Epoch 0004 Iter 0330 / 0490 | Time 0.1s | Iter Loss 0.1004 | Iter Mean Loss 0.1331\n",
      "CF Training: Epoch 0004 Iter 0331 / 0490 | Time 0.1s | Iter Loss 0.1646 | Iter Mean Loss 0.1332\n",
      "CF Training: Epoch 0004 Iter 0332 / 0490 | Time 0.1s | Iter Loss 0.1224 | Iter Mean Loss 0.1332\n",
      "CF Training: Epoch 0004 Iter 0333 / 0490 | Time 0.1s | Iter Loss 0.1217 | Iter Mean Loss 0.1331\n",
      "CF Training: Epoch 0004 Iter 0334 / 0490 | Time 0.1s | Iter Loss 0.1197 | Iter Mean Loss 0.1331\n",
      "CF Training: Epoch 0004 Iter 0335 / 0490 | Time 0.1s | Iter Loss 0.1204 | Iter Mean Loss 0.1330\n",
      "CF Training: Epoch 0004 Iter 0336 / 0490 | Time 0.1s | Iter Loss 0.1146 | Iter Mean Loss 0.1330\n",
      "CF Training: Epoch 0004 Iter 0337 / 0490 | Time 0.1s | Iter Loss 0.1277 | Iter Mean Loss 0.1330\n",
      "CF Training: Epoch 0004 Iter 0338 / 0490 | Time 0.1s | Iter Loss 0.0810 | Iter Mean Loss 0.1328\n",
      "CF Training: Epoch 0004 Iter 0339 / 0490 | Time 0.1s | Iter Loss 0.1711 | Iter Mean Loss 0.1329\n",
      "CF Training: Epoch 0004 Iter 0340 / 0490 | Time 0.1s | Iter Loss 0.1015 | Iter Mean Loss 0.1328\n",
      "CF Training: Epoch 0004 Iter 0341 / 0490 | Time 0.1s | Iter Loss 0.1592 | Iter Mean Loss 0.1329\n",
      "CF Training: Epoch 0004 Iter 0342 / 0490 | Time 0.1s | Iter Loss 0.1683 | Iter Mean Loss 0.1330\n",
      "CF Training: Epoch 0004 Iter 0343 / 0490 | Time 0.1s | Iter Loss 0.1373 | Iter Mean Loss 0.1330\n",
      "CF Training: Epoch 0004 Iter 0344 / 0490 | Time 0.1s | Iter Loss 0.1019 | Iter Mean Loss 0.1329\n",
      "CF Training: Epoch 0004 Iter 0345 / 0490 | Time 0.1s | Iter Loss 0.1251 | Iter Mean Loss 0.1329\n",
      "CF Training: Epoch 0004 Iter 0346 / 0490 | Time 0.1s | Iter Loss 0.1097 | Iter Mean Loss 0.1328\n",
      "CF Training: Epoch 0004 Iter 0347 / 0490 | Time 0.1s | Iter Loss 0.1177 | Iter Mean Loss 0.1328\n",
      "CF Training: Epoch 0004 Iter 0348 / 0490 | Time 0.1s | Iter Loss 0.1298 | Iter Mean Loss 0.1328\n",
      "CF Training: Epoch 0004 Iter 0349 / 0490 | Time 0.1s | Iter Loss 0.1590 | Iter Mean Loss 0.1329\n",
      "CF Training: Epoch 0004 Iter 0350 / 0490 | Time 0.1s | Iter Loss 0.1386 | Iter Mean Loss 0.1329\n",
      "CF Training: Epoch 0004 Iter 0351 / 0490 | Time 0.1s | Iter Loss 0.1529 | Iter Mean Loss 0.1329\n",
      "CF Training: Epoch 0004 Iter 0352 / 0490 | Time 0.1s | Iter Loss 0.0729 | Iter Mean Loss 0.1328\n",
      "CF Training: Epoch 0004 Iter 0353 / 0490 | Time 0.1s | Iter Loss 0.1204 | Iter Mean Loss 0.1327\n",
      "CF Training: Epoch 0004 Iter 0354 / 0490 | Time 0.1s | Iter Loss 0.1928 | Iter Mean Loss 0.1329\n",
      "CF Training: Epoch 0004 Iter 0355 / 0490 | Time 0.1s | Iter Loss 0.1756 | Iter Mean Loss 0.1330\n",
      "CF Training: Epoch 0004 Iter 0356 / 0490 | Time 0.1s | Iter Loss 0.1303 | Iter Mean Loss 0.1330\n",
      "CF Training: Epoch 0004 Iter 0357 / 0490 | Time 0.1s | Iter Loss 0.1224 | Iter Mean Loss 0.1330\n",
      "CF Training: Epoch 0004 Iter 0358 / 0490 | Time 0.1s | Iter Loss 0.1564 | Iter Mean Loss 0.1331\n",
      "CF Training: Epoch 0004 Iter 0359 / 0490 | Time 0.1s | Iter Loss 0.1511 | Iter Mean Loss 0.1331\n",
      "CF Training: Epoch 0004 Iter 0360 / 0490 | Time 0.1s | Iter Loss 0.1039 | Iter Mean Loss 0.1330\n",
      "CF Training: Epoch 0004 Iter 0361 / 0490 | Time 0.1s | Iter Loss 0.1260 | Iter Mean Loss 0.1330\n",
      "CF Training: Epoch 0004 Iter 0362 / 0490 | Time 0.1s | Iter Loss 0.1257 | Iter Mean Loss 0.1330\n",
      "CF Training: Epoch 0004 Iter 0363 / 0490 | Time 0.1s | Iter Loss 0.0980 | Iter Mean Loss 0.1329\n",
      "CF Training: Epoch 0004 Iter 0364 / 0490 | Time 0.1s | Iter Loss 0.1038 | Iter Mean Loss 0.1328\n",
      "CF Training: Epoch 0004 Iter 0365 / 0490 | Time 0.1s | Iter Loss 0.1048 | Iter Mean Loss 0.1327\n",
      "CF Training: Epoch 0004 Iter 0366 / 0490 | Time 0.1s | Iter Loss 0.1181 | Iter Mean Loss 0.1327\n",
      "CF Training: Epoch 0004 Iter 0367 / 0490 | Time 0.1s | Iter Loss 0.1165 | Iter Mean Loss 0.1326\n",
      "CF Training: Epoch 0004 Iter 0368 / 0490 | Time 0.1s | Iter Loss 0.1032 | Iter Mean Loss 0.1326\n",
      "CF Training: Epoch 0004 Iter 0369 / 0490 | Time 0.1s | Iter Loss 0.1413 | Iter Mean Loss 0.1326\n",
      "CF Training: Epoch 0004 Iter 0370 / 0490 | Time 0.1s | Iter Loss 0.1040 | Iter Mean Loss 0.1325\n",
      "CF Training: Epoch 0004 Iter 0371 / 0490 | Time 0.1s | Iter Loss 0.0792 | Iter Mean Loss 0.1324\n",
      "CF Training: Epoch 0004 Iter 0372 / 0490 | Time 0.1s | Iter Loss 0.1063 | Iter Mean Loss 0.1323\n",
      "CF Training: Epoch 0004 Iter 0373 / 0490 | Time 0.1s | Iter Loss 0.1091 | Iter Mean Loss 0.1322\n",
      "CF Training: Epoch 0004 Iter 0374 / 0490 | Time 0.1s | Iter Loss 0.0934 | Iter Mean Loss 0.1321\n",
      "CF Training: Epoch 0004 Iter 0375 / 0490 | Time 0.1s | Iter Loss 0.0939 | Iter Mean Loss 0.1320\n",
      "CF Training: Epoch 0004 Iter 0376 / 0490 | Time 0.1s | Iter Loss 0.0918 | Iter Mean Loss 0.1319\n",
      "CF Training: Epoch 0004 Iter 0377 / 0490 | Time 0.1s | Iter Loss 0.0930 | Iter Mean Loss 0.1318\n",
      "CF Training: Epoch 0004 Iter 0378 / 0490 | Time 0.1s | Iter Loss 0.0811 | Iter Mean Loss 0.1317\n",
      "CF Training: Epoch 0004 Iter 0379 / 0490 | Time 0.1s | Iter Loss 0.1183 | Iter Mean Loss 0.1317\n",
      "CF Training: Epoch 0004 Iter 0380 / 0490 | Time 0.1s | Iter Loss 0.1631 | Iter Mean Loss 0.1317\n",
      "CF Training: Epoch 0004 Iter 0381 / 0490 | Time 0.1s | Iter Loss 0.1270 | Iter Mean Loss 0.1317\n",
      "CF Training: Epoch 0004 Iter 0382 / 0490 | Time 0.1s | Iter Loss 0.1198 | Iter Mean Loss 0.1317\n",
      "CF Training: Epoch 0004 Iter 0383 / 0490 | Time 0.1s | Iter Loss 0.1113 | Iter Mean Loss 0.1316\n",
      "CF Training: Epoch 0004 Iter 0384 / 0490 | Time 0.1s | Iter Loss 0.1103 | Iter Mean Loss 0.1316\n",
      "CF Training: Epoch 0004 Iter 0385 / 0490 | Time 0.1s | Iter Loss 0.1100 | Iter Mean Loss 0.1315\n",
      "CF Training: Epoch 0004 Iter 0386 / 0490 | Time 0.1s | Iter Loss 0.1018 | Iter Mean Loss 0.1315\n",
      "CF Training: Epoch 0004 Iter 0387 / 0490 | Time 0.1s | Iter Loss 0.1230 | Iter Mean Loss 0.1314\n",
      "CF Training: Epoch 0004 Iter 0388 / 0490 | Time 0.1s | Iter Loss 0.0987 | Iter Mean Loss 0.1313\n",
      "CF Training: Epoch 0004 Iter 0389 / 0490 | Time 0.1s | Iter Loss 0.1450 | Iter Mean Loss 0.1314\n",
      "CF Training: Epoch 0004 Iter 0390 / 0490 | Time 0.1s | Iter Loss 0.1437 | Iter Mean Loss 0.1314\n",
      "CF Training: Epoch 0004 Iter 0391 / 0490 | Time 0.1s | Iter Loss 0.1105 | Iter Mean Loss 0.1314\n",
      "CF Training: Epoch 0004 Iter 0392 / 0490 | Time 0.1s | Iter Loss 0.1571 | Iter Mean Loss 0.1314\n",
      "CF Training: Epoch 0004 Iter 0393 / 0490 | Time 0.1s | Iter Loss 0.1144 | Iter Mean Loss 0.1314\n",
      "CF Training: Epoch 0004 Iter 0394 / 0490 | Time 0.1s | Iter Loss 0.1597 | Iter Mean Loss 0.1315\n",
      "CF Training: Epoch 0004 Iter 0395 / 0490 | Time 0.1s | Iter Loss 0.0958 | Iter Mean Loss 0.1314\n",
      "CF Training: Epoch 0004 Iter 0396 / 0490 | Time 0.1s | Iter Loss 0.1574 | Iter Mean Loss 0.1314\n",
      "CF Training: Epoch 0004 Iter 0397 / 0490 | Time 0.1s | Iter Loss 0.0954 | Iter Mean Loss 0.1313\n",
      "CF Training: Epoch 0004 Iter 0398 / 0490 | Time 0.1s | Iter Loss 0.1422 | Iter Mean Loss 0.1314\n",
      "CF Training: Epoch 0004 Iter 0399 / 0490 | Time 0.1s | Iter Loss 0.0999 | Iter Mean Loss 0.1313\n",
      "CF Training: Epoch 0004 Iter 0400 / 0490 | Time 0.1s | Iter Loss 0.0789 | Iter Mean Loss 0.1312\n",
      "CF Training: Epoch 0004 Iter 0401 / 0490 | Time 0.1s | Iter Loss 0.1077 | Iter Mean Loss 0.1311\n",
      "CF Training: Epoch 0004 Iter 0402 / 0490 | Time 0.1s | Iter Loss 0.1134 | Iter Mean Loss 0.1311\n",
      "CF Training: Epoch 0004 Iter 0403 / 0490 | Time 0.1s | Iter Loss 0.0995 | Iter Mean Loss 0.1310\n",
      "CF Training: Epoch 0004 Iter 0404 / 0490 | Time 0.1s | Iter Loss 0.1316 | Iter Mean Loss 0.1310\n",
      "CF Training: Epoch 0004 Iter 0405 / 0490 | Time 0.1s | Iter Loss 0.1103 | Iter Mean Loss 0.1309\n",
      "CF Training: Epoch 0004 Iter 0406 / 0490 | Time 0.1s | Iter Loss 0.1048 | Iter Mean Loss 0.1309\n",
      "CF Training: Epoch 0004 Iter 0407 / 0490 | Time 0.1s | Iter Loss 0.1220 | Iter Mean Loss 0.1308\n",
      "CF Training: Epoch 0004 Iter 0408 / 0490 | Time 0.1s | Iter Loss 0.1225 | Iter Mean Loss 0.1308\n",
      "CF Training: Epoch 0004 Iter 0409 / 0490 | Time 0.1s | Iter Loss 0.1290 | Iter Mean Loss 0.1308\n",
      "CF Training: Epoch 0004 Iter 0410 / 0490 | Time 0.1s | Iter Loss 0.0843 | Iter Mean Loss 0.1307\n",
      "CF Training: Epoch 0004 Iter 0411 / 0490 | Time 0.1s | Iter Loss 0.1116 | Iter Mean Loss 0.1307\n",
      "CF Training: Epoch 0004 Iter 0412 / 0490 | Time 0.1s | Iter Loss 0.1207 | Iter Mean Loss 0.1306\n",
      "CF Training: Epoch 0004 Iter 0413 / 0490 | Time 0.1s | Iter Loss 0.0888 | Iter Mean Loss 0.1305\n",
      "CF Training: Epoch 0004 Iter 0414 / 0490 | Time 0.1s | Iter Loss 0.1206 | Iter Mean Loss 0.1305\n",
      "CF Training: Epoch 0004 Iter 0415 / 0490 | Time 0.1s | Iter Loss 0.1049 | Iter Mean Loss 0.1304\n",
      "CF Training: Epoch 0004 Iter 0416 / 0490 | Time 0.1s | Iter Loss 0.1153 | Iter Mean Loss 0.1304\n",
      "CF Training: Epoch 0004 Iter 0417 / 0490 | Time 0.1s | Iter Loss 0.0984 | Iter Mean Loss 0.1303\n",
      "CF Training: Epoch 0004 Iter 0418 / 0490 | Time 0.1s | Iter Loss 0.1197 | Iter Mean Loss 0.1303\n",
      "CF Training: Epoch 0004 Iter 0419 / 0490 | Time 0.1s | Iter Loss 0.0984 | Iter Mean Loss 0.1302\n",
      "CF Training: Epoch 0004 Iter 0420 / 0490 | Time 0.1s | Iter Loss 0.1143 | Iter Mean Loss 0.1302\n",
      "CF Training: Epoch 0004 Iter 0421 / 0490 | Time 0.1s | Iter Loss 0.1163 | Iter Mean Loss 0.1302\n",
      "CF Training: Epoch 0004 Iter 0422 / 0490 | Time 0.1s | Iter Loss 0.1108 | Iter Mean Loss 0.1301\n",
      "CF Training: Epoch 0004 Iter 0423 / 0490 | Time 0.1s | Iter Loss 0.1107 | Iter Mean Loss 0.1301\n",
      "CF Training: Epoch 0004 Iter 0424 / 0490 | Time 0.1s | Iter Loss 0.1209 | Iter Mean Loss 0.1300\n",
      "CF Training: Epoch 0004 Iter 0425 / 0490 | Time 0.1s | Iter Loss 0.1399 | Iter Mean Loss 0.1301\n",
      "CF Training: Epoch 0004 Iter 0426 / 0490 | Time 0.1s | Iter Loss 0.1048 | Iter Mean Loss 0.1300\n",
      "CF Training: Epoch 0004 Iter 0427 / 0490 | Time 0.1s | Iter Loss 0.1742 | Iter Mean Loss 0.1301\n",
      "CF Training: Epoch 0004 Iter 0428 / 0490 | Time 0.1s | Iter Loss 0.1124 | Iter Mean Loss 0.1301\n",
      "CF Training: Epoch 0004 Iter 0429 / 0490 | Time 0.1s | Iter Loss 0.1606 | Iter Mean Loss 0.1301\n",
      "CF Training: Epoch 0004 Iter 0430 / 0490 | Time 0.1s | Iter Loss 0.0901 | Iter Mean Loss 0.1300\n",
      "CF Training: Epoch 0004 Iter 0431 / 0490 | Time 0.1s | Iter Loss 0.0703 | Iter Mean Loss 0.1299\n",
      "CF Training: Epoch 0004 Iter 0432 / 0490 | Time 0.1s | Iter Loss 0.1458 | Iter Mean Loss 0.1299\n",
      "CF Training: Epoch 0004 Iter 0433 / 0490 | Time 0.1s | Iter Loss 0.1007 | Iter Mean Loss 0.1299\n",
      "CF Training: Epoch 0004 Iter 0434 / 0490 | Time 0.1s | Iter Loss 0.1021 | Iter Mean Loss 0.1298\n",
      "CF Training: Epoch 0004 Iter 0435 / 0490 | Time 0.1s | Iter Loss 0.1187 | Iter Mean Loss 0.1298\n",
      "CF Training: Epoch 0004 Iter 0436 / 0490 | Time 0.1s | Iter Loss 0.1216 | Iter Mean Loss 0.1298\n",
      "CF Training: Epoch 0004 Iter 0437 / 0490 | Time 0.1s | Iter Loss 0.1500 | Iter Mean Loss 0.1298\n",
      "CF Training: Epoch 0004 Iter 0438 / 0490 | Time 0.1s | Iter Loss 0.0994 | Iter Mean Loss 0.1297\n",
      "CF Training: Epoch 0004 Iter 0439 / 0490 | Time 0.1s | Iter Loss 0.1259 | Iter Mean Loss 0.1297\n",
      "CF Training: Epoch 0004 Iter 0440 / 0490 | Time 0.1s | Iter Loss 0.0909 | Iter Mean Loss 0.1296\n",
      "CF Training: Epoch 0004 Iter 0441 / 0490 | Time 0.1s | Iter Loss 0.1275 | Iter Mean Loss 0.1296\n",
      "CF Training: Epoch 0004 Iter 0442 / 0490 | Time 0.1s | Iter Loss 0.1207 | Iter Mean Loss 0.1296\n",
      "CF Training: Epoch 0004 Iter 0443 / 0490 | Time 0.1s | Iter Loss 0.1286 | Iter Mean Loss 0.1296\n",
      "CF Training: Epoch 0004 Iter 0444 / 0490 | Time 0.1s | Iter Loss 0.1234 | Iter Mean Loss 0.1296\n",
      "CF Training: Epoch 0004 Iter 0445 / 0490 | Time 0.1s | Iter Loss 0.1733 | Iter Mean Loss 0.1297\n",
      "CF Training: Epoch 0004 Iter 0446 / 0490 | Time 0.1s | Iter Loss 0.1522 | Iter Mean Loss 0.1298\n",
      "CF Training: Epoch 0004 Iter 0447 / 0490 | Time 0.1s | Iter Loss 0.1189 | Iter Mean Loss 0.1297\n",
      "CF Training: Epoch 0004 Iter 0448 / 0490 | Time 0.1s | Iter Loss 0.1549 | Iter Mean Loss 0.1298\n",
      "CF Training: Epoch 0004 Iter 0449 / 0490 | Time 0.1s | Iter Loss 0.1241 | Iter Mean Loss 0.1298\n",
      "CF Training: Epoch 0004 Iter 0450 / 0490 | Time 0.1s | Iter Loss 0.1202 | Iter Mean Loss 0.1298\n",
      "CF Training: Epoch 0004 Iter 0451 / 0490 | Time 0.1s | Iter Loss 0.1093 | Iter Mean Loss 0.1297\n",
      "CF Training: Epoch 0004 Iter 0452 / 0490 | Time 0.1s | Iter Loss 0.1155 | Iter Mean Loss 0.1297\n",
      "CF Training: Epoch 0004 Iter 0453 / 0490 | Time 0.1s | Iter Loss 0.1399 | Iter Mean Loss 0.1297\n",
      "CF Training: Epoch 0004 Iter 0454 / 0490 | Time 0.1s | Iter Loss 0.1092 | Iter Mean Loss 0.1297\n",
      "CF Training: Epoch 0004 Iter 0455 / 0490 | Time 0.1s | Iter Loss 0.1324 | Iter Mean Loss 0.1297\n",
      "CF Training: Epoch 0004 Iter 0456 / 0490 | Time 0.1s | Iter Loss 0.1019 | Iter Mean Loss 0.1296\n",
      "CF Training: Epoch 0004 Iter 0457 / 0490 | Time 0.1s | Iter Loss 0.1062 | Iter Mean Loss 0.1295\n",
      "CF Training: Epoch 0004 Iter 0458 / 0490 | Time 0.1s | Iter Loss 0.0742 | Iter Mean Loss 0.1294\n",
      "CF Training: Epoch 0004 Iter 0459 / 0490 | Time 0.1s | Iter Loss 0.1147 | Iter Mean Loss 0.1294\n",
      "CF Training: Epoch 0004 Iter 0460 / 0490 | Time 0.1s | Iter Loss 0.1352 | Iter Mean Loss 0.1294\n",
      "CF Training: Epoch 0004 Iter 0461 / 0490 | Time 0.1s | Iter Loss 0.1594 | Iter Mean Loss 0.1295\n",
      "CF Training: Epoch 0004 Iter 0462 / 0490 | Time 0.1s | Iter Loss 0.1314 | Iter Mean Loss 0.1295\n",
      "CF Training: Epoch 0004 Iter 0463 / 0490 | Time 0.1s | Iter Loss 0.0960 | Iter Mean Loss 0.1294\n",
      "CF Training: Epoch 0004 Iter 0464 / 0490 | Time 0.1s | Iter Loss 0.1392 | Iter Mean Loss 0.1294\n",
      "CF Training: Epoch 0004 Iter 0465 / 0490 | Time 0.1s | Iter Loss 0.1036 | Iter Mean Loss 0.1294\n",
      "CF Training: Epoch 0004 Iter 0466 / 0490 | Time 0.1s | Iter Loss 0.1221 | Iter Mean Loss 0.1294\n",
      "CF Training: Epoch 0004 Iter 0467 / 0490 | Time 0.1s | Iter Loss 0.1279 | Iter Mean Loss 0.1294\n",
      "CF Training: Epoch 0004 Iter 0468 / 0490 | Time 0.1s | Iter Loss 0.0809 | Iter Mean Loss 0.1292\n",
      "CF Training: Epoch 0004 Iter 0469 / 0490 | Time 0.1s | Iter Loss 0.1177 | Iter Mean Loss 0.1292\n",
      "CF Training: Epoch 0004 Iter 0470 / 0490 | Time 0.1s | Iter Loss 0.1217 | Iter Mean Loss 0.1292\n",
      "CF Training: Epoch 0004 Iter 0471 / 0490 | Time 0.0s | Iter Loss 0.1243 | Iter Mean Loss 0.1292\n",
      "CF Training: Epoch 0004 Iter 0472 / 0490 | Time 0.1s | Iter Loss 0.1274 | Iter Mean Loss 0.1292\n",
      "CF Training: Epoch 0004 Iter 0473 / 0490 | Time 0.1s | Iter Loss 0.0881 | Iter Mean Loss 0.1291\n",
      "CF Training: Epoch 0004 Iter 0474 / 0490 | Time 0.1s | Iter Loss 0.1277 | Iter Mean Loss 0.1291\n",
      "CF Training: Epoch 0004 Iter 0475 / 0490 | Time 0.1s | Iter Loss 0.1216 | Iter Mean Loss 0.1291\n",
      "CF Training: Epoch 0004 Iter 0476 / 0490 | Time 0.1s | Iter Loss 0.1159 | Iter Mean Loss 0.1291\n",
      "CF Training: Epoch 0004 Iter 0477 / 0490 | Time 0.1s | Iter Loss 0.1618 | Iter Mean Loss 0.1291\n",
      "CF Training: Epoch 0004 Iter 0478 / 0490 | Time 0.1s | Iter Loss 0.1113 | Iter Mean Loss 0.1291\n",
      "CF Training: Epoch 0004 Iter 0479 / 0490 | Time 0.1s | Iter Loss 0.1049 | Iter Mean Loss 0.1290\n",
      "CF Training: Epoch 0004 Iter 0480 / 0490 | Time 0.1s | Iter Loss 0.1109 | Iter Mean Loss 0.1290\n",
      "CF Training: Epoch 0004 Iter 0481 / 0490 | Time 0.1s | Iter Loss 0.1449 | Iter Mean Loss 0.1290\n",
      "CF Training: Epoch 0004 Iter 0482 / 0490 | Time 0.1s | Iter Loss 0.1517 | Iter Mean Loss 0.1291\n",
      "CF Training: Epoch 0004 Iter 0483 / 0490 | Time 0.1s | Iter Loss 0.1403 | Iter Mean Loss 0.1291\n",
      "CF Training: Epoch 0004 Iter 0484 / 0490 | Time 0.1s | Iter Loss 0.0961 | Iter Mean Loss 0.1290\n",
      "CF Training: Epoch 0004 Iter 0485 / 0490 | Time 0.1s | Iter Loss 0.1215 | Iter Mean Loss 0.1290\n",
      "CF Training: Epoch 0004 Iter 0486 / 0490 | Time 0.1s | Iter Loss 0.1726 | Iter Mean Loss 0.1291\n",
      "CF Training: Epoch 0004 Iter 0487 / 0490 | Time 0.1s | Iter Loss 0.1184 | Iter Mean Loss 0.1291\n",
      "CF Training: Epoch 0004 Iter 0488 / 0490 | Time 0.1s | Iter Loss 0.1319 | Iter Mean Loss 0.1291\n",
      "CF Training: Epoch 0004 Iter 0489 / 0490 | Time 0.1s | Iter Loss 0.1258 | Iter Mean Loss 0.1291\n",
      "CF Training: Epoch 0004 Iter 0490 / 0490 | Time 0.1s | Iter Loss 0.1435 | Iter Mean Loss 0.1291\n",
      "CF Training: Epoch 0004 Total Iter 0490 | Total Time 48.4s | Iter Mean Loss 0.1291\n",
      "CF Training: Epoch 0005 Iter 0001 / 0490 | Time 0.1s | Iter Loss 0.1361 | Iter Mean Loss 0.1361\n",
      "CF Training: Epoch 0005 Iter 0002 / 0490 | Time 0.1s | Iter Loss 0.1339 | Iter Mean Loss 0.1350\n",
      "CF Training: Epoch 0005 Iter 0003 / 0490 | Time 0.0s | Iter Loss 0.1011 | Iter Mean Loss 0.1237\n",
      "CF Training: Epoch 0005 Iter 0004 / 0490 | Time 0.1s | Iter Loss 0.1263 | Iter Mean Loss 0.1244\n",
      "CF Training: Epoch 0005 Iter 0005 / 0490 | Time 0.1s | Iter Loss 0.1462 | Iter Mean Loss 0.1287\n",
      "CF Training: Epoch 0005 Iter 0006 / 0490 | Time 0.1s | Iter Loss 0.1243 | Iter Mean Loss 0.1280\n",
      "CF Training: Epoch 0005 Iter 0007 / 0490 | Time 0.1s | Iter Loss 0.0940 | Iter Mean Loss 0.1231\n",
      "CF Training: Epoch 0005 Iter 0008 / 0490 | Time 0.1s | Iter Loss 0.1248 | Iter Mean Loss 0.1233\n",
      "CF Training: Epoch 0005 Iter 0009 / 0490 | Time 0.1s | Iter Loss 0.1273 | Iter Mean Loss 0.1238\n",
      "CF Training: Epoch 0005 Iter 0010 / 0490 | Time 0.1s | Iter Loss 0.1191 | Iter Mean Loss 0.1233\n",
      "CF Training: Epoch 0005 Iter 0011 / 0490 | Time 0.0s | Iter Loss 0.1213 | Iter Mean Loss 0.1231\n",
      "CF Training: Epoch 0005 Iter 0012 / 0490 | Time 0.1s | Iter Loss 0.1128 | Iter Mean Loss 0.1223\n",
      "CF Training: Epoch 0005 Iter 0013 / 0490 | Time 0.1s | Iter Loss 0.1049 | Iter Mean Loss 0.1209\n",
      "CF Training: Epoch 0005 Iter 0014 / 0490 | Time 0.1s | Iter Loss 0.1189 | Iter Mean Loss 0.1208\n",
      "CF Training: Epoch 0005 Iter 0015 / 0490 | Time 0.1s | Iter Loss 0.1147 | Iter Mean Loss 0.1204\n",
      "CF Training: Epoch 0005 Iter 0016 / 0490 | Time 0.1s | Iter Loss 0.1141 | Iter Mean Loss 0.1200\n",
      "CF Training: Epoch 0005 Iter 0017 / 0490 | Time 0.1s | Iter Loss 0.1272 | Iter Mean Loss 0.1204\n",
      "CF Training: Epoch 0005 Iter 0018 / 0490 | Time 0.1s | Iter Loss 0.1381 | Iter Mean Loss 0.1214\n",
      "CF Training: Epoch 0005 Iter 0019 / 0490 | Time 0.1s | Iter Loss 0.1306 | Iter Mean Loss 0.1219\n",
      "CF Training: Epoch 0005 Iter 0020 / 0490 | Time 0.1s | Iter Loss 0.1163 | Iter Mean Loss 0.1216\n",
      "CF Training: Epoch 0005 Iter 0021 / 0490 | Time 0.1s | Iter Loss 0.0903 | Iter Mean Loss 0.1201\n",
      "CF Training: Epoch 0005 Iter 0022 / 0490 | Time 0.1s | Iter Loss 0.1069 | Iter Mean Loss 0.1195\n",
      "CF Training: Epoch 0005 Iter 0023 / 0490 | Time 0.1s | Iter Loss 0.1152 | Iter Mean Loss 0.1193\n",
      "CF Training: Epoch 0005 Iter 0024 / 0490 | Time 0.1s | Iter Loss 0.1390 | Iter Mean Loss 0.1201\n",
      "CF Training: Epoch 0005 Iter 0025 / 0490 | Time 0.1s | Iter Loss 0.1098 | Iter Mean Loss 0.1197\n",
      "CF Training: Epoch 0005 Iter 0026 / 0490 | Time 0.1s | Iter Loss 0.0755 | Iter Mean Loss 0.1180\n",
      "CF Training: Epoch 0005 Iter 0027 / 0490 | Time 0.1s | Iter Loss 0.1226 | Iter Mean Loss 0.1182\n",
      "CF Training: Epoch 0005 Iter 0028 / 0490 | Time 0.1s | Iter Loss 0.1106 | Iter Mean Loss 0.1179\n",
      "CF Training: Epoch 0005 Iter 0029 / 0490 | Time 0.1s | Iter Loss 0.1093 | Iter Mean Loss 0.1176\n",
      "CF Training: Epoch 0005 Iter 0030 / 0490 | Time 0.1s | Iter Loss 0.0835 | Iter Mean Loss 0.1165\n",
      "CF Training: Epoch 0005 Iter 0031 / 0490 | Time 0.1s | Iter Loss 0.1267 | Iter Mean Loss 0.1168\n",
      "CF Training: Epoch 0005 Iter 0032 / 0490 | Time 0.1s | Iter Loss 0.1576 | Iter Mean Loss 0.1181\n",
      "CF Training: Epoch 0005 Iter 0033 / 0490 | Time 0.1s | Iter Loss 0.1118 | Iter Mean Loss 0.1179\n",
      "CF Training: Epoch 0005 Iter 0034 / 0490 | Time 0.1s | Iter Loss 0.1471 | Iter Mean Loss 0.1188\n",
      "CF Training: Epoch 0005 Iter 0035 / 0490 | Time 0.1s | Iter Loss 0.1122 | Iter Mean Loss 0.1186\n",
      "CF Training: Epoch 0005 Iter 0036 / 0490 | Time 0.1s | Iter Loss 0.1045 | Iter Mean Loss 0.1182\n",
      "CF Training: Epoch 0005 Iter 0037 / 0490 | Time 0.1s | Iter Loss 0.1807 | Iter Mean Loss 0.1199\n",
      "CF Training: Epoch 0005 Iter 0038 / 0490 | Time 0.1s | Iter Loss 0.1391 | Iter Mean Loss 0.1204\n",
      "CF Training: Epoch 0005 Iter 0039 / 0490 | Time 0.1s | Iter Loss 0.1148 | Iter Mean Loss 0.1202\n",
      "CF Training: Epoch 0005 Iter 0040 / 0490 | Time 0.1s | Iter Loss 0.1185 | Iter Mean Loss 0.1202\n",
      "CF Training: Epoch 0005 Iter 0041 / 0490 | Time 0.1s | Iter Loss 0.1577 | Iter Mean Loss 0.1211\n",
      "CF Training: Epoch 0005 Iter 0042 / 0490 | Time 0.1s | Iter Loss 0.1225 | Iter Mean Loss 0.1211\n",
      "CF Training: Epoch 0005 Iter 0043 / 0490 | Time 0.1s | Iter Loss 0.1464 | Iter Mean Loss 0.1217\n",
      "CF Training: Epoch 0005 Iter 0044 / 0490 | Time 0.1s | Iter Loss 0.0992 | Iter Mean Loss 0.1212\n",
      "CF Training: Epoch 0005 Iter 0045 / 0490 | Time 0.1s | Iter Loss 0.1066 | Iter Mean Loss 0.1209\n",
      "CF Training: Epoch 0005 Iter 0046 / 0490 | Time 0.1s | Iter Loss 0.1204 | Iter Mean Loss 0.1209\n",
      "CF Training: Epoch 0005 Iter 0047 / 0490 | Time 0.0s | Iter Loss 0.1288 | Iter Mean Loss 0.1211\n",
      "CF Training: Epoch 0005 Iter 0048 / 0490 | Time 0.1s | Iter Loss 0.0990 | Iter Mean Loss 0.1206\n",
      "CF Training: Epoch 0005 Iter 0049 / 0490 | Time 0.1s | Iter Loss 0.0834 | Iter Mean Loss 0.1198\n",
      "CF Training: Epoch 0005 Iter 0050 / 0490 | Time 0.1s | Iter Loss 0.1075 | Iter Mean Loss 0.1196\n",
      "CF Training: Epoch 0005 Iter 0051 / 0490 | Time 0.1s | Iter Loss 0.1012 | Iter Mean Loss 0.1192\n",
      "CF Training: Epoch 0005 Iter 0052 / 0490 | Time 0.1s | Iter Loss 0.0978 | Iter Mean Loss 0.1188\n",
      "CF Training: Epoch 0005 Iter 0053 / 0490 | Time 0.1s | Iter Loss 0.1097 | Iter Mean Loss 0.1186\n",
      "CF Training: Epoch 0005 Iter 0054 / 0490 | Time 0.1s | Iter Loss 0.1554 | Iter Mean Loss 0.1193\n",
      "CF Training: Epoch 0005 Iter 0055 / 0490 | Time 0.1s | Iter Loss 0.0933 | Iter Mean Loss 0.1188\n",
      "CF Training: Epoch 0005 Iter 0056 / 0490 | Time 0.1s | Iter Loss 0.1100 | Iter Mean Loss 0.1187\n",
      "CF Training: Epoch 0005 Iter 0057 / 0490 | Time 0.1s | Iter Loss 0.0955 | Iter Mean Loss 0.1183\n",
      "CF Training: Epoch 0005 Iter 0058 / 0490 | Time 0.1s | Iter Loss 0.1096 | Iter Mean Loss 0.1181\n",
      "CF Training: Epoch 0005 Iter 0059 / 0490 | Time 0.1s | Iter Loss 0.1391 | Iter Mean Loss 0.1185\n",
      "CF Training: Epoch 0005 Iter 0060 / 0490 | Time 0.1s | Iter Loss 0.1173 | Iter Mean Loss 0.1185\n",
      "CF Training: Epoch 0005 Iter 0061 / 0490 | Time 0.1s | Iter Loss 0.0964 | Iter Mean Loss 0.1181\n",
      "CF Training: Epoch 0005 Iter 0062 / 0490 | Time 0.1s | Iter Loss 0.1578 | Iter Mean Loss 0.1187\n",
      "CF Training: Epoch 0005 Iter 0063 / 0490 | Time 0.1s | Iter Loss 0.1351 | Iter Mean Loss 0.1190\n",
      "CF Training: Epoch 0005 Iter 0064 / 0490 | Time 0.1s | Iter Loss 0.1112 | Iter Mean Loss 0.1189\n",
      "CF Training: Epoch 0005 Iter 0065 / 0490 | Time 0.1s | Iter Loss 0.1024 | Iter Mean Loss 0.1186\n",
      "CF Training: Epoch 0005 Iter 0066 / 0490 | Time 0.1s | Iter Loss 0.0877 | Iter Mean Loss 0.1182\n",
      "CF Training: Epoch 0005 Iter 0067 / 0490 | Time 0.1s | Iter Loss 0.1875 | Iter Mean Loss 0.1192\n",
      "CF Training: Epoch 0005 Iter 0068 / 0490 | Time 0.1s | Iter Loss 0.0933 | Iter Mean Loss 0.1188\n",
      "CF Training: Epoch 0005 Iter 0069 / 0490 | Time 0.1s | Iter Loss 0.1640 | Iter Mean Loss 0.1195\n",
      "CF Training: Epoch 0005 Iter 0070 / 0490 | Time 0.1s | Iter Loss 0.1226 | Iter Mean Loss 0.1195\n",
      "CF Training: Epoch 0005 Iter 0071 / 0490 | Time 0.1s | Iter Loss 0.1098 | Iter Mean Loss 0.1194\n",
      "CF Training: Epoch 0005 Iter 0072 / 0490 | Time 0.1s | Iter Loss 0.0989 | Iter Mean Loss 0.1191\n",
      "CF Training: Epoch 0005 Iter 0073 / 0490 | Time 0.1s | Iter Loss 0.1111 | Iter Mean Loss 0.1190\n",
      "CF Training: Epoch 0005 Iter 0074 / 0490 | Time 0.1s | Iter Loss 0.1229 | Iter Mean Loss 0.1190\n",
      "CF Training: Epoch 0005 Iter 0075 / 0490 | Time 0.1s | Iter Loss 0.1035 | Iter Mean Loss 0.1188\n",
      "CF Training: Epoch 0005 Iter 0076 / 0490 | Time 0.1s | Iter Loss 0.0969 | Iter Mean Loss 0.1185\n",
      "CF Training: Epoch 0005 Iter 0077 / 0490 | Time 0.1s | Iter Loss 0.1122 | Iter Mean Loss 0.1185\n",
      "CF Training: Epoch 0005 Iter 0078 / 0490 | Time 0.1s | Iter Loss 0.1012 | Iter Mean Loss 0.1182\n",
      "CF Training: Epoch 0005 Iter 0079 / 0490 | Time 0.1s | Iter Loss 0.0844 | Iter Mean Loss 0.1178\n",
      "CF Training: Epoch 0005 Iter 0080 / 0490 | Time 0.1s | Iter Loss 0.1330 | Iter Mean Loss 0.1180\n",
      "CF Training: Epoch 0005 Iter 0081 / 0490 | Time 0.1s | Iter Loss 0.1270 | Iter Mean Loss 0.1181\n",
      "CF Training: Epoch 0005 Iter 0082 / 0490 | Time 0.1s | Iter Loss 0.1328 | Iter Mean Loss 0.1183\n",
      "CF Training: Epoch 0005 Iter 0083 / 0490 | Time 0.1s | Iter Loss 0.1051 | Iter Mean Loss 0.1181\n",
      "CF Training: Epoch 0005 Iter 0084 / 0490 | Time 0.1s | Iter Loss 0.1495 | Iter Mean Loss 0.1185\n",
      "CF Training: Epoch 0005 Iter 0085 / 0490 | Time 0.1s | Iter Loss 0.1199 | Iter Mean Loss 0.1185\n",
      "CF Training: Epoch 0005 Iter 0086 / 0490 | Time 0.1s | Iter Loss 0.1387 | Iter Mean Loss 0.1188\n",
      "CF Training: Epoch 0005 Iter 0087 / 0490 | Time 0.1s | Iter Loss 0.0880 | Iter Mean Loss 0.1184\n",
      "CF Training: Epoch 0005 Iter 0088 / 0490 | Time 0.1s | Iter Loss 0.1124 | Iter Mean Loss 0.1183\n",
      "CF Training: Epoch 0005 Iter 0089 / 0490 | Time 0.1s | Iter Loss 0.1020 | Iter Mean Loss 0.1181\n",
      "CF Training: Epoch 0005 Iter 0090 / 0490 | Time 0.1s | Iter Loss 0.1085 | Iter Mean Loss 0.1180\n",
      "CF Training: Epoch 0005 Iter 0091 / 0490 | Time 0.1s | Iter Loss 0.1365 | Iter Mean Loss 0.1182\n",
      "CF Training: Epoch 0005 Iter 0092 / 0490 | Time 0.1s | Iter Loss 0.0910 | Iter Mean Loss 0.1179\n",
      "CF Training: Epoch 0005 Iter 0093 / 0490 | Time 0.1s | Iter Loss 0.1037 | Iter Mean Loss 0.1178\n",
      "CF Training: Epoch 0005 Iter 0094 / 0490 | Time 0.1s | Iter Loss 0.1061 | Iter Mean Loss 0.1177\n",
      "CF Training: Epoch 0005 Iter 0095 / 0490 | Time 0.1s | Iter Loss 0.1276 | Iter Mean Loss 0.1178\n",
      "CF Training: Epoch 0005 Iter 0096 / 0490 | Time 0.1s | Iter Loss 0.0979 | Iter Mean Loss 0.1176\n",
      "CF Training: Epoch 0005 Iter 0097 / 0490 | Time 0.1s | Iter Loss 0.0991 | Iter Mean Loss 0.1174\n",
      "CF Training: Epoch 0005 Iter 0098 / 0490 | Time 0.1s | Iter Loss 0.1209 | Iter Mean Loss 0.1174\n",
      "CF Training: Epoch 0005 Iter 0099 / 0490 | Time 0.1s | Iter Loss 0.1303 | Iter Mean Loss 0.1175\n",
      "CF Training: Epoch 0005 Iter 0100 / 0490 | Time 0.1s | Iter Loss 0.1067 | Iter Mean Loss 0.1174\n",
      "CF Training: Epoch 0005 Iter 0101 / 0490 | Time 0.1s | Iter Loss 0.1339 | Iter Mean Loss 0.1176\n",
      "CF Training: Epoch 0005 Iter 0102 / 0490 | Time 0.1s | Iter Loss 0.0953 | Iter Mean Loss 0.1174\n",
      "CF Training: Epoch 0005 Iter 0103 / 0490 | Time 0.1s | Iter Loss 0.1363 | Iter Mean Loss 0.1176\n",
      "CF Training: Epoch 0005 Iter 0104 / 0490 | Time 0.1s | Iter Loss 0.1333 | Iter Mean Loss 0.1177\n",
      "CF Training: Epoch 0005 Iter 0105 / 0490 | Time 0.1s | Iter Loss 0.1079 | Iter Mean Loss 0.1176\n",
      "CF Training: Epoch 0005 Iter 0106 / 0490 | Time 0.1s | Iter Loss 0.1403 | Iter Mean Loss 0.1178\n",
      "CF Training: Epoch 0005 Iter 0107 / 0490 | Time 0.0s | Iter Loss 0.1031 | Iter Mean Loss 0.1177\n",
      "CF Training: Epoch 0005 Iter 0108 / 0490 | Time 0.1s | Iter Loss 0.1167 | Iter Mean Loss 0.1177\n",
      "CF Training: Epoch 0005 Iter 0109 / 0490 | Time 0.1s | Iter Loss 0.0733 | Iter Mean Loss 0.1173\n",
      "CF Training: Epoch 0005 Iter 0110 / 0490 | Time 0.1s | Iter Loss 0.0996 | Iter Mean Loss 0.1171\n",
      "CF Training: Epoch 0005 Iter 0111 / 0490 | Time 0.1s | Iter Loss 0.0911 | Iter Mean Loss 0.1169\n",
      "CF Training: Epoch 0005 Iter 0112 / 0490 | Time 0.1s | Iter Loss 0.1450 | Iter Mean Loss 0.1171\n",
      "CF Training: Epoch 0005 Iter 0113 / 0490 | Time 0.1s | Iter Loss 0.1285 | Iter Mean Loss 0.1172\n",
      "CF Training: Epoch 0005 Iter 0114 / 0490 | Time 0.1s | Iter Loss 0.1106 | Iter Mean Loss 0.1172\n",
      "CF Training: Epoch 0005 Iter 0115 / 0490 | Time 0.1s | Iter Loss 0.1010 | Iter Mean Loss 0.1170\n",
      "CF Training: Epoch 0005 Iter 0116 / 0490 | Time 0.1s | Iter Loss 0.1190 | Iter Mean Loss 0.1171\n",
      "CF Training: Epoch 0005 Iter 0117 / 0490 | Time 0.1s | Iter Loss 0.1227 | Iter Mean Loss 0.1171\n",
      "CF Training: Epoch 0005 Iter 0118 / 0490 | Time 0.1s | Iter Loss 0.1119 | Iter Mean Loss 0.1171\n",
      "CF Training: Epoch 0005 Iter 0119 / 0490 | Time 0.1s | Iter Loss 0.1256 | Iter Mean Loss 0.1171\n",
      "CF Training: Epoch 0005 Iter 0120 / 0490 | Time 0.1s | Iter Loss 0.0967 | Iter Mean Loss 0.1170\n",
      "CF Training: Epoch 0005 Iter 0121 / 0490 | Time 0.1s | Iter Loss 0.1044 | Iter Mean Loss 0.1169\n",
      "CF Training: Epoch 0005 Iter 0122 / 0490 | Time 0.1s | Iter Loss 0.0997 | Iter Mean Loss 0.1167\n",
      "CF Training: Epoch 0005 Iter 0123 / 0490 | Time 0.1s | Iter Loss 0.1113 | Iter Mean Loss 0.1167\n",
      "CF Training: Epoch 0005 Iter 0124 / 0490 | Time 0.1s | Iter Loss 0.0968 | Iter Mean Loss 0.1165\n",
      "CF Training: Epoch 0005 Iter 0125 / 0490 | Time 0.1s | Iter Loss 0.1272 | Iter Mean Loss 0.1166\n",
      "CF Training: Epoch 0005 Iter 0126 / 0490 | Time 0.1s | Iter Loss 0.1372 | Iter Mean Loss 0.1168\n",
      "CF Training: Epoch 0005 Iter 0127 / 0490 | Time 0.1s | Iter Loss 0.0960 | Iter Mean Loss 0.1166\n",
      "CF Training: Epoch 0005 Iter 0128 / 0490 | Time 0.1s | Iter Loss 0.1230 | Iter Mean Loss 0.1166\n",
      "CF Training: Epoch 0005 Iter 0129 / 0490 | Time 0.1s | Iter Loss 0.1133 | Iter Mean Loss 0.1166\n",
      "CF Training: Epoch 0005 Iter 0130 / 0490 | Time 0.1s | Iter Loss 0.1017 | Iter Mean Loss 0.1165\n",
      "CF Training: Epoch 0005 Iter 0131 / 0490 | Time 0.0s | Iter Loss 0.0851 | Iter Mean Loss 0.1163\n",
      "CF Training: Epoch 0005 Iter 0132 / 0490 | Time 0.1s | Iter Loss 0.1147 | Iter Mean Loss 0.1163\n",
      "CF Training: Epoch 0005 Iter 0133 / 0490 | Time 0.1s | Iter Loss 0.1491 | Iter Mean Loss 0.1165\n",
      "CF Training: Epoch 0005 Iter 0134 / 0490 | Time 0.1s | Iter Loss 0.1074 | Iter Mean Loss 0.1164\n",
      "CF Training: Epoch 0005 Iter 0135 / 0490 | Time 0.1s | Iter Loss 0.1167 | Iter Mean Loss 0.1164\n",
      "CF Training: Epoch 0005 Iter 0136 / 0490 | Time 0.1s | Iter Loss 0.1372 | Iter Mean Loss 0.1166\n",
      "CF Training: Epoch 0005 Iter 0137 / 0490 | Time 0.1s | Iter Loss 0.0920 | Iter Mean Loss 0.1164\n",
      "CF Training: Epoch 0005 Iter 0138 / 0490 | Time 0.1s | Iter Loss 0.1170 | Iter Mean Loss 0.1164\n",
      "CF Training: Epoch 0005 Iter 0139 / 0490 | Time 0.1s | Iter Loss 0.1645 | Iter Mean Loss 0.1168\n",
      "CF Training: Epoch 0005 Iter 0140 / 0490 | Time 0.1s | Iter Loss 0.0932 | Iter Mean Loss 0.1166\n",
      "CF Training: Epoch 0005 Iter 0141 / 0490 | Time 0.1s | Iter Loss 0.0854 | Iter Mean Loss 0.1164\n",
      "CF Training: Epoch 0005 Iter 0142 / 0490 | Time 0.1s | Iter Loss 0.1109 | Iter Mean Loss 0.1163\n",
      "CF Training: Epoch 0005 Iter 0143 / 0490 | Time 0.1s | Iter Loss 0.1034 | Iter Mean Loss 0.1162\n",
      "CF Training: Epoch 0005 Iter 0144 / 0490 | Time 0.1s | Iter Loss 0.1208 | Iter Mean Loss 0.1163\n",
      "CF Training: Epoch 0005 Iter 0145 / 0490 | Time 0.1s | Iter Loss 0.0983 | Iter Mean Loss 0.1162\n",
      "CF Training: Epoch 0005 Iter 0146 / 0490 | Time 0.1s | Iter Loss 0.1105 | Iter Mean Loss 0.1161\n",
      "CF Training: Epoch 0005 Iter 0147 / 0490 | Time 0.1s | Iter Loss 0.1179 | Iter Mean Loss 0.1161\n",
      "CF Training: Epoch 0005 Iter 0148 / 0490 | Time 0.1s | Iter Loss 0.1135 | Iter Mean Loss 0.1161\n",
      "CF Training: Epoch 0005 Iter 0149 / 0490 | Time 0.1s | Iter Loss 0.0872 | Iter Mean Loss 0.1159\n",
      "CF Training: Epoch 0005 Iter 0150 / 0490 | Time 0.1s | Iter Loss 0.0696 | Iter Mean Loss 0.1156\n",
      "CF Training: Epoch 0005 Iter 0151 / 0490 | Time 0.1s | Iter Loss 0.0973 | Iter Mean Loss 0.1155\n",
      "CF Training: Epoch 0005 Iter 0152 / 0490 | Time 0.1s | Iter Loss 0.0802 | Iter Mean Loss 0.1153\n",
      "CF Training: Epoch 0005 Iter 0153 / 0490 | Time 0.1s | Iter Loss 0.1059 | Iter Mean Loss 0.1152\n",
      "CF Training: Epoch 0005 Iter 0154 / 0490 | Time 0.1s | Iter Loss 0.1401 | Iter Mean Loss 0.1154\n",
      "CF Training: Epoch 0005 Iter 0155 / 0490 | Time 0.1s | Iter Loss 0.0985 | Iter Mean Loss 0.1152\n",
      "CF Training: Epoch 0005 Iter 0156 / 0490 | Time 0.1s | Iter Loss 0.0838 | Iter Mean Loss 0.1150\n",
      "CF Training: Epoch 0005 Iter 0157 / 0490 | Time 0.1s | Iter Loss 0.0956 | Iter Mean Loss 0.1149\n",
      "CF Training: Epoch 0005 Iter 0158 / 0490 | Time 0.1s | Iter Loss 0.1014 | Iter Mean Loss 0.1148\n",
      "CF Training: Epoch 0005 Iter 0159 / 0490 | Time 0.1s | Iter Loss 0.1016 | Iter Mean Loss 0.1147\n",
      "CF Training: Epoch 0005 Iter 0160 / 0490 | Time 0.1s | Iter Loss 0.1282 | Iter Mean Loss 0.1148\n",
      "CF Training: Epoch 0005 Iter 0161 / 0490 | Time 0.1s | Iter Loss 0.0969 | Iter Mean Loss 0.1147\n",
      "CF Training: Epoch 0005 Iter 0162 / 0490 | Time 0.1s | Iter Loss 0.1040 | Iter Mean Loss 0.1147\n",
      "CF Training: Epoch 0005 Iter 0163 / 0490 | Time 0.1s | Iter Loss 0.1135 | Iter Mean Loss 0.1146\n",
      "CF Training: Epoch 0005 Iter 0164 / 0490 | Time 0.1s | Iter Loss 0.0949 | Iter Mean Loss 0.1145\n",
      "CF Training: Epoch 0005 Iter 0165 / 0490 | Time 0.1s | Iter Loss 0.0961 | Iter Mean Loss 0.1144\n",
      "CF Training: Epoch 0005 Iter 0166 / 0490 | Time 0.1s | Iter Loss 0.1359 | Iter Mean Loss 0.1145\n",
      "CF Training: Epoch 0005 Iter 0167 / 0490 | Time 0.1s | Iter Loss 0.0849 | Iter Mean Loss 0.1144\n",
      "CF Training: Epoch 0005 Iter 0168 / 0490 | Time 0.1s | Iter Loss 0.1314 | Iter Mean Loss 0.1145\n",
      "CF Training: Epoch 0005 Iter 0169 / 0490 | Time 0.1s | Iter Loss 0.1530 | Iter Mean Loss 0.1147\n",
      "CF Training: Epoch 0005 Iter 0170 / 0490 | Time 0.1s | Iter Loss 0.0912 | Iter Mean Loss 0.1146\n",
      "CF Training: Epoch 0005 Iter 0171 / 0490 | Time 0.1s | Iter Loss 0.1162 | Iter Mean Loss 0.1146\n",
      "CF Training: Epoch 0005 Iter 0172 / 0490 | Time 0.1s | Iter Loss 0.0981 | Iter Mean Loss 0.1145\n",
      "CF Training: Epoch 0005 Iter 0173 / 0490 | Time 0.0s | Iter Loss 0.1149 | Iter Mean Loss 0.1145\n",
      "CF Training: Epoch 0005 Iter 0174 / 0490 | Time 0.1s | Iter Loss 0.1241 | Iter Mean Loss 0.1145\n",
      "CF Training: Epoch 0005 Iter 0175 / 0490 | Time 0.1s | Iter Loss 0.0805 | Iter Mean Loss 0.1143\n",
      "CF Training: Epoch 0005 Iter 0176 / 0490 | Time 0.1s | Iter Loss 0.1074 | Iter Mean Loss 0.1143\n",
      "CF Training: Epoch 0005 Iter 0177 / 0490 | Time 0.1s | Iter Loss 0.1240 | Iter Mean Loss 0.1144\n",
      "CF Training: Epoch 0005 Iter 0178 / 0490 | Time 0.1s | Iter Loss 0.1332 | Iter Mean Loss 0.1145\n",
      "CF Training: Epoch 0005 Iter 0179 / 0490 | Time 0.1s | Iter Loss 0.1145 | Iter Mean Loss 0.1145\n",
      "CF Training: Epoch 0005 Iter 0180 / 0490 | Time 0.1s | Iter Loss 0.1357 | Iter Mean Loss 0.1146\n",
      "CF Training: Epoch 0005 Iter 0181 / 0490 | Time 0.1s | Iter Loss 0.1124 | Iter Mean Loss 0.1146\n",
      "CF Training: Epoch 0005 Iter 0182 / 0490 | Time 0.1s | Iter Loss 0.1199 | Iter Mean Loss 0.1146\n",
      "CF Training: Epoch 0005 Iter 0183 / 0490 | Time 0.1s | Iter Loss 0.1098 | Iter Mean Loss 0.1146\n",
      "CF Training: Epoch 0005 Iter 0184 / 0490 | Time 0.1s | Iter Loss 0.1321 | Iter Mean Loss 0.1147\n",
      "CF Training: Epoch 0005 Iter 0185 / 0490 | Time 0.1s | Iter Loss 0.0901 | Iter Mean Loss 0.1145\n",
      "CF Training: Epoch 0005 Iter 0186 / 0490 | Time 0.1s | Iter Loss 0.1105 | Iter Mean Loss 0.1145\n",
      "CF Training: Epoch 0005 Iter 0187 / 0490 | Time 0.1s | Iter Loss 0.0918 | Iter Mean Loss 0.1144\n",
      "CF Training: Epoch 0005 Iter 0188 / 0490 | Time 0.1s | Iter Loss 0.0836 | Iter Mean Loss 0.1142\n",
      "CF Training: Epoch 0005 Iter 0189 / 0490 | Time 0.1s | Iter Loss 0.1251 | Iter Mean Loss 0.1143\n",
      "CF Training: Epoch 0005 Iter 0190 / 0490 | Time 0.1s | Iter Loss 0.0832 | Iter Mean Loss 0.1141\n",
      "CF Training: Epoch 0005 Iter 0191 / 0490 | Time 0.1s | Iter Loss 0.0810 | Iter Mean Loss 0.1139\n",
      "CF Training: Epoch 0005 Iter 0192 / 0490 | Time 0.1s | Iter Loss 0.1601 | Iter Mean Loss 0.1142\n",
      "CF Training: Epoch 0005 Iter 0193 / 0490 | Time 0.1s | Iter Loss 0.0978 | Iter Mean Loss 0.1141\n",
      "CF Training: Epoch 0005 Iter 0194 / 0490 | Time 0.1s | Iter Loss 0.1225 | Iter Mean Loss 0.1141\n",
      "CF Training: Epoch 0005 Iter 0195 / 0490 | Time 0.1s | Iter Loss 0.0894 | Iter Mean Loss 0.1140\n",
      "CF Training: Epoch 0005 Iter 0196 / 0490 | Time 0.1s | Iter Loss 0.1033 | Iter Mean Loss 0.1140\n",
      "CF Training: Epoch 0005 Iter 0197 / 0490 | Time 0.1s | Iter Loss 0.1058 | Iter Mean Loss 0.1139\n",
      "CF Training: Epoch 0005 Iter 0198 / 0490 | Time 0.1s | Iter Loss 0.1212 | Iter Mean Loss 0.1140\n",
      "CF Training: Epoch 0005 Iter 0199 / 0490 | Time 0.1s | Iter Loss 0.1055 | Iter Mean Loss 0.1139\n",
      "CF Training: Epoch 0005 Iter 0200 / 0490 | Time 0.1s | Iter Loss 0.1525 | Iter Mean Loss 0.1141\n",
      "CF Training: Epoch 0005 Iter 0201 / 0490 | Time 0.1s | Iter Loss 0.1343 | Iter Mean Loss 0.1142\n",
      "CF Training: Epoch 0005 Iter 0202 / 0490 | Time 0.1s | Iter Loss 0.1074 | Iter Mean Loss 0.1142\n",
      "CF Training: Epoch 0005 Iter 0203 / 0490 | Time 0.1s | Iter Loss 0.1304 | Iter Mean Loss 0.1143\n",
      "CF Training: Epoch 0005 Iter 0204 / 0490 | Time 0.1s | Iter Loss 0.1336 | Iter Mean Loss 0.1143\n",
      "CF Training: Epoch 0005 Iter 0205 / 0490 | Time 0.1s | Iter Loss 0.1354 | Iter Mean Loss 0.1144\n",
      "CF Training: Epoch 0005 Iter 0206 / 0490 | Time 0.1s | Iter Loss 0.0865 | Iter Mean Loss 0.1143\n",
      "CF Training: Epoch 0005 Iter 0207 / 0490 | Time 0.1s | Iter Loss 0.0975 | Iter Mean Loss 0.1142\n",
      "CF Training: Epoch 0005 Iter 0208 / 0490 | Time 0.1s | Iter Loss 0.1154 | Iter Mean Loss 0.1142\n",
      "CF Training: Epoch 0005 Iter 0209 / 0490 | Time 0.1s | Iter Loss 0.1052 | Iter Mean Loss 0.1142\n",
      "CF Training: Epoch 0005 Iter 0210 / 0490 | Time 0.1s | Iter Loss 0.0858 | Iter Mean Loss 0.1141\n",
      "CF Training: Epoch 0005 Iter 0211 / 0490 | Time 0.1s | Iter Loss 0.1459 | Iter Mean Loss 0.1142\n",
      "CF Training: Epoch 0005 Iter 0212 / 0490 | Time 0.1s | Iter Loss 0.1381 | Iter Mean Loss 0.1143\n",
      "CF Training: Epoch 0005 Iter 0213 / 0490 | Time 0.1s | Iter Loss 0.0923 | Iter Mean Loss 0.1142\n",
      "CF Training: Epoch 0005 Iter 0214 / 0490 | Time 0.1s | Iter Loss 0.0939 | Iter Mean Loss 0.1141\n",
      "CF Training: Epoch 0005 Iter 0215 / 0490 | Time 0.1s | Iter Loss 0.1153 | Iter Mean Loss 0.1141\n",
      "CF Training: Epoch 0005 Iter 0216 / 0490 | Time 0.1s | Iter Loss 0.0839 | Iter Mean Loss 0.1140\n",
      "CF Training: Epoch 0005 Iter 0217 / 0490 | Time 0.1s | Iter Loss 0.0999 | Iter Mean Loss 0.1139\n",
      "CF Training: Epoch 0005 Iter 0218 / 0490 | Time 0.1s | Iter Loss 0.1231 | Iter Mean Loss 0.1140\n",
      "CF Training: Epoch 0005 Iter 0219 / 0490 | Time 0.1s | Iter Loss 0.1213 | Iter Mean Loss 0.1140\n",
      "CF Training: Epoch 0005 Iter 0220 / 0490 | Time 0.1s | Iter Loss 0.0904 | Iter Mean Loss 0.1139\n",
      "CF Training: Epoch 0005 Iter 0221 / 0490 | Time 0.1s | Iter Loss 0.1079 | Iter Mean Loss 0.1139\n",
      "CF Training: Epoch 0005 Iter 0222 / 0490 | Time 0.1s | Iter Loss 0.1287 | Iter Mean Loss 0.1139\n",
      "CF Training: Epoch 0005 Iter 0223 / 0490 | Time 0.1s | Iter Loss 0.0940 | Iter Mean Loss 0.1138\n",
      "CF Training: Epoch 0005 Iter 0224 / 0490 | Time 0.1s | Iter Loss 0.0961 | Iter Mean Loss 0.1138\n",
      "CF Training: Epoch 0005 Iter 0225 / 0490 | Time 0.1s | Iter Loss 0.0980 | Iter Mean Loss 0.1137\n",
      "CF Training: Epoch 0005 Iter 0226 / 0490 | Time 0.1s | Iter Loss 0.1218 | Iter Mean Loss 0.1137\n",
      "CF Training: Epoch 0005 Iter 0227 / 0490 | Time 0.1s | Iter Loss 0.0947 | Iter Mean Loss 0.1136\n",
      "CF Training: Epoch 0005 Iter 0228 / 0490 | Time 0.1s | Iter Loss 0.0992 | Iter Mean Loss 0.1136\n",
      "CF Training: Epoch 0005 Iter 0229 / 0490 | Time 0.1s | Iter Loss 0.1219 | Iter Mean Loss 0.1136\n",
      "CF Training: Epoch 0005 Iter 0230 / 0490 | Time 0.1s | Iter Loss 0.1039 | Iter Mean Loss 0.1136\n",
      "CF Training: Epoch 0005 Iter 0231 / 0490 | Time 0.1s | Iter Loss 0.1091 | Iter Mean Loss 0.1136\n",
      "CF Training: Epoch 0005 Iter 0232 / 0490 | Time 0.1s | Iter Loss 0.0977 | Iter Mean Loss 0.1135\n",
      "CF Training: Epoch 0005 Iter 0233 / 0490 | Time 0.1s | Iter Loss 0.1307 | Iter Mean Loss 0.1136\n",
      "CF Training: Epoch 0005 Iter 0234 / 0490 | Time 0.1s | Iter Loss 0.0877 | Iter Mean Loss 0.1135\n",
      "CF Training: Epoch 0005 Iter 0235 / 0490 | Time 0.1s | Iter Loss 0.1061 | Iter Mean Loss 0.1134\n",
      "CF Training: Epoch 0005 Iter 0236 / 0490 | Time 0.1s | Iter Loss 0.1468 | Iter Mean Loss 0.1136\n",
      "CF Training: Epoch 0005 Iter 0237 / 0490 | Time 0.1s | Iter Loss 0.0992 | Iter Mean Loss 0.1135\n",
      "CF Training: Epoch 0005 Iter 0238 / 0490 | Time 0.1s | Iter Loss 0.1183 | Iter Mean Loss 0.1135\n",
      "CF Training: Epoch 0005 Iter 0239 / 0490 | Time 0.1s | Iter Loss 0.0886 | Iter Mean Loss 0.1134\n",
      "CF Training: Epoch 0005 Iter 0240 / 0490 | Time 0.1s | Iter Loss 0.1402 | Iter Mean Loss 0.1135\n",
      "CF Training: Epoch 0005 Iter 0241 / 0490 | Time 0.1s | Iter Loss 0.0744 | Iter Mean Loss 0.1134\n",
      "CF Training: Epoch 0005 Iter 0242 / 0490 | Time 0.1s | Iter Loss 0.1020 | Iter Mean Loss 0.1133\n",
      "CF Training: Epoch 0005 Iter 0243 / 0490 | Time 0.1s | Iter Loss 0.0943 | Iter Mean Loss 0.1132\n",
      "CF Training: Epoch 0005 Iter 0244 / 0490 | Time 0.1s | Iter Loss 0.1042 | Iter Mean Loss 0.1132\n",
      "CF Training: Epoch 0005 Iter 0245 / 0490 | Time 0.1s | Iter Loss 0.1168 | Iter Mean Loss 0.1132\n",
      "CF Training: Epoch 0005 Iter 0246 / 0490 | Time 0.1s | Iter Loss 0.0981 | Iter Mean Loss 0.1132\n",
      "CF Training: Epoch 0005 Iter 0247 / 0490 | Time 0.1s | Iter Loss 0.1158 | Iter Mean Loss 0.1132\n",
      "CF Training: Epoch 0005 Iter 0248 / 0490 | Time 0.1s | Iter Loss 0.1153 | Iter Mean Loss 0.1132\n",
      "CF Training: Epoch 0005 Iter 0249 / 0490 | Time 0.1s | Iter Loss 0.1441 | Iter Mean Loss 0.1133\n",
      "CF Training: Epoch 0005 Iter 0250 / 0490 | Time 0.1s | Iter Loss 0.1106 | Iter Mean Loss 0.1133\n",
      "CF Training: Epoch 0005 Iter 0251 / 0490 | Time 0.1s | Iter Loss 0.1550 | Iter Mean Loss 0.1135\n",
      "CF Training: Epoch 0005 Iter 0252 / 0490 | Time 0.1s | Iter Loss 0.0912 | Iter Mean Loss 0.1134\n",
      "CF Training: Epoch 0005 Iter 0253 / 0490 | Time 0.1s | Iter Loss 0.1060 | Iter Mean Loss 0.1133\n",
      "CF Training: Epoch 0005 Iter 0254 / 0490 | Time 0.1s | Iter Loss 0.1057 | Iter Mean Loss 0.1133\n",
      "CF Training: Epoch 0005 Iter 0255 / 0490 | Time 0.1s | Iter Loss 0.1097 | Iter Mean Loss 0.1133\n",
      "CF Training: Epoch 0005 Iter 0256 / 0490 | Time 0.1s | Iter Loss 0.1093 | Iter Mean Loss 0.1133\n",
      "CF Training: Epoch 0005 Iter 0257 / 0490 | Time 0.1s | Iter Loss 0.1414 | Iter Mean Loss 0.1134\n",
      "CF Training: Epoch 0005 Iter 0258 / 0490 | Time 0.1s | Iter Loss 0.1274 | Iter Mean Loss 0.1134\n",
      "CF Training: Epoch 0005 Iter 0259 / 0490 | Time 0.1s | Iter Loss 0.0934 | Iter Mean Loss 0.1134\n",
      "CF Training: Epoch 0005 Iter 0260 / 0490 | Time 0.1s | Iter Loss 0.1277 | Iter Mean Loss 0.1134\n",
      "CF Training: Epoch 0005 Iter 0261 / 0490 | Time 0.1s | Iter Loss 0.1288 | Iter Mean Loss 0.1135\n",
      "CF Training: Epoch 0005 Iter 0262 / 0490 | Time 0.1s | Iter Loss 0.1515 | Iter Mean Loss 0.1136\n",
      "CF Training: Epoch 0005 Iter 0263 / 0490 | Time 0.1s | Iter Loss 0.1036 | Iter Mean Loss 0.1136\n",
      "CF Training: Epoch 0005 Iter 0264 / 0490 | Time 0.1s | Iter Loss 0.1284 | Iter Mean Loss 0.1136\n",
      "CF Training: Epoch 0005 Iter 0265 / 0490 | Time 0.1s | Iter Loss 0.1500 | Iter Mean Loss 0.1138\n",
      "CF Training: Epoch 0005 Iter 0266 / 0490 | Time 0.1s | Iter Loss 0.0800 | Iter Mean Loss 0.1137\n",
      "CF Training: Epoch 0005 Iter 0267 / 0490 | Time 0.1s | Iter Loss 0.1066 | Iter Mean Loss 0.1136\n",
      "CF Training: Epoch 0005 Iter 0268 / 0490 | Time 0.1s | Iter Loss 0.0800 | Iter Mean Loss 0.1135\n",
      "CF Training: Epoch 0005 Iter 0269 / 0490 | Time 0.1s | Iter Loss 0.1356 | Iter Mean Loss 0.1136\n",
      "CF Training: Epoch 0005 Iter 0270 / 0490 | Time 0.1s | Iter Loss 0.0949 | Iter Mean Loss 0.1135\n",
      "CF Training: Epoch 0005 Iter 0271 / 0490 | Time 0.1s | Iter Loss 0.1526 | Iter Mean Loss 0.1137\n",
      "CF Training: Epoch 0005 Iter 0272 / 0490 | Time 0.1s | Iter Loss 0.1200 | Iter Mean Loss 0.1137\n",
      "CF Training: Epoch 0005 Iter 0273 / 0490 | Time 0.1s | Iter Loss 0.1171 | Iter Mean Loss 0.1137\n",
      "CF Training: Epoch 0005 Iter 0274 / 0490 | Time 0.1s | Iter Loss 0.1042 | Iter Mean Loss 0.1137\n",
      "CF Training: Epoch 0005 Iter 0275 / 0490 | Time 0.1s | Iter Loss 0.1271 | Iter Mean Loss 0.1137\n",
      "CF Training: Epoch 0005 Iter 0276 / 0490 | Time 0.1s | Iter Loss 0.0960 | Iter Mean Loss 0.1136\n",
      "CF Training: Epoch 0005 Iter 0277 / 0490 | Time 0.1s | Iter Loss 0.1028 | Iter Mean Loss 0.1136\n",
      "CF Training: Epoch 0005 Iter 0278 / 0490 | Time 0.1s | Iter Loss 0.1727 | Iter Mean Loss 0.1138\n",
      "CF Training: Epoch 0005 Iter 0279 / 0490 | Time 0.1s | Iter Loss 0.1207 | Iter Mean Loss 0.1138\n",
      "CF Training: Epoch 0005 Iter 0280 / 0490 | Time 0.1s | Iter Loss 0.1130 | Iter Mean Loss 0.1138\n",
      "CF Training: Epoch 0005 Iter 0281 / 0490 | Time 0.1s | Iter Loss 0.1312 | Iter Mean Loss 0.1139\n",
      "CF Training: Epoch 0005 Iter 0282 / 0490 | Time 0.1s | Iter Loss 0.0921 | Iter Mean Loss 0.1138\n",
      "CF Training: Epoch 0005 Iter 0283 / 0490 | Time 0.1s | Iter Loss 0.1138 | Iter Mean Loss 0.1138\n",
      "CF Training: Epoch 0005 Iter 0284 / 0490 | Time 0.1s | Iter Loss 0.1144 | Iter Mean Loss 0.1138\n",
      "CF Training: Epoch 0005 Iter 0285 / 0490 | Time 0.1s | Iter Loss 0.0746 | Iter Mean Loss 0.1137\n",
      "CF Training: Epoch 0005 Iter 0286 / 0490 | Time 0.1s | Iter Loss 0.0987 | Iter Mean Loss 0.1136\n",
      "CF Training: Epoch 0005 Iter 0287 / 0490 | Time 0.1s | Iter Loss 0.1061 | Iter Mean Loss 0.1136\n",
      "CF Training: Epoch 0005 Iter 0288 / 0490 | Time 0.1s | Iter Loss 0.0962 | Iter Mean Loss 0.1135\n",
      "CF Training: Epoch 0005 Iter 0289 / 0490 | Time 0.1s | Iter Loss 0.0926 | Iter Mean Loss 0.1135\n",
      "CF Training: Epoch 0005 Iter 0290 / 0490 | Time 0.1s | Iter Loss 0.1287 | Iter Mean Loss 0.1135\n",
      "CF Training: Epoch 0005 Iter 0291 / 0490 | Time 0.1s | Iter Loss 0.1053 | Iter Mean Loss 0.1135\n",
      "CF Training: Epoch 0005 Iter 0292 / 0490 | Time 0.1s | Iter Loss 0.0835 | Iter Mean Loss 0.1134\n",
      "CF Training: Epoch 0005 Iter 0293 / 0490 | Time 0.1s | Iter Loss 0.1559 | Iter Mean Loss 0.1135\n",
      "CF Training: Epoch 0005 Iter 0294 / 0490 | Time 0.1s | Iter Loss 0.0839 | Iter Mean Loss 0.1134\n",
      "CF Training: Epoch 0005 Iter 0295 / 0490 | Time 0.1s | Iter Loss 0.1047 | Iter Mean Loss 0.1134\n",
      "CF Training: Epoch 0005 Iter 0296 / 0490 | Time 0.1s | Iter Loss 0.0968 | Iter Mean Loss 0.1134\n",
      "CF Training: Epoch 0005 Iter 0297 / 0490 | Time 0.1s | Iter Loss 0.1108 | Iter Mean Loss 0.1133\n",
      "CF Training: Epoch 0005 Iter 0298 / 0490 | Time 0.1s | Iter Loss 0.0833 | Iter Mean Loss 0.1132\n",
      "CF Training: Epoch 0005 Iter 0299 / 0490 | Time 0.1s | Iter Loss 0.1139 | Iter Mean Loss 0.1132\n",
      "CF Training: Epoch 0005 Iter 0300 / 0490 | Time 0.1s | Iter Loss 0.0898 | Iter Mean Loss 0.1132\n",
      "CF Training: Epoch 0005 Iter 0301 / 0490 | Time 0.1s | Iter Loss 0.1373 | Iter Mean Loss 0.1133\n",
      "CF Training: Epoch 0005 Iter 0302 / 0490 | Time 0.1s | Iter Loss 0.1378 | Iter Mean Loss 0.1133\n",
      "CF Training: Epoch 0005 Iter 0303 / 0490 | Time 0.1s | Iter Loss 0.0938 | Iter Mean Loss 0.1133\n",
      "CF Training: Epoch 0005 Iter 0304 / 0490 | Time 0.1s | Iter Loss 0.0942 | Iter Mean Loss 0.1132\n",
      "CF Training: Epoch 0005 Iter 0305 / 0490 | Time 0.1s | Iter Loss 0.0882 | Iter Mean Loss 0.1131\n",
      "CF Training: Epoch 0005 Iter 0306 / 0490 | Time 0.1s | Iter Loss 0.1274 | Iter Mean Loss 0.1132\n",
      "CF Training: Epoch 0005 Iter 0307 / 0490 | Time 0.1s | Iter Loss 0.1292 | Iter Mean Loss 0.1132\n",
      "CF Training: Epoch 0005 Iter 0308 / 0490 | Time 0.1s | Iter Loss 0.1677 | Iter Mean Loss 0.1134\n",
      "CF Training: Epoch 0005 Iter 0309 / 0490 | Time 0.1s | Iter Loss 0.0917 | Iter Mean Loss 0.1133\n",
      "CF Training: Epoch 0005 Iter 0310 / 0490 | Time 0.1s | Iter Loss 0.0743 | Iter Mean Loss 0.1132\n",
      "CF Training: Epoch 0005 Iter 0311 / 0490 | Time 0.1s | Iter Loss 0.1305 | Iter Mean Loss 0.1133\n",
      "CF Training: Epoch 0005 Iter 0312 / 0490 | Time 0.1s | Iter Loss 0.1226 | Iter Mean Loss 0.1133\n",
      "CF Training: Epoch 0005 Iter 0313 / 0490 | Time 0.1s | Iter Loss 0.1238 | Iter Mean Loss 0.1133\n",
      "CF Training: Epoch 0005 Iter 0314 / 0490 | Time 0.1s | Iter Loss 0.0767 | Iter Mean Loss 0.1132\n",
      "CF Training: Epoch 0005 Iter 0315 / 0490 | Time 0.1s | Iter Loss 0.1026 | Iter Mean Loss 0.1132\n",
      "CF Training: Epoch 0005 Iter 0316 / 0490 | Time 0.1s | Iter Loss 0.0988 | Iter Mean Loss 0.1131\n",
      "CF Training: Epoch 0005 Iter 0317 / 0490 | Time 0.1s | Iter Loss 0.1016 | Iter Mean Loss 0.1131\n",
      "CF Training: Epoch 0005 Iter 0318 / 0490 | Time 0.1s | Iter Loss 0.1031 | Iter Mean Loss 0.1131\n",
      "CF Training: Epoch 0005 Iter 0319 / 0490 | Time 0.1s | Iter Loss 0.0736 | Iter Mean Loss 0.1129\n",
      "CF Training: Epoch 0005 Iter 0320 / 0490 | Time 0.1s | Iter Loss 0.0914 | Iter Mean Loss 0.1129\n",
      "CF Training: Epoch 0005 Iter 0321 / 0490 | Time 0.1s | Iter Loss 0.0967 | Iter Mean Loss 0.1128\n",
      "CF Training: Epoch 0005 Iter 0322 / 0490 | Time 0.1s | Iter Loss 0.1192 | Iter Mean Loss 0.1128\n",
      "CF Training: Epoch 0005 Iter 0323 / 0490 | Time 0.1s | Iter Loss 0.1218 | Iter Mean Loss 0.1129\n",
      "CF Training: Epoch 0005 Iter 0324 / 0490 | Time 0.1s | Iter Loss 0.0983 | Iter Mean Loss 0.1128\n",
      "CF Training: Epoch 0005 Iter 0325 / 0490 | Time 0.0s | Iter Loss 0.0790 | Iter Mean Loss 0.1127\n",
      "CF Training: Epoch 0005 Iter 0326 / 0490 | Time 0.1s | Iter Loss 0.0924 | Iter Mean Loss 0.1127\n",
      "CF Training: Epoch 0005 Iter 0327 / 0490 | Time 0.1s | Iter Loss 0.0910 | Iter Mean Loss 0.1126\n",
      "CF Training: Epoch 0005 Iter 0328 / 0490 | Time 0.1s | Iter Loss 0.0747 | Iter Mean Loss 0.1125\n",
      "CF Training: Epoch 0005 Iter 0329 / 0490 | Time 0.1s | Iter Loss 0.1197 | Iter Mean Loss 0.1125\n",
      "CF Training: Epoch 0005 Iter 0330 / 0490 | Time 0.1s | Iter Loss 0.1273 | Iter Mean Loss 0.1125\n",
      "CF Training: Epoch 0005 Iter 0331 / 0490 | Time 0.1s | Iter Loss 0.1342 | Iter Mean Loss 0.1126\n",
      "CF Training: Epoch 0005 Iter 0332 / 0490 | Time 0.1s | Iter Loss 0.1232 | Iter Mean Loss 0.1126\n",
      "CF Training: Epoch 0005 Iter 0333 / 0490 | Time 0.1s | Iter Loss 0.1357 | Iter Mean Loss 0.1127\n",
      "CF Training: Epoch 0005 Iter 0334 / 0490 | Time 0.1s | Iter Loss 0.1318 | Iter Mean Loss 0.1128\n",
      "CF Training: Epoch 0005 Iter 0335 / 0490 | Time 0.1s | Iter Loss 0.0903 | Iter Mean Loss 0.1127\n",
      "CF Training: Epoch 0005 Iter 0336 / 0490 | Time 0.1s | Iter Loss 0.1243 | Iter Mean Loss 0.1127\n",
      "CF Training: Epoch 0005 Iter 0337 / 0490 | Time 0.1s | Iter Loss 0.0797 | Iter Mean Loss 0.1126\n",
      "CF Training: Epoch 0005 Iter 0338 / 0490 | Time 0.1s | Iter Loss 0.0840 | Iter Mean Loss 0.1125\n",
      "CF Training: Epoch 0005 Iter 0339 / 0490 | Time 0.1s | Iter Loss 0.1062 | Iter Mean Loss 0.1125\n",
      "CF Training: Epoch 0005 Iter 0340 / 0490 | Time 0.1s | Iter Loss 0.0741 | Iter Mean Loss 0.1124\n",
      "CF Training: Epoch 0005 Iter 0341 / 0490 | Time 0.1s | Iter Loss 0.1224 | Iter Mean Loss 0.1124\n",
      "CF Training: Epoch 0005 Iter 0342 / 0490 | Time 0.0s | Iter Loss 0.0868 | Iter Mean Loss 0.1124\n",
      "CF Training: Epoch 0005 Iter 0343 / 0490 | Time 0.1s | Iter Loss 0.0934 | Iter Mean Loss 0.1123\n",
      "CF Training: Epoch 0005 Iter 0344 / 0490 | Time 0.1s | Iter Loss 0.1075 | Iter Mean Loss 0.1123\n",
      "CF Training: Epoch 0005 Iter 0345 / 0490 | Time 0.1s | Iter Loss 0.0772 | Iter Mean Loss 0.1122\n",
      "CF Training: Epoch 0005 Iter 0346 / 0490 | Time 0.1s | Iter Loss 0.1136 | Iter Mean Loss 0.1122\n",
      "CF Training: Epoch 0005 Iter 0347 / 0490 | Time 0.1s | Iter Loss 0.1237 | Iter Mean Loss 0.1122\n",
      "CF Training: Epoch 0005 Iter 0348 / 0490 | Time 0.1s | Iter Loss 0.1091 | Iter Mean Loss 0.1122\n",
      "CF Training: Epoch 0005 Iter 0349 / 0490 | Time 0.1s | Iter Loss 0.1080 | Iter Mean Loss 0.1122\n",
      "CF Training: Epoch 0005 Iter 0350 / 0490 | Time 0.1s | Iter Loss 0.1073 | Iter Mean Loss 0.1122\n",
      "CF Training: Epoch 0005 Iter 0351 / 0490 | Time 0.1s | Iter Loss 0.0942 | Iter Mean Loss 0.1122\n",
      "CF Training: Epoch 0005 Iter 0352 / 0490 | Time 0.1s | Iter Loss 0.0956 | Iter Mean Loss 0.1121\n",
      "CF Training: Epoch 0005 Iter 0353 / 0490 | Time 0.1s | Iter Loss 0.0926 | Iter Mean Loss 0.1120\n",
      "CF Training: Epoch 0005 Iter 0354 / 0490 | Time 0.1s | Iter Loss 0.1404 | Iter Mean Loss 0.1121\n",
      "CF Training: Epoch 0005 Iter 0355 / 0490 | Time 0.1s | Iter Loss 0.1239 | Iter Mean Loss 0.1122\n",
      "CF Training: Epoch 0005 Iter 0356 / 0490 | Time 0.1s | Iter Loss 0.0816 | Iter Mean Loss 0.1121\n",
      "CF Training: Epoch 0005 Iter 0357 / 0490 | Time 0.1s | Iter Loss 0.1029 | Iter Mean Loss 0.1120\n",
      "CF Training: Epoch 0005 Iter 0358 / 0490 | Time 0.1s | Iter Loss 0.0928 | Iter Mean Loss 0.1120\n",
      "CF Training: Epoch 0005 Iter 0359 / 0490 | Time 0.1s | Iter Loss 0.1145 | Iter Mean Loss 0.1120\n",
      "CF Training: Epoch 0005 Iter 0360 / 0490 | Time 0.1s | Iter Loss 0.1036 | Iter Mean Loss 0.1120\n",
      "CF Training: Epoch 0005 Iter 0361 / 0490 | Time 0.1s | Iter Loss 0.0917 | Iter Mean Loss 0.1119\n",
      "CF Training: Epoch 0005 Iter 0362 / 0490 | Time 0.1s | Iter Loss 0.1042 | Iter Mean Loss 0.1119\n",
      "CF Training: Epoch 0005 Iter 0363 / 0490 | Time 0.1s | Iter Loss 0.1263 | Iter Mean Loss 0.1119\n",
      "CF Training: Epoch 0005 Iter 0364 / 0490 | Time 0.1s | Iter Loss 0.0829 | Iter Mean Loss 0.1119\n",
      "CF Training: Epoch 0005 Iter 0365 / 0490 | Time 0.1s | Iter Loss 0.1196 | Iter Mean Loss 0.1119\n",
      "CF Training: Epoch 0005 Iter 0366 / 0490 | Time 0.1s | Iter Loss 0.1203 | Iter Mean Loss 0.1119\n",
      "CF Training: Epoch 0005 Iter 0367 / 0490 | Time 0.1s | Iter Loss 0.1140 | Iter Mean Loss 0.1119\n",
      "CF Training: Epoch 0005 Iter 0368 / 0490 | Time 0.1s | Iter Loss 0.0788 | Iter Mean Loss 0.1118\n",
      "CF Training: Epoch 0005 Iter 0369 / 0490 | Time 0.1s | Iter Loss 0.1233 | Iter Mean Loss 0.1119\n",
      "CF Training: Epoch 0005 Iter 0370 / 0490 | Time 0.1s | Iter Loss 0.1061 | Iter Mean Loss 0.1118\n",
      "CF Training: Epoch 0005 Iter 0371 / 0490 | Time 0.1s | Iter Loss 0.0988 | Iter Mean Loss 0.1118\n",
      "CF Training: Epoch 0005 Iter 0372 / 0490 | Time 0.1s | Iter Loss 0.1087 | Iter Mean Loss 0.1118\n",
      "CF Training: Epoch 0005 Iter 0373 / 0490 | Time 0.1s | Iter Loss 0.1018 | Iter Mean Loss 0.1118\n",
      "CF Training: Epoch 0005 Iter 0374 / 0490 | Time 0.1s | Iter Loss 0.1318 | Iter Mean Loss 0.1118\n",
      "CF Training: Epoch 0005 Iter 0375 / 0490 | Time 0.0s | Iter Loss 0.0927 | Iter Mean Loss 0.1118\n",
      "CF Training: Epoch 0005 Iter 0376 / 0490 | Time 0.1s | Iter Loss 0.1089 | Iter Mean Loss 0.1118\n",
      "CF Training: Epoch 0005 Iter 0377 / 0490 | Time 0.1s | Iter Loss 0.1043 | Iter Mean Loss 0.1117\n",
      "CF Training: Epoch 0005 Iter 0378 / 0490 | Time 0.1s | Iter Loss 0.1084 | Iter Mean Loss 0.1117\n",
      "CF Training: Epoch 0005 Iter 0379 / 0490 | Time 0.1s | Iter Loss 0.1018 | Iter Mean Loss 0.1117\n",
      "CF Training: Epoch 0005 Iter 0380 / 0490 | Time 0.1s | Iter Loss 0.0835 | Iter Mean Loss 0.1116\n",
      "CF Training: Epoch 0005 Iter 0381 / 0490 | Time 0.1s | Iter Loss 0.1014 | Iter Mean Loss 0.1116\n",
      "CF Training: Epoch 0005 Iter 0382 / 0490 | Time 0.1s | Iter Loss 0.0952 | Iter Mean Loss 0.1116\n",
      "CF Training: Epoch 0005 Iter 0383 / 0490 | Time 0.1s | Iter Loss 0.1281 | Iter Mean Loss 0.1116\n",
      "CF Training: Epoch 0005 Iter 0384 / 0490 | Time 0.0s | Iter Loss 0.1557 | Iter Mean Loss 0.1117\n",
      "CF Training: Epoch 0005 Iter 0385 / 0490 | Time 0.1s | Iter Loss 0.0997 | Iter Mean Loss 0.1117\n",
      "CF Training: Epoch 0005 Iter 0386 / 0490 | Time 0.1s | Iter Loss 0.0786 | Iter Mean Loss 0.1116\n",
      "CF Training: Epoch 0005 Iter 0387 / 0490 | Time 0.1s | Iter Loss 0.1447 | Iter Mean Loss 0.1117\n",
      "CF Training: Epoch 0005 Iter 0388 / 0490 | Time 0.1s | Iter Loss 0.0954 | Iter Mean Loss 0.1116\n",
      "CF Training: Epoch 0005 Iter 0389 / 0490 | Time 0.1s | Iter Loss 0.0975 | Iter Mean Loss 0.1116\n",
      "CF Training: Epoch 0005 Iter 0390 / 0490 | Time 0.1s | Iter Loss 0.0845 | Iter Mean Loss 0.1115\n",
      "CF Training: Epoch 0005 Iter 0391 / 0490 | Time 0.1s | Iter Loss 0.0732 | Iter Mean Loss 0.1114\n",
      "CF Training: Epoch 0005 Iter 0392 / 0490 | Time 0.1s | Iter Loss 0.1010 | Iter Mean Loss 0.1114\n",
      "CF Training: Epoch 0005 Iter 0393 / 0490 | Time 0.1s | Iter Loss 0.1235 | Iter Mean Loss 0.1114\n",
      "CF Training: Epoch 0005 Iter 0394 / 0490 | Time 0.1s | Iter Loss 0.0989 | Iter Mean Loss 0.1114\n",
      "CF Training: Epoch 0005 Iter 0395 / 0490 | Time 0.1s | Iter Loss 0.1052 | Iter Mean Loss 0.1114\n",
      "CF Training: Epoch 0005 Iter 0396 / 0490 | Time 0.1s | Iter Loss 0.1170 | Iter Mean Loss 0.1114\n",
      "CF Training: Epoch 0005 Iter 0397 / 0490 | Time 0.1s | Iter Loss 0.1561 | Iter Mean Loss 0.1115\n",
      "CF Training: Epoch 0005 Iter 0398 / 0490 | Time 0.1s | Iter Loss 0.1200 | Iter Mean Loss 0.1115\n",
      "CF Training: Epoch 0005 Iter 0399 / 0490 | Time 0.1s | Iter Loss 0.1269 | Iter Mean Loss 0.1116\n",
      "CF Training: Epoch 0005 Iter 0400 / 0490 | Time 0.0s | Iter Loss 0.1033 | Iter Mean Loss 0.1116\n",
      "CF Training: Epoch 0005 Iter 0401 / 0490 | Time 0.1s | Iter Loss 0.1161 | Iter Mean Loss 0.1116\n",
      "CF Training: Epoch 0005 Iter 0402 / 0490 | Time 0.1s | Iter Loss 0.1037 | Iter Mean Loss 0.1116\n",
      "CF Training: Epoch 0005 Iter 0403 / 0490 | Time 0.1s | Iter Loss 0.0944 | Iter Mean Loss 0.1115\n",
      "CF Training: Epoch 0005 Iter 0404 / 0490 | Time 0.1s | Iter Loss 0.0893 | Iter Mean Loss 0.1115\n",
      "CF Training: Epoch 0005 Iter 0405 / 0490 | Time 0.1s | Iter Loss 0.0924 | Iter Mean Loss 0.1114\n",
      "CF Training: Epoch 0005 Iter 0406 / 0490 | Time 0.1s | Iter Loss 0.1109 | Iter Mean Loss 0.1114\n",
      "CF Training: Epoch 0005 Iter 0407 / 0490 | Time 0.1s | Iter Loss 0.1070 | Iter Mean Loss 0.1114\n",
      "CF Training: Epoch 0005 Iter 0408 / 0490 | Time 0.1s | Iter Loss 0.0740 | Iter Mean Loss 0.1113\n",
      "CF Training: Epoch 0005 Iter 0409 / 0490 | Time 0.1s | Iter Loss 0.1395 | Iter Mean Loss 0.1114\n",
      "CF Training: Epoch 0005 Iter 0410 / 0490 | Time 0.0s | Iter Loss 0.1119 | Iter Mean Loss 0.1114\n",
      "CF Training: Epoch 0005 Iter 0411 / 0490 | Time 0.1s | Iter Loss 0.1164 | Iter Mean Loss 0.1114\n",
      "CF Training: Epoch 0005 Iter 0412 / 0490 | Time 0.1s | Iter Loss 0.1256 | Iter Mean Loss 0.1114\n",
      "CF Training: Epoch 0005 Iter 0413 / 0490 | Time 0.1s | Iter Loss 0.1264 | Iter Mean Loss 0.1115\n",
      "CF Training: Epoch 0005 Iter 0414 / 0490 | Time 0.1s | Iter Loss 0.0897 | Iter Mean Loss 0.1114\n",
      "CF Training: Epoch 0005 Iter 0415 / 0490 | Time 0.1s | Iter Loss 0.1033 | Iter Mean Loss 0.1114\n",
      "CF Training: Epoch 0005 Iter 0416 / 0490 | Time 0.1s | Iter Loss 0.1134 | Iter Mean Loss 0.1114\n",
      "CF Training: Epoch 0005 Iter 0417 / 0490 | Time 0.1s | Iter Loss 0.1175 | Iter Mean Loss 0.1114\n",
      "CF Training: Epoch 0005 Iter 0418 / 0490 | Time 0.1s | Iter Loss 0.0774 | Iter Mean Loss 0.1113\n",
      "CF Training: Epoch 0005 Iter 0419 / 0490 | Time 0.1s | Iter Loss 0.0847 | Iter Mean Loss 0.1113\n",
      "CF Training: Epoch 0005 Iter 0420 / 0490 | Time 0.1s | Iter Loss 0.1465 | Iter Mean Loss 0.1113\n",
      "CF Training: Epoch 0005 Iter 0421 / 0490 | Time 0.1s | Iter Loss 0.0961 | Iter Mean Loss 0.1113\n",
      "CF Training: Epoch 0005 Iter 0422 / 0490 | Time 0.1s | Iter Loss 0.0946 | Iter Mean Loss 0.1113\n",
      "CF Training: Epoch 0005 Iter 0423 / 0490 | Time 0.1s | Iter Loss 0.0866 | Iter Mean Loss 0.1112\n",
      "CF Training: Epoch 0005 Iter 0424 / 0490 | Time 0.1s | Iter Loss 0.0875 | Iter Mean Loss 0.1112\n",
      "CF Training: Epoch 0005 Iter 0425 / 0490 | Time 0.1s | Iter Loss 0.1005 | Iter Mean Loss 0.1111\n",
      "CF Training: Epoch 0005 Iter 0426 / 0490 | Time 0.1s | Iter Loss 0.1156 | Iter Mean Loss 0.1111\n",
      "CF Training: Epoch 0005 Iter 0427 / 0490 | Time 0.1s | Iter Loss 0.1220 | Iter Mean Loss 0.1112\n",
      "CF Training: Epoch 0005 Iter 0428 / 0490 | Time 0.1s | Iter Loss 0.1014 | Iter Mean Loss 0.1111\n",
      "CF Training: Epoch 0005 Iter 0429 / 0490 | Time 0.1s | Iter Loss 0.0746 | Iter Mean Loss 0.1111\n",
      "CF Training: Epoch 0005 Iter 0430 / 0490 | Time 0.1s | Iter Loss 0.0967 | Iter Mean Loss 0.1110\n",
      "CF Training: Epoch 0005 Iter 0431 / 0490 | Time 0.1s | Iter Loss 0.0933 | Iter Mean Loss 0.1110\n",
      "CF Training: Epoch 0005 Iter 0432 / 0490 | Time 0.1s | Iter Loss 0.1142 | Iter Mean Loss 0.1110\n",
      "CF Training: Epoch 0005 Iter 0433 / 0490 | Time 0.1s | Iter Loss 0.1544 | Iter Mean Loss 0.1111\n",
      "CF Training: Epoch 0005 Iter 0434 / 0490 | Time 0.1s | Iter Loss 0.0857 | Iter Mean Loss 0.1110\n",
      "CF Training: Epoch 0005 Iter 0435 / 0490 | Time 0.1s | Iter Loss 0.1064 | Iter Mean Loss 0.1110\n",
      "CF Training: Epoch 0005 Iter 0436 / 0490 | Time 0.1s | Iter Loss 0.0958 | Iter Mean Loss 0.1110\n",
      "CF Training: Epoch 0005 Iter 0437 / 0490 | Time 0.0s | Iter Loss 0.1049 | Iter Mean Loss 0.1110\n",
      "CF Training: Epoch 0005 Iter 0438 / 0490 | Time 0.1s | Iter Loss 0.1102 | Iter Mean Loss 0.1110\n",
      "CF Training: Epoch 0005 Iter 0439 / 0490 | Time 0.1s | Iter Loss 0.1477 | Iter Mean Loss 0.1111\n",
      "CF Training: Epoch 0005 Iter 0440 / 0490 | Time 0.1s | Iter Loss 0.0813 | Iter Mean Loss 0.1110\n",
      "CF Training: Epoch 0005 Iter 0441 / 0490 | Time 0.1s | Iter Loss 0.1141 | Iter Mean Loss 0.1110\n",
      "CF Training: Epoch 0005 Iter 0442 / 0490 | Time 0.1s | Iter Loss 0.1310 | Iter Mean Loss 0.1110\n",
      "CF Training: Epoch 0005 Iter 0443 / 0490 | Time 0.1s | Iter Loss 0.1245 | Iter Mean Loss 0.1111\n",
      "CF Training: Epoch 0005 Iter 0444 / 0490 | Time 0.1s | Iter Loss 0.1240 | Iter Mean Loss 0.1111\n",
      "CF Training: Epoch 0005 Iter 0445 / 0490 | Time 0.1s | Iter Loss 0.0954 | Iter Mean Loss 0.1111\n",
      "CF Training: Epoch 0005 Iter 0446 / 0490 | Time 0.1s | Iter Loss 0.0868 | Iter Mean Loss 0.1110\n",
      "CF Training: Epoch 0005 Iter 0447 / 0490 | Time 0.1s | Iter Loss 0.0855 | Iter Mean Loss 0.1110\n",
      "CF Training: Epoch 0005 Iter 0448 / 0490 | Time 0.1s | Iter Loss 0.0832 | Iter Mean Loss 0.1109\n",
      "CF Training: Epoch 0005 Iter 0449 / 0490 | Time 0.1s | Iter Loss 0.1511 | Iter Mean Loss 0.1110\n",
      "CF Training: Epoch 0005 Iter 0450 / 0490 | Time 0.1s | Iter Loss 0.1004 | Iter Mean Loss 0.1110\n",
      "CF Training: Epoch 0005 Iter 0451 / 0490 | Time 0.1s | Iter Loss 0.1147 | Iter Mean Loss 0.1110\n",
      "CF Training: Epoch 0005 Iter 0452 / 0490 | Time 0.1s | Iter Loss 0.1394 | Iter Mean Loss 0.1110\n",
      "CF Training: Epoch 0005 Iter 0453 / 0490 | Time 0.1s | Iter Loss 0.0922 | Iter Mean Loss 0.1110\n",
      "CF Training: Epoch 0005 Iter 0454 / 0490 | Time 0.1s | Iter Loss 0.0876 | Iter Mean Loss 0.1109\n",
      "CF Training: Epoch 0005 Iter 0455 / 0490 | Time 0.1s | Iter Loss 0.0839 | Iter Mean Loss 0.1109\n",
      "CF Training: Epoch 0005 Iter 0456 / 0490 | Time 0.1s | Iter Loss 0.1017 | Iter Mean Loss 0.1109\n",
      "CF Training: Epoch 0005 Iter 0457 / 0490 | Time 0.1s | Iter Loss 0.0910 | Iter Mean Loss 0.1108\n",
      "CF Training: Epoch 0005 Iter 0458 / 0490 | Time 0.1s | Iter Loss 0.1202 | Iter Mean Loss 0.1108\n",
      "CF Training: Epoch 0005 Iter 0459 / 0490 | Time 0.1s | Iter Loss 0.0895 | Iter Mean Loss 0.1108\n",
      "CF Training: Epoch 0005 Iter 0460 / 0490 | Time 0.1s | Iter Loss 0.0925 | Iter Mean Loss 0.1107\n",
      "CF Training: Epoch 0005 Iter 0461 / 0490 | Time 0.1s | Iter Loss 0.0773 | Iter Mean Loss 0.1107\n",
      "CF Training: Epoch 0005 Iter 0462 / 0490 | Time 0.1s | Iter Loss 0.0985 | Iter Mean Loss 0.1107\n",
      "CF Training: Epoch 0005 Iter 0463 / 0490 | Time 0.1s | Iter Loss 0.0923 | Iter Mean Loss 0.1106\n",
      "CF Training: Epoch 0005 Iter 0464 / 0490 | Time 0.1s | Iter Loss 0.0844 | Iter Mean Loss 0.1106\n",
      "CF Training: Epoch 0005 Iter 0465 / 0490 | Time 0.1s | Iter Loss 0.0849 | Iter Mean Loss 0.1105\n",
      "CF Training: Epoch 0005 Iter 0466 / 0490 | Time 0.1s | Iter Loss 0.1006 | Iter Mean Loss 0.1105\n",
      "CF Training: Epoch 0005 Iter 0467 / 0490 | Time 0.1s | Iter Loss 0.1204 | Iter Mean Loss 0.1105\n",
      "CF Training: Epoch 0005 Iter 0468 / 0490 | Time 0.1s | Iter Loss 0.0922 | Iter Mean Loss 0.1105\n",
      "CF Training: Epoch 0005 Iter 0469 / 0490 | Time 0.1s | Iter Loss 0.0845 | Iter Mean Loss 0.1104\n",
      "CF Training: Epoch 0005 Iter 0470 / 0490 | Time 0.1s | Iter Loss 0.0847 | Iter Mean Loss 0.1104\n",
      "CF Training: Epoch 0005 Iter 0471 / 0490 | Time 0.1s | Iter Loss 0.0948 | Iter Mean Loss 0.1103\n",
      "CF Training: Epoch 0005 Iter 0472 / 0490 | Time 0.1s | Iter Loss 0.0978 | Iter Mean Loss 0.1103\n",
      "CF Training: Epoch 0005 Iter 0473 / 0490 | Time 0.1s | Iter Loss 0.1077 | Iter Mean Loss 0.1103\n",
      "CF Training: Epoch 0005 Iter 0474 / 0490 | Time 0.1s | Iter Loss 0.1073 | Iter Mean Loss 0.1103\n",
      "CF Training: Epoch 0005 Iter 0475 / 0490 | Time 0.1s | Iter Loss 0.0696 | Iter Mean Loss 0.1102\n",
      "CF Training: Epoch 0005 Iter 0476 / 0490 | Time 0.1s | Iter Loss 0.1163 | Iter Mean Loss 0.1102\n",
      "CF Training: Epoch 0005 Iter 0477 / 0490 | Time 0.0s | Iter Loss 0.0834 | Iter Mean Loss 0.1102\n",
      "CF Training: Epoch 0005 Iter 0478 / 0490 | Time 0.1s | Iter Loss 0.0848 | Iter Mean Loss 0.1101\n",
      "CF Training: Epoch 0005 Iter 0479 / 0490 | Time 0.1s | Iter Loss 0.1378 | Iter Mean Loss 0.1102\n",
      "CF Training: Epoch 0005 Iter 0480 / 0490 | Time 0.1s | Iter Loss 0.1087 | Iter Mean Loss 0.1102\n",
      "CF Training: Epoch 0005 Iter 0481 / 0490 | Time 0.1s | Iter Loss 0.0760 | Iter Mean Loss 0.1101\n",
      "CF Training: Epoch 0005 Iter 0482 / 0490 | Time 0.1s | Iter Loss 0.1210 | Iter Mean Loss 0.1101\n",
      "CF Training: Epoch 0005 Iter 0483 / 0490 | Time 0.1s | Iter Loss 0.1293 | Iter Mean Loss 0.1101\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 33\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data, args)\u001b[0m\n\u001b[1;32m     31\u001b[0m cf_batch_user \u001b[38;5;241m=\u001b[39m cf_batch_user\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     32\u001b[0m cf_batch_pos_item \u001b[38;5;241m=\u001b[39m cf_batch_pos_item\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m---> 33\u001b[0m cf_batch_neg_item \u001b[38;5;241m=\u001b[39m \u001b[43mcf_batch_neg_item\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m user_emb, item_emb \u001b[38;5;241m=\u001b[39m model()\n\u001b[1;32m     36\u001b[0m anchor_emb \u001b[38;5;241m=\u001b[39m user_emb[cf_batch_user]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, data, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hungvv",
   "language": "python",
   "name": "hungvv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
