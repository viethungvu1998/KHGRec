{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93e5bfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jun/anaconda3/envs/hungvv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.init import xavier_normal_, xavier_uniform_\n",
    "import argparse\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "from os.path import abspath\n",
    "\n",
    "from base.graph_recommender import GraphRecommender\n",
    "from util.sampler import next_batch_pairwise_kg, next_batch_pairwise\n",
    "from util.conf import OptionConf\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from scipy.sparse import coo_matrix\n",
    "from util.loss_torch import l2_reg_loss, EmbLoss, contrastLoss\n",
    "from util.init import *\n",
    "from base.torch_interface import TorchGraphInterface\n",
    "import os\n",
    "import numpy as np \n",
    "import time \n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from itertools import product\n",
    "\n",
    "from data.loader import FileIO\n",
    "from util.conf import ModelConf\n",
    "from base.recommender import Recommender\n",
    "from data.ui_graph import Interaction\n",
    "# from data.knowledge import Knowledge\n",
    "from util.algorithm import find_k_largest\n",
    "from time import strftime, localtime\n",
    "from data.loader import FileIO\n",
    "from util.evaluation import ranking_evaluation\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8330375",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f647e36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6807114",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GraphRecommender(Recommender):\n",
    "    def __init__(self, conf, training_set, test_set, knowledge_set, **kwargs):\n",
    "        super(GraphRecommender, self).__init__(conf, training_set, test_set, knowledge_set, **kwargs)\n",
    "        self.data = Interaction(conf, training_set, test_set, knowledge_set, self.knowledge)\n",
    "        self.bestPerformance = []\n",
    "        top = self.ranking['-topN'].split(',')\n",
    "        self.topN = [int(num) for num in top]\n",
    "        self.max_N = max(self.topN)\n",
    "        \n",
    "        self.output = f\"./results/LPHGTransformer/{self.dataset}/@{self.model_name}-inp_emb:{kwargs['input_dim']}-hyper_emb:{kwargs['hyper_dim']}-bs:{self.batch_size}-lr:{kwargs['lr']}-lrd:{kwargs['lr_decay']}-reg:{kwargs['reg']}-leaky:{kwargs['p']}-dropout:{kwargs['drop_rate']}-n_layers:{kwargs['n_layers']}-n_heads:{kwargs['n_heads']}-n_self_att:{kwargs['n_self_att']}/\"\n",
    "        if not os.path.exists(self.output):\n",
    "            os.makedirs(self.output)\n",
    "\n",
    "    def print_model_info(self):\n",
    "        super(GraphRecommender, self).print_model_info()\n",
    "        # # print dataset statistics\n",
    "        print('Training Set Size: (user number: %d, item number %d, interaction number: %d)' % (self.data.training_size()))\n",
    "        print('Test Set Size: (user number: %d, item number %d, interaction number: %d)' % (self.data.test_size()))\n",
    "        print('=' * 80)\n",
    "\n",
    "    def build(self):\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        pass\n",
    "\n",
    "    def predict(self, u):\n",
    "        pass\n",
    "\n",
    "    def test(self, user_emb, item_emb):\n",
    "        def process_bar(num, total):\n",
    "            rate = float(num) / total\n",
    "            ratenum = int(50 * rate)\n",
    "            r = '\\rProgress: [{}{}]{}%'.format('+' * ratenum, ' ' * (50 - ratenum), ratenum*2)\n",
    "            sys.stdout.write(r)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        # predict\n",
    "        rec_list = {}\n",
    "        user_count = len(self.data.test_set)\n",
    "        for i, user in enumerate(self.data.test_set):\n",
    "            # s_find_candidates = time.time()\n",
    "            \n",
    "            # candidates = predict(user)\n",
    "            user_id  = self.data.get_user_id(user)\n",
    "            score = torch.matmul(user_emb[user_id], item_emb.transpose(0, 1))\n",
    "            candidates = score.cpu().numpy()\n",
    "            \n",
    "            # e_find_candidates = time.time()\n",
    "            # print(\"Calculate candidates time: %f s\" % (e_find_candidates - s_find_candidates))\n",
    "            # predictedItems = denormalize(predictedItems, self.data.rScale[-1], self.data.rScale[0])\n",
    "            rated_list, li = self.data.user_rated(user)\n",
    "            for item in rated_list:\n",
    "                candidates[self.data.item[item]] = -10e8\n",
    "            \n",
    "            # s_find_k_largest = time.time()\n",
    "            ids, scores = find_k_largest(self.max_N, candidates)\n",
    "            # e_find_k_largest = time.time()\n",
    "            # print(\"Find k largest candidates: %f s\" % (e_find_k_largest - s_find_k_largest))\n",
    "            item_names = [self.data.id2item[iid] for iid in ids]\n",
    "            rec_list[user] = list(zip(item_names, scores))\n",
    "            if i % 1000 == 0:\n",
    "                process_bar(i, user_count)\n",
    "        process_bar(user_count, user_count)\n",
    "        print('')\n",
    "        return rec_list\n",
    "\n",
    "    def evaluate(self, rec_list):\n",
    "        self.recOutput.append('userId: recommendations in (itemId, ranking score) pairs, * means the item is hit.\\n')\n",
    "        for user in self.data.test_set:\n",
    "            line = str(user) + ':'\n",
    "            for item in rec_list[user]:\n",
    "                line += ' (' + str(item[0]) + ',' + str(item[1]) + ')'\n",
    "                if item[0] in self.data.test_set[user]:\n",
    "                    line += '*'\n",
    "            line += '\\n'\n",
    "            self.recOutput.append(line)\n",
    "        current_time = strftime(\"%Y-%m-%d %H-%M-%S\", localtime(time.time()))\n",
    "        # output prediction result\n",
    "        out_dir = self.output\n",
    "        # out_dir = self.output['-dir']\n",
    "        file_name = self.config['model.name'] + '@' + current_time + '-top-' + str(self.max_N) + 'items' + '.txt'\n",
    "        FileIO.write_file(out_dir, file_name, self.recOutput)\n",
    "        print('The result has been output to ', abspath(out_dir), '.')\n",
    "        file_name = self.config['model.name'] + '@' + current_time + '-performance' + '.txt'\n",
    "        self.result = ranking_evaluation(self.data.test_set, rec_list, self.topN)\n",
    "        self.model_log.add('###Evaluation Results###')\n",
    "        self.model_log.add(self.result)\n",
    "        FileIO.write_file(out_dir, file_name, self.result)\n",
    "        print('The result of %s:\\n%s' % (self.model_name, ''.join(self.result)))\n",
    "\n",
    "    def fast_evaluation(self, model, epoch, user_emb, item_emb, kwargs=None):\n",
    "        print('Evaluating the model...')\n",
    "        s_test = time.time()\n",
    "        rec_list = self.test(user_emb, item_emb)\n",
    "        e_test = time.time() \n",
    "        print(\"Test time: %f s\" % (e_test - s_test))\n",
    "        \n",
    "        s_measure = time.time()\n",
    "        all_measures = ranking_evaluation(self.data.test_set, rec_list, self.topN)\n",
    "        len_measures = len(self.topN)\n",
    "        lst_res = []\n",
    "    \n",
    "        data_ep = {}\n",
    "        data_ep['epoch'] = epoch\n",
    "        for i in range(0, len(all_measures), 5):\n",
    "            mes = all_measures[i:i+5]\n",
    "            topk = int(mes[0].split(' ')[1][:-1])\n",
    "            hitk = float(mes[1].split(':')[1][:-1])\n",
    "            preck = float(mes[2].split(':')[1][:-1])\n",
    "            reck = float(mes[3].split(':')[1][:-1])\n",
    "            ndcgk = float(mes[4].split(':')[1][:-1])\n",
    "            data_ep[f'hit@{topk}'] = hitk\n",
    "            data_ep[f'precision@{topk}'] = preck\n",
    "            data_ep[f'recall@{topk}'] = reck\n",
    "            data_ep[f'ndcg@{topk}'] = ndcgk\n",
    "\n",
    "        measure = all_measures[(len_measures - 1) * 5: (len_measures - 1) * 5 + 5]\n",
    "        e_measure = time.time()\n",
    "        print(\"Measure time: %f s\" % (e_measure - s_measure))\n",
    "        if len(self.bestPerformance) > 0:\n",
    "            count = 0\n",
    "            performance = {}\n",
    "            for m in measure[1:]:\n",
    "                k, v = m.strip().split(':')\n",
    "                performance[k] = float(v)\n",
    "            for k in self.bestPerformance[1]:\n",
    "                if self.bestPerformance[1][k] > performance[k]:\n",
    "                    count += 1\n",
    "                else:\n",
    "                    count -= 1\n",
    "            if count < 0:\n",
    "                self.bestPerformance[1] = performance\n",
    "                self.bestPerformance[0] = epoch + 1\n",
    "                self.save(model)\n",
    "        else:\n",
    "            self.bestPerformance.append(epoch + 1)\n",
    "            performance = {}\n",
    "            for m in measure[1:]:\n",
    "                k, v = m.strip().split(':')\n",
    "                performance[k] = float(v)\n",
    "            self.bestPerformance.append(performance)\n",
    "            self.save(model)\n",
    "\n",
    "        print('-' * 120)\n",
    "        print('Real-Time Ranking Performance ' + ' (Top-' + str(self.max_N) + ' Item Recommendation)')\n",
    "        measure = [m.strip() for m in measure[1:]]\n",
    "        print('*Current Performance*')\n",
    "        print('Epoch:', str(epoch + 1) + ',', '  |  '.join(measure))\n",
    "        bp = ''\n",
    "        # for k in self.bestPerformance[1]:\n",
    "        #     bp+=k+':'+str(self.bestPerformance[1][k])+' | '\n",
    "        bp += 'Hit Ratio' + ':' + str(self.bestPerformance[1]['Hit Ratio']) + '  |  '\n",
    "        bp += 'Precision' + ':' + str(self.bestPerformance[1]['Precision']) + '  |  '\n",
    "        bp += 'Recall' + ':' + str(self.bestPerformance[1]['Recall']) + '  |  '\n",
    "        # bp += 'F1' + ':' + str(self.bestPerformance[1]['F1']) + ' | '\n",
    "        bp += 'NDCG' + ':' + str(self.bestPerformance[1]['NDCG'])\n",
    "        print('*Best Performance* ')\n",
    "        print('Epoch:', str(self.bestPerformance[0]) + ',', bp)\n",
    "        print('-' * 120)\n",
    "        return measure, data_ep\n",
    "    \n",
    "    def save(self, model):\n",
    "        with torch.no_grad():\n",
    "            self.best_user_emb, self.best_item_emb = model()\n",
    "        self.save_model(model)\n",
    "    \n",
    "    def save_model(self, model):\n",
    "        # save model \n",
    "        current_time = strftime(\"%Y-%m-%d\", localtime(time.time()))\n",
    "        out_dir = self.output\n",
    "        file_name =  self.config['model.name'] + '@' + current_time + '-weight' + '.pth'\n",
    "        weight_file = out_dir + '/' + file_name \n",
    "        torch.save(model.state_dict(), weight_file)\n",
    "\n",
    "    def save_loss(self, train_losses, rec_losses, reg_losses):\n",
    "        df_train_loss = pd.DataFrame(train_losses, columns = ['ep', 'loss'])\n",
    "        df_rec_loss = pd.DataFrame(rec_losses, columns = ['ep', 'loss'])\n",
    "        df_reg_loss = pd.DataFrame(reg_losses, columns = ['ep', 'loss'])\n",
    "        df_train_loss.to_csv(self.output + '/train_loss.csv')\n",
    "        df_rec_loss.to_csv(self.output + '/rec_loss.csv')\n",
    "        df_reg_loss.to_csv(self.output + '/reg_loss.csv')\n",
    "\n",
    "    def save_perfomance_training(self, log_train):\n",
    "        df_train_log = pd.DataFrame(log_train)\n",
    "        df_train_log.to_csv(self.output + '/train_performance.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d609a90-7628-4830-9bed-376e25948692",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d5842a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HGNNConv(nn.Module):\n",
    "    def __init__(self, leaky, input_dim, hyper_dim):\n",
    "        super(HGNNConv, self).__init__()\n",
    "        self.hyper_dim = hyper_dim\n",
    "        self.act = nn.LeakyReLU(negative_slope=leaky)\n",
    "        self.fc1 = nn.Linear(input_dim, hyper_dim ,bias=False).to(device)\n",
    "        self.fc2 = nn.Linear(hyper_dim, hyper_dim ,bias=False).to(device)\n",
    "        self.fc3 = nn.Linear(hyper_dim, hyper_dim ,bias=False).to(device)\n",
    "\n",
    "    def forward(self, adj, embeds):\n",
    "        lat1 = self.act(adj.T @ embeds)\n",
    "        lat2 = self.act(self.fc1(lat1)) +  lat1\n",
    "        lat3 = self.act(self.fc2(lat2)) + lat2\n",
    "        lat4 = self.act(self.fc3(lat3)) + lat3 \n",
    "        ret = self.act(adj @ lat4)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd233643-af9c-4506-8c50-66369ffa503e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DHCF(nn.Module):\n",
    "    def __init__(self, config, data, args):        \n",
    "        super(DHCF, self).__init__()\n",
    "        \n",
    "        self.data = data\n",
    "        self.norm_adj = data.norm_adj\n",
    "        self.sparse_norm_adj = TorchGraphInterface.convert_sparse_mat_to_tensor(self.norm_adj).to(device)\n",
    "        self._parse_args(args)\n",
    "        \n",
    "        hyper_uu = self.data.interaction_mat\n",
    "        hyper_ii = self.data.interaction_mat.T\n",
    "        \n",
    "        self.hyper_uu = TorchGraphInterface.convert_sparse_mat_to_tensor(hyper_uu).to_dense().to(device)\n",
    "        self.hyper_ii = TorchGraphInterface.convert_sparse_mat_to_tensor(hyper_ii).to_dense().to(device)\n",
    "        # embeddings\n",
    "        initializer = nn.init.xavier_uniform_\n",
    "        self.user_embed = nn.Parameter(initializer(torch.Tensor(self.data.user_num, self.emb_size)).to(device))\n",
    "        self.item_embed = nn.Parameter(initializer(torch.Tensor(self.data.item_num, self.emb_size)).to(device))\n",
    "        # self.user_embed = nn.Parameter(torch.Tensor(self.data.user_num, self.emb_size)).to(device)\n",
    "        # self.item_embed = nn.Parameter(torch.Tensor(self.data.item_num, self.emb_size)).to(device)\n",
    "        \n",
    "        # self.layers = [DHCFLayer(self.emb_size, self.emb_size, self.leaky, self.emb_size) for _ in range(self.n_layers)]\n",
    "        self.dropout = nn.Dropout(self.drop_rate)\n",
    "    \n",
    "        self.hgnn_u = [HGNNConv(self.leaky, input_dim=self.hyper_dim, hyper_dim=self.hyper_dim) for _ in range(self.n_layers)]\n",
    "        self.hgnn_i = [HGNNConv(self.leaky, input_dim=self.hyper_dim, hyper_dim=self.hyper_dim ) for _ in range(self.n_layers)]\n",
    "        self.non_linear = nn.ReLU()\n",
    "        \n",
    "        self.fc_u = nn.Linear(self.emb_size, self.hyper_dim)\n",
    "        self.fc_i = nn.Linear(self.emb_size, self.hyper_dim)\n",
    "    \n",
    "    def _parse_args(self, args):\n",
    "        self.n_layers = args['n_layers']\n",
    "        self.drop_rate = args['drop_rate'] \n",
    "        self.emb_size = args['input_dim']\n",
    "        self.hyper_dim = args['hyper_dim']\n",
    "        self.leaky = args['p']\n",
    "        self.n_edges = args['hyper_num']\n",
    "        \n",
    "    def forward(self):\n",
    "        user_emb = self.user_embed\n",
    "        item_emb = self.item_embed\n",
    "        \n",
    "        user_emb = self.dropout(self.non_linear(self.fc_u(user_emb)))\n",
    "        item_emb = self.dropout(self.non_linear(self.fc_i(item_emb)))\n",
    "        \n",
    "        user_out = user_emb.clone()\n",
    "        item_out = item_emb.clone()\n",
    "        \n",
    "        for idx,layer in enumerate(range(self.n_layers)):\n",
    "            user_hyper = torch.where(self.hyper_uu >= 0.5, 1., 0.)\n",
    "            user_hyper = user_hyper.to(torch.float32)\n",
    "            \n",
    "            item_hyper = torch.where(self.hyper_ii >= 0.5, 1., 0.)\n",
    "            item_hyper = item_hyper.to(torch.float32)\n",
    "            \n",
    "            hyperULat = self.hgnn_u[idx](user_hyper, user_emb)\n",
    "            hyperILat = self.hgnn_i[idx](item_hyper, item_emb)\n",
    "            \n",
    "            # user_emb = self.dropout[idx](self.user_embed)\n",
    "            # item_emb = self.dropout[idx](self.item_embed)\n",
    "            #user_emb, item_emb = layer(self.H, user_emb, item_emb)\n",
    "            # user_emb, item_emb = layer(self.hyper_uu, self.hyper_ii, user_emb, item_emb) \n",
    "            \n",
    "            user_out = torch.cat((user_out, hyperULat), dim=1)\n",
    "            item_out = torch.cat((item_out, hyperILat), dim=1)    \n",
    "        return user_out, item_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f30906d-1c4f-43a5-952e-943d20c668d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DHCFLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, leaky, emb_size, bias=True):\n",
    "        super(DHCFLayer, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        # self.weight = nn.Parameter(torch.Tensor(in_channels, out_channels).to(device))\n",
    "        self.hgnn_u = HGNNConv(leaky, input_dim=in_channels, hyper_dim=out_channels)\n",
    "        self.hgnn_i = HGNNConv(leaky, input_dim=in_channels, hyper_dim=out_channels)\n",
    "    \n",
    "    def forward(self, H_u, H_i, user_emb, item_emb):\n",
    "#         user_hyper = torch.cat((H, torch.matmul(H, torch.matmul(H.t(), H))), dim=1)\n",
    "        user_hyper = H_u\n",
    "        user_hyper = torch.where(user_hyper >= 0.5, 1., 0.)\n",
    "        user_hyper = user_hyper.to(torch.float32)\n",
    "        hyperULat = self.hgnn_u(user_hyper, user_emb)\n",
    "        user_out = hyperULat \n",
    "        \n",
    "        item_hyper = H_i\n",
    "        item_hyper = torch.where(abs(item_hyper)>=0.5, 1., 0.)\n",
    "        item_hyper = item_hyper.to(torch.float32)\n",
    "        hyperILat = self.hgnn_i(item_hyper, item_emb)\n",
    "        \n",
    "        item_out = hyperILat\n",
    "        \n",
    "#         user_out = torch.matmul(M_u, self.weight) + self.bias\n",
    "#         item_out = torch.matmul(M_i, self.weight) + self.bias\n",
    "        \n",
    "        return user_out, item_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e95340-75fd-4845-862d-b3156e6119b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f4904e-6ee6-4a32-9a8c-a41a453663c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "416b3faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(anchor_emb, pos_emb, neg_emb, batch_size, reg):\n",
    "    def bpr_loss(user_emb, pos_item_emb, neg_item_emb):\n",
    "        pos_score = torch.mul(user_emb, pos_item_emb).sum(dim=1)\n",
    "        neg_score = torch.mul(user_emb, neg_item_emb).sum(dim=1)\n",
    "        loss = -torch.log(10e-6 + torch.sigmoid(pos_score - neg_score))\n",
    "        return torch.mean(loss)\n",
    "    \n",
    "    calc_reg_loss = EmbLoss()\n",
    "    rec_loss = bpr_loss(anchor_emb, pos_emb, neg_emb)\n",
    "    reg_loss = reg * calc_reg_loss(anchor_emb, pos_emb, neg_emb) / batch_size\n",
    "    return rec_loss, reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3856d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(u, rec, user_emb, item_emb):\n",
    "    user_id  = rec.data.get_user_id(u)\n",
    "    score = torch.matmul(user_emb[user_id], item_emb.transpose(0, 1))\n",
    "    return score.cpu().numpy()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8abb93",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da114e6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    lst_train_losses = []\n",
    "    lst_rec_losses = []\n",
    "    lst_reg_losses = []\n",
    "    lst_cl_losses = []\n",
    "    lst_kg_losses = [] \n",
    "    lst_performances = []\n",
    "    \n",
    "    for ep in range(maxEpoch):\n",
    "        train_losses = []\n",
    "        rec_losses = []\n",
    "        reg_losses = []\n",
    "        cl_losses = []\n",
    "        kg_losses = []\n",
    "        \n",
    "        for n, batch in enumerate(next_batch_pairwise(rec.data, batchSize)):\n",
    "            user_idx, pos_idx, neg_idx = batch\n",
    "            train_model.train()\n",
    "            user_emb, item_emb = train_model()\n",
    "            anchor_emb = user_emb[user_idx]\n",
    "            pos_emb = item_emb[pos_idx]\n",
    "            neg_emb = item_emb[neg_idx]\n",
    "            \n",
    "            rec_loss, reg_loss = calculate_loss(anchor_emb, pos_emb, neg_emb, batchSize, reg)\n",
    "            # print(\"Rec loss\", rec_loss)\n",
    "            # print(\"Reg loss\", reg_loss)\n",
    "            batch_loss = rec_loss + reg_loss \n",
    "            \n",
    "            train_losses.append(batch_loss.item())\n",
    "            rec_losses.append(rec_loss.item())\n",
    "            reg_losses.append(reg_loss.item())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            # torch.nn.utils.clip_grad_norm_(train_model.parameters(), 3)\n",
    "            optimizer.step()\n",
    "\n",
    "        batch_train_loss = np.mean(train_losses)\n",
    "        scheduler.step(batch_train_loss)\n",
    "        \n",
    "        train_loss = np.mean(train_losses)\n",
    "        rec_loss = np.mean(rec_losses)\n",
    "        reg_loss = np.mean(reg_losses)\n",
    "        \n",
    "        lst_train_losses.append([ep, train_loss])\n",
    "        lst_rec_losses.append([ep,rec_loss])\n",
    "        lst_reg_losses.append([ep, reg_loss])\n",
    "\n",
    "        # Evaluation\n",
    "        train_model.eval()\n",
    "        with torch.no_grad():\n",
    "            user_emb, item_emb = train_model()\n",
    "            _, data_ep = rec.fast_evaluation(train_model, ep, user_emb, item_emb)\n",
    "        lst_performances.append(data_ep)\n",
    "\n",
    "    rec.save_loss(lst_train_losses, lst_rec_losses, lst_reg_losses)\n",
    "    rec.save_perfomance_training(lst_performances)\n",
    "    user_emb, item_emb = rec.best_user_emb, rec.best_item_emb\n",
    "    return user_emb, item_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca8bf35",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d338b049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(rec, user_emb, item_emb):\n",
    "    def process_bar(num, total):\n",
    "        rate = float(num) / total\n",
    "        ratenum = int(50 * rate)\n",
    "        r = '\\rProgress: [{}{}]{}%'.format('+' * ratenum, ' ' * (50 - ratenum), ratenum*2)\n",
    "        sys.stdout.write(r)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    # predict\n",
    "    rec_list = {}\n",
    "    user_count = len(rec.data.test_set)\n",
    "    for i, user in enumerate(rec.data.test_set):\n",
    "        # s_find_candidates = time.time()\n",
    "        candidates = predict(user, rec, user_emb, item_emb)\n",
    "        # e_find_candidates = time.time()\n",
    "        # print(\"Calculate candidates time: %f s\" % (e_find_candidates - s_find_candidates))\n",
    "        # predictedItems = denormalize(predictedItems, self.data.rScale[-1], self.data.rScale[0])\n",
    "        rated_list, li = rec.data.user_rated(user)\n",
    "        for item in rated_list:\n",
    "            candidates[rec.data.item[item]] = -10e8\n",
    "\n",
    "        # s_find_k_largest = time.time()\n",
    "        ids, scores = find_k_largest(rec.max_N, candidates)\n",
    "        # e_find_k_largest = time.time()\n",
    "        # print(\"Find k largest candidates: %f s\" % (e_find_k_largest - s_find_k_largest))\n",
    "        item_names = [rec.data.id2item[iid] for iid in ids]\n",
    "        rec_list[user] = list(zip(item_names, scores))\n",
    "        if i % 1000 == 0:\n",
    "            process_bar(i, user_count)\n",
    "    process_bar(user_count, user_count)\n",
    "    print('')\n",
    "    rec.evaluate(rec_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5557c0ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9cc8433",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = 'DHCF'\n",
    "config = ModelConf('./conf/' + model + '.conf')\n",
    "\n",
    "dataset = 'ml-1m'\n",
    "batchSize = int(config['batch_size'])\n",
    "# lRate = float(config['learnRate'])\n",
    "# maxEpoch = int(config['num.max.epoch'])\n",
    "# reg = float(config['reg.lambda'])\n",
    "maxEpoch = 500\n",
    "lRates = [0.05]\n",
    "lrDecays = [0.9]\n",
    "regs = [0.1]\n",
    "hyperDims = [128]\n",
    "hyperNum = [128]\n",
    "inputDims = [32]\n",
    "ps = [0.1]\n",
    "dropRates = [0.1]\n",
    "nLayers = [2]\n",
    "nHeads = [1]\n",
    "nSelfAtt = [1]\n",
    "\n",
    "if dataset == 'lastfm':\n",
    "    training_set = \"./dataset/lastfm/train.txt\"\n",
    "    test_set = './dataset/lastfm/test.txt'\n",
    "    knowledge_data='./dataset/lastfm/processed/lastfm.kg'\n",
    "elif dataset == 'ml-1m':\n",
    "    training_set = './dataset/ml-1m/train.txt'\n",
    "    test_set = './dataset/ml-1m/test.txt'\n",
    "    knowledge_data = './dataset/ml-1m/processed/ml-1m.kg'\n",
    "hyperparameters = [lRates, lrDecays, regs, hyperDims, inputDims, ps, dropRates, nLayers, hyperNum, nHeads, nSelfAtt]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb333fdf-dc2e-4df7-b4c4-11e3f1a6ec11",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed338200",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.05, 0.9, 0.1, 128, 32, 0.1, 0.1, 2, 128, 1, 1)\n",
      "parameter ss_rate is not found in the configuration file!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/hungvv/recommender/SELFRec/base/torch_interface.py:10: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:204.)\n",
      "  i = torch.LongTensor([coo.row, coo.col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "Test time: 3.705601 s\n",
      "Measure time: 0.081426 s\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 1, Hit Ratio:0.11546  |  Precision:0.05771  |  Recall:0.12877  |  NDCG:0.11299\n",
      "*Best Performance* \n",
      "Epoch: 1, Hit Ratio:0.11546  |  Precision:0.05771  |  Recall:0.12877  |  NDCG:0.11299\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "Test time: 1.031502 s\n",
      "Measure time: 0.079197 s\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 2, Hit Ratio:0.11404  |  Precision:0.057  |  Recall:0.12677  |  NDCG:0.11369\n",
      "*Best Performance* \n",
      "Epoch: 1, Hit Ratio:0.11546  |  Precision:0.05771  |  Recall:0.12877  |  NDCG:0.11299\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "Test time: 0.926700 s\n",
      "Measure time: 0.077042 s\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 3, Hit Ratio:0.10866  |  Precision:0.05431  |  Recall:0.12063  |  NDCG:0.10949\n",
      "*Best Performance* \n",
      "Epoch: 1, Hit Ratio:0.11546  |  Precision:0.05771  |  Recall:0.12877  |  NDCG:0.11299\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "Test time: 1.024801 s\n",
      "Measure time: 0.078295 s\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 4, Hit Ratio:0.11525  |  Precision:0.05761  |  Recall:0.12763  |  NDCG:0.11246\n",
      "*Best Performance* \n",
      "Epoch: 1, Hit Ratio:0.11546  |  Precision:0.05771  |  Recall:0.12877  |  NDCG:0.11299\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "Test time: 1.025649 s\n",
      "Measure time: 0.079450 s\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 5, Hit Ratio:0.11583  |  Precision:0.0579  |  Recall:0.12909  |  NDCG:0.11272\n",
      "*Best Performance* \n",
      "Epoch: 5, Hit Ratio:0.11583  |  Precision:0.0579  |  Recall:0.12909  |  NDCG:0.11272\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "Test time: 1.031560 s\n",
      "Measure time: 0.080364 s\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 6, Hit Ratio:0.11184  |  Precision:0.0559  |  Recall:0.12255  |  NDCG:0.10949\n",
      "*Best Performance* \n",
      "Epoch: 5, Hit Ratio:0.11583  |  Precision:0.0579  |  Recall:0.12909  |  NDCG:0.11272\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "Test time: 1.032144 s\n",
      "Measure time: 0.078286 s\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 7, Hit Ratio:0.10957  |  Precision:0.05477  |  Recall:0.11804  |  NDCG:0.10736\n",
      "*Best Performance* \n",
      "Epoch: 5, Hit Ratio:0.11583  |  Precision:0.0579  |  Recall:0.12909  |  NDCG:0.11272\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "Test time: 1.027857 s\n",
      "Measure time: 0.080425 s\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 8, Hit Ratio:0.10857  |  Precision:0.05427  |  Recall:0.11756  |  NDCG:0.10684\n",
      "*Best Performance* \n",
      "Epoch: 5, Hit Ratio:0.11583  |  Precision:0.0579  |  Recall:0.12909  |  NDCG:0.11272\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "Test time: 1.013795 s\n",
      "Measure time: 0.081260 s\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 9, Hit Ratio:0.10855  |  Precision:0.05426  |  Recall:0.11663  |  NDCG:0.1061\n",
      "*Best Performance* \n",
      "Epoch: 5, Hit Ratio:0.11583  |  Precision:0.0579  |  Recall:0.12909  |  NDCG:0.11272\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "Test time: 1.022651 s\n",
      "Measure time: 0.077239 s\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 10, Hit Ratio:0.10971  |  Precision:0.05484  |  Recall:0.1213  |  NDCG:0.10781\n",
      "*Best Performance* \n",
      "Epoch: 5, Hit Ratio:0.11583  |  Precision:0.0579  |  Recall:0.12909  |  NDCG:0.11272\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "Test time: 1.023407 s\n",
      "Measure time: 0.076947 s\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 11, Hit Ratio:0.11094  |  Precision:0.05545  |  Recall:0.12095  |  NDCG:0.10759\n",
      "*Best Performance* \n",
      "Epoch: 5, Hit Ratio:0.11583  |  Precision:0.0579  |  Recall:0.12909  |  NDCG:0.11272\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "Test time: 1.027861 s\n",
      "Measure time: 0.077859 s\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 12, Hit Ratio:0.10994  |  Precision:0.05495  |  Recall:0.12121  |  NDCG:0.1082\n",
      "*Best Performance* \n",
      "Epoch: 5, Hit Ratio:0.11583  |  Precision:0.0579  |  Recall:0.12909  |  NDCG:0.11272\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "Test time: 1.028929 s\n",
      "Measure time: 0.079448 s\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 13, Hit Ratio:0.10724  |  Precision:0.0536  |  Recall:0.11732  |  NDCG:0.10594\n",
      "*Best Performance* \n",
      "Epoch: 5, Hit Ratio:0.11583  |  Precision:0.0579  |  Recall:0.12909  |  NDCG:0.11272\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "Test time: 1.037282 s\n",
      "Measure time: 0.078660 s\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 14, Hit Ratio:0.10635  |  Precision:0.05316  |  Recall:0.11593  |  NDCG:0.10482\n",
      "*Best Performance* \n",
      "Epoch: 5, Hit Ratio:0.11583  |  Precision:0.0579  |  Recall:0.12909  |  NDCG:0.11272\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "Test time: 1.037758 s\n",
      "Measure time: 0.077573 s\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 15, Hit Ratio:0.10568  |  Precision:0.05282  |  Recall:0.1146  |  NDCG:0.10365\n",
      "*Best Performance* \n",
      "Epoch: 5, Hit Ratio:0.11583  |  Precision:0.0579  |  Recall:0.12909  |  NDCG:0.11272\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "Test time: 0.755000 s\n",
      "Measure time: 0.079274 s\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 16, Hit Ratio:0.10481  |  Precision:0.05239  |  Recall:0.11329  |  NDCG:0.10297\n",
      "*Best Performance* \n",
      "Epoch: 5, Hit Ratio:0.11583  |  Precision:0.0579  |  Recall:0.12909  |  NDCG:0.11272\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "Test time: 1.048117 s\n",
      "Measure time: 0.078581 s\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 17, Hit Ratio:0.10474  |  Precision:0.05235  |  Recall:0.11326  |  NDCG:0.10293\n",
      "*Best Performance* \n",
      "Epoch: 5, Hit Ratio:0.11583  |  Precision:0.0579  |  Recall:0.12909  |  NDCG:0.11272\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "Test time: 1.036489 s\n",
      "Measure time: 0.078865 s\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 18, Hit Ratio:0.10462  |  Precision:0.05229  |  Recall:0.11312  |  NDCG:0.10283\n",
      "*Best Performance* \n",
      "Epoch: 5, Hit Ratio:0.11583  |  Precision:0.0579  |  Recall:0.12909  |  NDCG:0.11272\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "Test time: 1.030377 s\n",
      "Measure time: 0.077700 s\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 19, Hit Ratio:0.10438  |  Precision:0.05217  |  Recall:0.11296  |  NDCG:0.10255\n",
      "*Best Performance* \n",
      "Epoch: 5, Hit Ratio:0.11583  |  Precision:0.0579  |  Recall:0.12909  |  NDCG:0.11272\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "Test time: 1.035683 s\n",
      "Measure time: 0.077885 s\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 20, Hit Ratio:0.10412  |  Precision:0.05204  |  Recall:0.11245  |  NDCG:0.10222\n",
      "*Best Performance* \n",
      "Epoch: 5, Hit Ratio:0.11583  |  Precision:0.0579  |  Recall:0.12909  |  NDCG:0.11272\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "Test time: 1.037843 s\n",
      "Measure time: 0.077094 s\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 21, Hit Ratio:0.10405  |  Precision:0.05201  |  Recall:0.11244  |  NDCG:0.10218\n",
      "*Best Performance* \n",
      "Epoch: 5, Hit Ratio:0.11583  |  Precision:0.0579  |  Recall:0.12909  |  NDCG:0.11272\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "Test time: 1.169958 s\n",
      "Measure time: 0.084656 s\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 22, Hit Ratio:0.10344  |  Precision:0.05171  |  Recall:0.11214  |  NDCG:0.1015\n",
      "*Best Performance* \n",
      "Epoch: 5, Hit Ratio:0.11583  |  Precision:0.0579  |  Recall:0.12909  |  NDCG:0.11272\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "Test time: 1.046476 s\n",
      "Measure time: 0.079564 s\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 23, Hit Ratio:0.10245  |  Precision:0.05121  |  Recall:0.11137  |  NDCG:0.10095\n",
      "*Best Performance* \n",
      "Epoch: 5, Hit Ratio:0.11583  |  Precision:0.0579  |  Recall:0.12909  |  NDCG:0.11272\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "Test time: 1.041843 s\n",
      "Measure time: 0.078705 s\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 24, Hit Ratio:0.10188  |  Precision:0.05093  |  Recall:0.11118  |  NDCG:0.10065\n",
      "*Best Performance* \n",
      "Epoch: 5, Hit Ratio:0.11583  |  Precision:0.0579  |  Recall:0.12909  |  NDCG:0.11272\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for params in product(*hyperparameters):\n",
    "    lr, lr_decay, reg, hyper_dim, input_dim, prob, drop_rate, n_layers, hyper_num, n_heads, n_self_att = params\n",
    "    print(params)\n",
    "    args = {\n",
    "        'lr': lr,\n",
    "        'lr_decay': lr_decay,\n",
    "        'reg': reg,\n",
    "        'input_dim': input_dim,\n",
    "        'hyper_dim': hyper_dim,\n",
    "        'hyper_num': hyper_num,\n",
    "        'p': prob,\n",
    "        'drop_rate': drop_rate,\n",
    "        'n_layers': n_layers,\n",
    "        'n_heads': n_heads,\n",
    "        'n_self_att': n_self_att,\n",
    "        'dataset': dataset\n",
    "    }\n",
    "    training_data = FileIO.load_data_set(training_set, config['model.type'])\n",
    "    test_data = FileIO.load_data_set(test_set, config['model.type'])\n",
    "    knowledge_set = FileIO.load_kg_data(knowledge_data)\n",
    "\n",
    "    rec = GraphRecommender(config, training_data, test_data, knowledge_set, **args)\n",
    "    train_model = DHCF(rec.config, rec.data, args).to(device)\n",
    "    optimizer  = torch.optim.Adam(train_model.parameters(), lr=lr,weight_decay=1e-3)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', factor=lr_decay, patience=5)\n",
    "    user_emb, item_emb = train()\n",
    "    test(rec, user_emb, item_emb)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695490d5-5f2d-4f37-9252-e4c847801b67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reg = 0.1\n",
    "# args = {\n",
    "#     'p':  0.1,\n",
    "#     'drop_rate': 0.1,\n",
    "#     'leaky': 0.1,\n",
    "#     'n_layers': 2,\n",
    "#     'input_dim': 64,\n",
    "#     'hyper_dim': 128,\n",
    "#     'hyperedge_num': 128,\n",
    "#     'lr': 0.005,\n",
    "#     'reg': reg,\n",
    "#     'dataset': dataset\n",
    "# }\n",
    "\n",
    "\n",
    "# training_data = FileIO.load_data_set(training_set, config['model.type'])\n",
    "# test_data = FileIO.load_data_set(test_set, config['model.type'])\n",
    "# knowledge_set = FileIO.load_kg_data(knowledge_data)\n",
    "\n",
    "\n",
    "# rec = GraphRecommender(config, training_data, test_data, knowledge_set, **args)\n",
    "# if model == 'HGNN':\n",
    "#     train_model = Model(rec.config, rec.data, args).to(device)\n",
    "# elif model == 'LightGCN':\n",
    "#     train_model = LGCN_Encoder(rec.data, args['input_dim'], args['n_layers'])\n",
    "# elif model == 'HCCF':\n",
    "#     train_model = HCCFEncoder(rec.data, args['input_dim'], args['n_layers'], args['hyperedge_num'], args['leaky'], args['drop_rate'])\n",
    "# elif model == 'DHCF':\n",
    "#     train_model = DHCFEncoder(rec.data, args['input_dim'], args['n_layers'], args['hyperedge_num'], args['drop_rate'], args['leaky'], args['hyper_dim'])\n",
    "    \n",
    "# optimizer  = torch.optim.Adam(train_model.parameters(), lr=args['lr'], weight_decay=0.001)\n",
    "# scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=5)\n",
    "# user_emb, item_emb = train(args)\n",
    "# test(rec, user_emb, item_emb, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4185727",
   "metadata": {},
   "source": [
    "## Different hyperpamater settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e169bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, params in enumerate(product(*hyperparameters)):\n",
    "#     lr, lr_decay, reg, hyper_dim, input_dim, prob, drop_rate, n_layers, hyperedge_num = params\n",
    "#     print(\"hyperparameters:{}\".format(params))\n",
    "#     args = {\n",
    "#         'p': prob,\n",
    "#         'drop_rate': drop_rate,\n",
    "#         'n_layers': n_layers,\n",
    "#         'input_dim': input_dim,\n",
    "#         'hyper_dim': hyper_dim,\n",
    "#         'hyperedge_num': hyperedge_num,\n",
    "#         'lr': lr,\n",
    "#         'reg': reg,\n",
    "#         'leaky': 0.5,\n",
    "#         'dataset': dataset\n",
    "#     }\n",
    "\n",
    "#     training_data = FileIO.load_data_set(training_set, config['model.type'])\n",
    "#     test_data = FileIO.load_data_set(test_set, config['model.type'])\n",
    "\n",
    "#     rec = GraphRecommender(config, training_data, test_data, **args)\n",
    "#     if model == 'HGNN':\n",
    "#         train_model = Model(rec.config, rec.data, args).to(device)\n",
    "#     elif model == 'LightGCN':\n",
    "#         train_model = LGCN_Encoder(rec.data, input_dim, args['n_layers'])\n",
    "#     elif model == 'HCCF':\n",
    "#         train_model = HCCFEncoder(rec.data, input_dim, args['n_layers'], args['hyperedge_num'], args['leaky'], args['drop_rate'])\n",
    "#     elif model == 'DHCF':\n",
    "#         train_model = DHCFEncoder(rec.data, input_dim, args['n_layers'], args['hyperedge_num'], args['drop_rate'], args['leaky'], args['hyper_dim'])\n",
    "\n",
    "#     optimizer  = torch.optim.Adam(train_model.parameters(), lr=lr)\n",
    "#     scheduler = ReduceLROnPlateau(optimizer, 'min', factor=lr_decay, patience=5)\n",
    "#     user_emb, item_emb = train(args)\n",
    "#     test(rec, user_emb, item_emb, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4cdb51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hungvv",
   "language": "python",
   "name": "hungvv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
